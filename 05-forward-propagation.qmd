# 순전파 (Forward propagation) {#forward}

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
Sys.setenv(RETICULATE_PYTHON = "/opt/homebrew/Caskroom/miniconda/base/envs/torch/bin/python")
library(reticulate)
use_python("/opt/homebrew/Caskroom/miniconda/base/envs/torch/bin/python", required = T)
```

```{python}
import torch
import torch.nn.functional as F
```

## 신경망의 구조

딥러닝의 시작점인 신경망(Neural network)을 공부하기 위해서, 앞으로 우리가 다룰 모델 중 가장 간단하면서, 딥러닝에서 어떤 일이 벌어지고 있는지 상상이 가능한 신경망을 먼저 학습하기로 하자. 우리가 오늘 예로 생각할 신경망은 다음과 같다.

```{r neuralnet-example, echo=FALSE, fig.cap="세상에서 가장 간단하지만 있을 건 다있는 신경망", fig.align='center', out.width = '100%'}
knitr::include_graphics("./image/neuralnet1.png")
```

위의 그림과 같은 신경망을 2단 신경망이라고 부른다. 일반적으로 단수를 셀 때 제일 처음 입력하는 층은 단수에 포함하지 않는 것에 주의하자. 각 녹색, 회색, 그리고 빨간색의 노드(node)들은 신경망의 요소를 이루는데, 각각의 이름은 다음과 같다.

-   입력층(input layer) - 2개의 녹색 노드(node)
-   은닉층(hidden layer) - 3개의 회색 노드(node)
-   출력층(output layer) - 1개의 빨강색 노드(node)

자 이제부터, 녹색 노드에는 무엇이 들어가는지, 그리고, 어떤 과정을 거쳐서 빨강색의 값이 나오는지에 대하여 알아보자. 딥러닝에서 녹색이 입력값을 넣어서 빨간색의 결과값을 얻는 과정을 **순전파(Forward propagation)**라고 부른다. `propagation`의 뜻은 증식, 혹은 번식인데, 식물이나 동물이 자라나는 것을 의미하는데, 녹색의 입력값들이 어떠한 과정을 거쳐 빨간색으로 자라나는지 한번 알아보자.

## 순전파(Forward propagation)

우리가 사용할 데이터 역시 아주 간단하다.

$$
X =\left(\begin{array}{cc}
1 & 2\\
3 & 4\\
5 & 6
\end{array}\right)
$$ 

가로 행이 하나의 표본을 의미하고, 세로 열 각각은 변수를 의미한다. 즉, 위의 자료 행렬은 2개의 변수 정보가 들어있는 세 개의 표본들이 있는 자료을 의미한다.

### 표본 1개, 경로 1개만 생각해보기

주의할 것은, 우리가 그려놓은 신경망의 입력층의 노드는 2개이고, 자료 행렬은 3행 2열이라는 것이다. 우리가 그려놓은 신경망으로 샘플 하나 하나가 입력층에 각각 입력되어 표본별 결과값 생성되는 것이다. 따라서 신경망을 잘 이해하기 위해서 딱 하나의 표본, 그리고 딱 하나의 경로만을 생각해보자.

```         
> 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 경로를 타고 어떻게 자라나는지 생각해보자. 
```

```{r neuralnet-path, echo=FALSE, fig.cap="예시 경로 1", fig.align='center', out.width = '100%'}
knitr::include_graphics("./image/neuralnet3.png")
```

그림에서 $\beta$는 노드와 노드 사이를 지나갈 때 부여되는 웨이트들을 의미하고, $\sigma()$는 다음의 시그모이드(sigmoid) 함수를 의미한다.

$$
\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x+1}
$$

자료 행렬을 위에 색칠된 경로로 보낸다는 의미는 다음과 같은 계산과정을 거친다는 것이다.
```{python}
# 입력 데이터
X = torch.tensor([[1.0, 2.0]], dtype=torch.float64)  # 1행 2열
print(X)

# 첫 번째 은닉층 가중치 (beta_1)
beta_1 = torch.tensor([[0.5], [0.8]], dtype=torch.float64)  # 2행 1열
print(beta_1)

# 은닉층 값 계산
z_21 = X @ beta_1  # 행렬 곱
print(z_21)

# 활성화 함수 통과 (시그모이드 함수)
a_21 = torch.sigmoid(z_21)
print(a_21)

# 출력층 가중치 (gamma_1)
gamma_1 = torch.tensor([[0.7]], dtype=torch.float64)  # 상수

# 출력층 값 계산
z_31 = a_21 * gamma_1
print(z_31)

# 최종 출력 (시그모이드 함수)
y_hat = torch.sigmoid(z_31)
print(y_hat)
```

즉, 우리가 생각하는 표본은 빨간색 노드에 도착하기 위해서 두번째 은닉층의 첫번째 노드를 통과하여 올 수 있다. 하지만 빨간색 노드에는 방금 우리가 생각한 경로 뿐만 아니라 두 개의 선택지가 더 존재한다.

### 1개의 표본, 경로 한꺼번에 생각하기

세가지의 경로를 모두 생각해보면, 우리의 표본은 다음의 경로를 통해서 도착한다.

```         
> 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 세가지 경로를 타고 어떻게 하나로 합쳐지는지 이해해보자. 
```

```{r neuralnet-allpath, echo=FALSE, fig.cap="3가지 경로", fig.align='center', out.width = '100%'}
knitr::include_graphics("./image/neuralnet3.png")
```

이 과정을 우리가 통계 시간에 배운 회귀분석에 연결지어 생각해보면, 다음의 해석이 가능하다. 두번째 은닉층의 각각의 노드들이 하나의 회귀분석 예측 모델들이라고 생각하면, 신경망은 세 개의 회귀분석을 한 대 모아놓은 거대한 회귀분석 집합체라고 생각할 수 있게 된다. 즉, 각 회귀분석 모델들이 예측한 표본에 대한 대응변수 예측값들을 은닉층에 저장한 후, 그 예측값들을 모두 모아 마지막 빨간색 노드에서 합치면서 좀 더 좋은 예측값을 만들어 내는 것이다. 이 때, $\gamma$ 벡터를 통해 가중치를 부여하는 것이라고 해석이 가능하다.

이 과정을 `torch` 텐서를 사용하여 깔끔하게 나타내보자.

```{python}
import torch
import torch.nn.functional as F

# 1개 표본
# 1 by 2
X = torch.tensor([[1.0, 2.0]], dtype=torch.float64)
print(X)

# 베타벡터가 세 개 존재함
# 2 by 3
beta_1 = torch.tensor([[0.5], [0.8]], dtype=torch.float64)
beta_2 = torch.tensor([[0.3], [0.2]], dtype=torch.float64)
beta_3 = torch.tensor([[0.1], [0.4]], dtype=torch.float64)

# 정의된 베타벡터를 cbind in torch (열 방향 결합)
beta = torch.cat((beta_1, beta_2, beta_3), dim=1)
print(beta)

# 2번째 레이어 z_2
# 1 by 3
z_2 = X @ beta
print(z_2)

# 2번째 레이어 sigmoid 함수 통과
# 1 by 3
a_2 = torch.sigmoid(z_2)
print(a_2)

# 2번째 레이어에 관한 웨이트 (감마) 벡터
# 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여
# gamma vector 3 by 1
gamma = torch.tensor([[0.7], [0.5], [0.9]], dtype=torch.float64)
print(gamma)

# 3번째 레이어 z_3
# 1 by 1
z_3 = a_2 @ gamma
print(z_3)

# 마지막 레이어에서 시그모이드 함수 통과
# 1 by 1
y_hat = torch.sigmoid(z_3)
print(y_hat)
```


### 전체 표본, 경로 전체 생각해보기

이제 자료 행렬 전체를 한꺼번에 넣는 방법을 생각해보자. 입력값이 자료 행렬 전체이므로, 결과값은 이에 대응하도록 행의 갯수와 같은 벡터 형식이 될 것이라는 것을 예상하고 코드를 따라오도록 하자.

```         
> 목표: 전체 표본이 신경망을 통해서 예측되는 구조를 이해하자. 
```

```{python}
import torch
import torch.nn.functional as F

# 데이터 텐서
# 3 by 2
X = torch.tensor([[1.0, 2.0],
                  [3.0, 4.0],
                  [5.0, 6.0]], dtype=torch.float64)
print(X)

# 베타벡터가 세 개 존재함
# 2 by 3
beta = torch.tensor([[0.5, 0.3, 0.1],
                     [0.8, 0.2, 0.4]], dtype=torch.float64)
print(beta)

# 2번째 레이어 z_2
# 3 by 3
z_2 = X @ beta
print(z_2)

# 2번째 레이어 sigmoid 함수 통과
# 3 by 3
a_2 = torch.sigmoid(z_2)
print(a_2)

# 2번째 레이어에 관한 웨이트 (감마) 벡터
# gamma vector 3 by 1
gamma = torch.tensor([[0.7], [0.5], [0.9]], dtype=torch.float64)
print(gamma)

# 3번째 레이어 z_3
# 3 by 1
z_3 = a_2 @ gamma
print(z_3)

# 마지막 레이어에서 시그모이드 함수 통과
# 3 by 1
y_hat = torch.sigmoid(z_3)
print(y_hat)
```
