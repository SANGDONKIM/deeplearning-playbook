# `torch_nn` 모듈로 첫 신경망 정의하기

이제까지 `torch`의 자동미분(auto grad) 기능과 순전파(forward propagation)에 대하여 알아보았다. 오늘은 드디어, `torch` 라이브러리에서 제공하는 함수들을 이용해서 챕터 \@ref(forward) 에서 정의해본 신경망을 정의해 보도록 한다.

```{r neuralnet-example2, echo=FALSE, fig.cap="다시 두두등장! 세상 간단한 신경망", fig.align='center', out.width = '100%'}
knitr::include_graphics("./image/neuralnet1.png")
```

## 신경망 정의 (Custom nn Modules)

토치를 사용해서 신경망을 정의할 때 사용하는 함수가 있다. 바로 `nn.Module`이라는 함수인데, `torch`에서 신경망을 정의할 때, 이 함수를 사용해서 "클래스"를 만들어 정의한다! 왜 우리가 챕터 \@ref(class)에서 클래스 내용을 그렇게도 공부했었는지에 대한 답을 바로 이 챕터에서 찾을 수 있을 것이다.

### `nn.Module`과 클래스 상속

`nn.Module`이 어떤 역할을 하는지에 대하여 알아보기 위해 가장 간단한 신경망을 작성해보도록 하자. 바로 우리가 앞서 살펴본 2단 레이어 네트워크 예제에서 사용한 데이터를 만들어 보자.

```{python}
# PyTorch로 첫 신경망 정의하기
import torch
import torch.nn as nn
import torch.nn.functional as F

# 데이터 정의
X = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print("Input tensor X:\n", X)
```

먼저, `TwoLayerNet`이라는 이름의 신경망 클래스를 정의한다(기억하시나? 클래스의 이름은 카멜 형식이다!). `nn.Module()` 함수는 클래스를 정의하는 함수인데, 이 함수를 사용해서 만들어진 클래스는 자동으로 신경망과 관련한 클래스인 `basic-nn-module` 클래스를 상속하게 만든다. 즉, `nn.Module`안에는 신경망 관련 클래스들 속에는 신경망과 관련한 많은 함수가 정의되어 있을 것이고, 이것을 다 상속받아서 클래스가 만들어지는 것이다. 다음의 코드는 위의 신경망을 정의한 코드이다.

```{python}
# TwoLayerNet 신경망 정의
class TwoLayerNet(nn.Module):
    def __init__(self, data_in, hidden, data_out):
        super(TwoLayerNet, self).__init__()
        print("Initiation complete!")
        
        self.hidden_layer = nn.Linear(data_in, hidden, bias=False)
        self.output_layer = nn.Linear(hidden, data_out, bias=False)
        
# 모델 생성
myfirst_model = TwoLayerNet(2, 3, 1)
print(myfirst_model)
```

결과를 살펴보면 `TwoLayerNet` 클래스에 의하여 만들어진 `myfirst_model`는 두 개의 층이 들어있는 것을 확인할 수 있다. 이 두개 층에 관련한 모수 갯수를 그림과 한번 연결 시켜보면 잘 정의가 되어있다는 것을 알 수 있다.

* hidden_layer: 그림에서 첫번째와 두번째 층을 연결하는 다리가 6개라는 것을 주목하자. 모수의 갯수는 그래서 6개!
* output_layer: 그림에서 두번째와 마지막 층을 연결하는 다리는 3개이므로, 모수의 갯수는 3개가 된다.

## `nn.Linear` 클래스

`nn.Linear`의 입력값은 입력변수의 갯수, 출력변수의 갯수, 그리고 bias 항의 유무를 나타내는 옵션 이렇게 세개가 된다. 예제의 경우, 데이터 텐서 $X$의 features 갯수가 2개이므로, 히든 레이어의 입력값 갯수가 2개가 되어야 한다. 또한 히든 레이어의 노드 갯수가 3개이므로 결과 행력의 features 갯수가 3개가 되어야 한다. 

### bias 없는 경우

우리가 예전에 다루었던 예제에서는 `bias` 항이 없었으므로, `bias=False`를 해주어야 함에 주의하자.

```{python}
# nn.Linear 사용: Bias 없는 경우
mat_op = nn.Linear(2, 3, bias=False)
print("Weights:\n", mat_op.weight)
```


`mat_op`을 nn.Linear(2, 3) 클래스로 만들어진 클래스 생성자로 이해 할 수 있다. 그리고 이것의 수학적 의미는 행렬 연산으로 이해할 수 있겠다. `mat_op`가 생성될 때 임의의 `weight` 텐서, $W$, 와 `bias`, $b$,가 생성이 되고, 입력값으로 들어오는 `X`에 대하여 다음의 연산을 수행한 후 결괏값을 내보낸다.

$$
y = X\beta = XW^T
$$

결과를 코드로 확인해보자.

```{python}
# 행렬 연산 확인
print("Matrix multiplication output:\n", X @ mat_op.weight.T)

# nn.Linear 사용
print("nn.Linear output:\n", mat_op(X))
```


### bias 있는 경우

`bias=True`를 해주면 `weight` 텐서 $W$와 더불어 bias 텐서가 생성이 된다.

```{python}
# nn.Linear 사용: Bias 있는 경우
mat_op2 = nn.Linear(2, 3, bias=True)
print("Weights:\n", mat_op2.weight)
print("Bias:\n", mat_op2.bias)
```

따라서 정의된 신경망의 연산 역시 다음과 같이 바뀐다.

$$
y = X\beta + b = XW^T + b
$$

```{python}
# 행렬 연산 확인
print("Matrix multiplication with bias:\n", X @ mat_op2.weight.T + mat_op2.bias)

# nn.Linear 사용
print("nn.Linear output:\n", mat_op2(X))
```


## 순전파(Forward propagation) 정의

`torch`를 공부하면서 신기한 걸 많이 배우고 있다. 그 중 한가지가 바로 객체지향 프로그래밍을 사용해서 신경망을 정의한다는 것이다. 앞선 예제를 이어가보면, 우리는 신경망의 순전파를 구현해야 한다.

순전파의 경우 다음과 같이 `forward` 멤버 함수를 정의해서 구현할 수 있다.

```{python}
# TwoLayerNet 클래스 수정: 순전파 정의 추가
class TwoLayerNet(nn.Module):
    def __init__(self, data_in, hidden, data_out):
        super(TwoLayerNet, self).__init__()
        print("Initiation complete!")
        
        self.hidden_layer = nn.Linear(data_in, hidden, bias=False)
        self.output_layer = nn.Linear(hidden, data_out, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    # 순전파 정의
    def forward(self, X):
        z1 = self.hidden_layer(X)
        a1 = self.sigmoid(z1)
        z2 = self.output_layer(a1)
        y_hat = self.sigmoid(z2)
        return y_hat

# 입력과 출력 차원 정의
D_in, H, D_out = 2, 3, 1

# 모델 생성
my_net = TwoLayerNet(D_in, H, D_out)

# 순전파 수행
output = my_net(X)
print("Output of forward propagation:\n", output)
```

새로 정의된 `TwoLayerNet` 클래스에는 \@ref(fig:neuralnet-example2)의 2단 신경망의 순전파(forward propagation)가 구현된 멤버함수 `forward`가 정의되어 있다. 이 함수는 입력 텐서 `X`가 신경망으로 들어오게 되면, 은닉층(hidden_layer) $\rightarrow$ 활성함수 (activation function; 여기서는 nn_sigmoid 함수) $\rightarrow$ 출력층(output_layer) $\rightarrow$ 활성함수 순으로 내보내게 된다.


