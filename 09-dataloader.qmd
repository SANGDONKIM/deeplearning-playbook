# `Dataset`과 `Dataloader` 클래스

이번 챕터에서는 실제 딥러닝을 사용하여 모델링을 할 때 사용될 데이터를 어떻게 `torch`에 넣을수 있는지에 대하여 알아보자. 이 과정에서 우리가 알아야하는 클래스가 두개가 있는데, 바로 `Dataset` 클래스와 `Dataloader` 클래스이다.

## 예제 데이터

언제나 그렇듯, 본 공략집은 예제를 통해서 설명하는 것을 선호한다. 이번 챕터에서는 가상의 학생들의 공부시간과 연습한 문제 갯수, 그리고 시험의 합격 여부에 대한 자료를 만들어보았다.

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# 데이터 생성
np.random.seed(2021)

# 독립변수 생성
x1 = np.concatenate([np.random.normal(5, 2, 300), np.random.normal(10, 2, 200)])
x2 = np.maximum(np.random.normal(10, 2, 300), 0).astype(int)
x2 = np.concatenate([x2, np.maximum(np.random.normal(13, 2, 200), 0).astype(int)])

# 종속변수 생성
y = np.concatenate([np.zeros(300), np.ones(200)])

# 데이터프레임 생성
data = pd.DataFrame({'study_time': x1, 'n_question': x2, 'pass_exam': y})
data = data.sample(frac=1, random_state=2021).reset_index(drop=True)

```


### 데이터 나누기

이전 챕터에서 우리는 전체 데이터를 모두 사용하여 신경망을 학습했었다. 하지만, 이럴 경우 과적합의 문제가 발생하기 때문에 언제 학습을 할 때, 우리 모델이 새로운 데이터에 얼마나 잘 되어있는지, 혹시 학습이 과적합은 일어나고 있는게 아닌지 판단해야 한다. 이런 것들을 하기 위해서 주어진 데이터를 두 개로 쪼갠다; 하나는 학습용(train data set), 하나는 평가용(test data set)으로 나눈다.

다음의 코드는 주어진 `study_data`의 70%는 학습용, 30%는 평가용으로 나누는 코드이다.

```{python}
from sklearn.model_selection import train_test_split
# 데이터 나누기 (train 70%, test 30%)
train_data, test_data = train_test_split(data, test_size=0.3, random_state=2021)

print(f"Train data shape: {train_data.shape}")
print(f"Test data shape: {test_data.shape}")
```

### 시각화

학습용 데이터를 사용하여 데이터를 시각화보면 다음과 같다.

```{python echo=FALSE, fig.align="center", fig.cap="`study_data` 학습용 데이터(train data) 시각화"}
import matplotlib.pyplot as plt
import seaborn as sns

# 학습용 데이터 시각화  
plt.figure(figsize=(8, 6))
sns.scatterplot(
    data=train_data,
    x="study_time",
    y="n_question",
    hue="pass_exam",
    palette={0: "red", 1: "blue"},
    alpha=0.7
)

plt.xlabel("Study Time")
plt.ylabel("Number of Questions")
plt.title("Train Data Visualization")
plt.legend(title="Exam", labels=["Fail", "Success"], loc="upper left")
plt.grid(alpha=0.3)
plt.show()
```

## `Dataset` 클래스

`Dataset` 클래스는 우리가 가지고 있는 데이터를 `torch`에서 접근할 때 어떻게 접근을 해야하는지 알려줄때 사용한다. 예를 들어, 예제 데이터와 같이 행과 열로 구성된 데이터에서 어떤 열이 독립변수(independant variables)를 의미하고 있는지, 어떤 열이 종속변수(dependant variable)를 의미하는지 알려줘야하고, 만약 모델에 넣기 전, 특정 전처리가 필요하다면, 그 과정 역시 넣어줄 수 있다.

다음은 `dataset` 함수를 사용하여 `StudyDataset` 클래스 생성자를 정의하는 코드이다. 일단 `StudyDataset` 생성자는 객체를 만들때, `__init__`가 실행된다. 이것을 통하여 결과값이 클래스의 `x`, `y`로 저장된다.

```{python}
from torch.utils.data import Dataset, DataLoader

class StudyDataset(Dataset):
    def __init__(self, data):
        # 데이터를 Tensor로 변환
        self.x = torch.tensor(data[["study_time", "n_question"]].values, dtype=torch.float32)
        self.y = torch.tensor(data["pass_exam"].values, dtype=torch.float32).view(-1, 1)
    
    def __len__(self):
        return len(self.x)
    
    def __getitem__(self, index):
        return self.x[index], self.y[index]

# Dataset 객체 생성
train_dataset = StudyDataset(train_data)
test_dataset = StudyDataset(test_data)

test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
```


위의 코드를 살펴보면 앞에서 정의한 `data` 데이터셋을 가져온다. 그리고, `__getitem__()`과 `__len__()`의 멤버함수를 통하여 데이터에 접근할 수 있도록 만들어져 있다. 정의된 `StudyDataset()` 클래스 생성자를 통하여 데이터를 만들어보자.

```{python}
# Dataset 클래스 생성 및 객체 생성
study_dataset = StudyDataset(train_data)
# Dataset 내용 확인
print(study_dataset)
```

```{python}
# 특정 인덱스 데이터 확인
for i in range(6):
    x, y = study_dataset[i]
    print(f"Index {i}: x = {x.numpy()}, y = {y.numpy()}")
    
# 원래 train_data 확인
print(train_data.head(6))
```

위 코드를 보면 study_dataset[i]를 통해 특정 인덱스 i의 데이터를 확인합니다. 그 다음 numpy()를 사용하여 PyTorch 텐서를 numpy 배열로 변환하여 값을 출력합니다. 마지막으로, 원래의 train_data 데이터프레임과 비교하여 데이터가 잘 전달되었는지 확인합니다.

## 데이터로더 (Dataloader) 클래스

`study_dataset()` 생성자를 통하여 데이터를 `torch` 상에서 접근하도록 만들었다. 앞선 신경망 학습 예제에서 신경망을 학습할 때, 주어진 데이터 전체를 사용하여 미분값(gradient)을 구했다. 하지만, 실제 많은 딥러닝 문제의 경우 데이터의 크기가 너무 커서 한꺼번에 모든 표본을 메모리에 올린 후 학습을 하지 않고, 잘게 쪼갠 여러 개의 배치(batch)를 만든 뒤에 학습을 진행한다.

다음 차례는 torch에서 신경망을 학습시킬때 데이터의 부분부분을 잘라서 접근 할 수 있도록 만들어 줘야하는데, 이 부분은 `dataloader` 클래스에서 담당하고 있다. 다음의 코드는 앞에서 정의한 `torch_data`를 학습할 때 한번에 불러오는 표본 갯수(batch_size)를 8개로 설정한다.

```{python}
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
```

8개를 기준으로 한 세트를 이루므로, 전체 350개의 표본은 총 44개의 `batch`로 이루어져 있다는 것을 확인 할 수 있다. 

```{block, type='rmdwarning'}
### 주의하기
마지막 배치의 경우는 8개가 아닌 6개의 표본들로 이루어져 있다는 것도 짐작할 수 있어야 한다.
```

```{python}
print(train_loader.__len__())
```


## 모델 설정 - 로지스틱 회귀모형

자료를 `torch`에 보내고, 어떻게 접근하는지까지 알아보았다. 이번에는 분류 문제를 푸는 통계 모델 중에서 가장 유명한 모델인 로지스틱 회귀모형을 `torch`로 정의해보자. 로지스틱 회귀모형은 일반화 선형모형(GLM)의 한 종류이다. 이름에서도 느껴지겠지만, 이 모형은 선형모형의 연장선에 있다. 왜 연장선 상에 있다고 하는 것일까?

일반적인 회귀모형에서는 종속변수인 $Y$값이 정규분포를 따른다고 가정한다. 왜냐하면 데이터의 발생이 다음과 같은 가정에서 출발하기 때문이다.

$$
Y = X \beta + \epsilon,  \quad \epsilon \sim \mathcal{N}(0, \sigma^2I)
$$

이것을 $Y$의 입장에서 생각해보면, 결국 $Y$라는 확률변수는 정규분포를 따르고, $X$가 정해졌을때의 평균값은 $X\beta$가 된다.

$$
Y \sim \mathcal{N}(X\beta, \sigma^2 I)
$$
따라서, 다음의 관계가 성립하게 된다.

$$
\mathbb{E}[Y|X] = X\beta = f(X\beta)
$$

위의 등식에서 $f(x)$는 $f(x)=x$인 항등함수(identity function)를 나타낸다. 즉, 일반적인 회귀모형의 경우, 종속변수 $Y$를 정규분포를 따르는 확률변수로 생각하고 있고, 그 평균과 $X\beta$를 항등함수로 이어놓은 형태인 것이다. 만약 우리가 종속변수를 다른 확률변수라고 생각하고, 그것의 평균을 이어주는 어떤 함수 $f$를 찾는다면 어떨까?

수리 통계학에서 가장 먼저 배우는 함수 중 하나가 바로 0과 1을 갖는 베르누이 확률변수인데, 베르누이 확률변수의 평균은 바로 1이 나오는 확률을 의미하는 $p$이다. $p$값이 그 의미상 0과 1사이에 위치해야 하므로, $X\beta$에서 나오는 값들을 0과 1사이로 모아줘야 하는데 여기서 시그모이드(sigmoid) 함수, $\sigma(x)$,를 사용한다.

$$
f(x) := \sigma(x) = \frac{e^x}{1+e^x}
$$

따라서, 로지스틱 회귀모형에서 종속변수 $Y$는 다음과 같이 모수 $\sigma(X\beta)$인 베르누이 확률변수를 따른다고 생각하면 된다.

$$
Y \sim Bernulli(\sigma(X\beta))
$$

```{block logistic, type='rmdnote'}

### 알아두기

로지스틱 회귀모형은 종속변수를 베르누이 확률변수로 가정하고, 그것의 평균인 $p$와 $X\beta$를 시그모이드(sigmoid) 함수로 이어놓은 형태로 볼 수 있다.

```


문제는 왜 그럼 로지스틱 회귀모형인가 하는건데, [사실은 시그모이드(sigmoid) 함수가 바로 로지스틱 함수이기 때문이다.](https://en.wikipedia.org/wiki/Sigmoid_function) 좀 더 엄밀하게 말하면 우리가 알고 있는 시그모이드 함수의 정확한 명칭은 로지스틱 함수이고, 시그모이드 함수는 S 자 곡선 형태를 띄는 함수들을 통칭해서 부르는 말이다. 우리가 딥러닝에서 많이 쓰는 활성함수(activation function)중 하나인 `Hyperbolic tangent` 역시 시그모이드 함수이다.

```{block, type='rmdwarning'}
### 주의하기
우리가 알고 있는 시그모이드(sigmoid) 함수의 정확한 명칭은 로지스틱(logistic) 함수이다.
```

### torch 코드 구현

이제 `torch`로 특정 신경망 구조를 구현하는 코드는 익숙해졌으리라고 생각한다. 로지스틱 회귀모형은 단층 모형이고, 마지막 활성함수를 `sigmoid()` 함수로 감싸줘야하는 것이 특징이다.

```{python}
import torch.nn as nn

class LogisticRegressionModel(nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.fc = nn.Linear(2, 1)  # 입력 변수 2개, 출력 변수 1개
        self.sigmoid = nn.Sigmoid()  # 시그모이드 활성화 함수

    def forward(self, x):
        return self.sigmoid(self.fc(x))

# 모델 초기화
model = LogisticRegressionModel()
print(model)
```


코드에서 볼 수 있다시피, 로지스틱 회귀모형은 입력값을 독립변수 갯수인 2개로 받고, 출력값은 하나로 나가는 모형이다. 마지막 층에 `sigmoid()` 함수는 마지막 출력값을 0과 1사이로 보내기 위하여 사용되었다. 최적화 알고리즘은 `optim.SGD`으로 설정하였다. 

```{python}
# 최적화 함수 (Stochastic Gradient Descent)
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)
```


### 손실함수 설정

로지스틱 회귀모형의 구현에서 핵심 파트는 손실함수(loss function)를 설정하는 부분이다. 앞에서 로지스틱 회귀모형이 종속변수 $Y$를 베르누이 확률변수(Bernoulli random variable)로 모델링을 한다는 것을 살펴보았다.

로지스틱 회귀분석의 계수를 구하기 위해서는 우도함수(Likelihood function)를 정의한 후, 그것을 최대로 만드는 최대우도 추정량(Maximum likelihood estimator; MLE) 값을 찾아야 한다.

확률변서 $Y$가 베르누이 확률변수를 따를 때, 확률질량함수(p.m.f)는 다음과 같다.

$$
f_Y(y; p) = p^{y}(1-p)^{1-y}, \text{ for }y = 1, 0, \text{ and } 0 \le p \le 1.
$$
따라서, 로지스틱 회귀모형의 가정을 위의 확률질량함수와 같이 생각해보면, 주어진 데이터에 대한 우도함수 $p$는 다음과 같다.

$$
\begin{align}
p\left(\beta|\mathbf{X}, \underline{y}\right) & =\prod_{i=1}^{n}p\left(\beta|\mathbf{x}_{i},y_{i}\right)\\
 & =\prod_{i=1}^{n}\sigma\left(\mathbf{x}_{i}^{T}\beta\right)^{y_{i}}\left(1-\sigma\left(\mathbf{x}_{i}^{T}\beta\right)\right)^{1-y_{i}}\\
 & =\prod_{i=1}^{n}\pi_{i}^{y_{i}}\left(1-\pi_{i}\right)^{1-y_{i}}
\end{align}
$$
위의 수식에서 계산의 편의를 위하여 $\pi_i$를 사용하여 다음의 항을 간단히 표현했음에 주의한다.

$$
\pi_{i}:=\sigma\left(\mathbf{x}_{i}^{T}\beta\right)
$$

보통의 최적화 알고리즘의 경우, 손실함수 값을 최소로 만드는 값을 찾는 알고리즘이다. 하지만 MLE의 경우 주어진 우도함수를 최대로 만드는 값이기 때문에 최적화 알고리즘에 사용할 수 있도록 음수값을 붙여주고, 함수를 좀 더 완만하게 만들기 위해서 로그값을 취해준, 음우도함수(negative log-likelihood function)을 사용한다.

$$
\begin{align*}
-\ell\left(\beta\right) & =-log\left(p\left(\underline{y}|\mathbf{X},\beta\right)\right)\\
 & =-\sum_{i=1}^{n}\left\{ y_{i}log\left(\sigma\left(\mathbf{x}_{i}^{T}\beta\right)\right)+\left(1-y_{i}\right)log\left(1-\sigma\left(\mathbf{x}_{i}^{T}\beta\right)\right)\right\} \\
 & =-\sum_{i=1}^{n}\left\{ y_{i}log\left(\pi_{i}\right)+\left(1-y_{i}\right)log\left(1-\pi_{i}\right)\right\}
\end{align*}
$$

위의 함수 $-\ell(\cdot)$은 `torch`에서 `nnf_binary_cross_entropy()` 함수에 정의되어 있다. 따라서 로지스틱 회귀모형의 계수는 다음과 같이 데이터 행렬($X$)과 레이블($y$)가 주어졌을때 $-\ell(\cdot)$ 함수를 최소로 만드는 $\beta$값으로 표현된다.

$$
\hat{\beta} \overset{set}{=} \underset{\beta}{arg \ min} \ \ -\ell\left(\beta; X, y\right)
$$
이 값을 찾기 위해서 경사하강법을 이용해 $\hat{\beta}$을 찾아나아가는 과정이 `torch`에서는 단 두 줄로 표현이 된다.

```{python}
# 손실 함수 (Binary Cross-Entropy Loss)
criterion = nn.BCELoss()
```

앞에서 정의한 데이터로더를 사용하여 로지스틱 회귀모형을 학습시키는 코드는 다음과 같다.

```{python}
# 학습 함수
def train_model(model, train_loader, criterion, optimizer, epochs=1000):
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0
        for x_batch, y_batch in train_loader:
            optimizer.zero_grad()  # 그래디언트 초기화
            y_pred = model(x_batch)  # 예측값 계산
            loss = criterion(y_pred, y_batch)  # 손실 계산
            loss.backward()  # 역전파
            optimizer.step()  # 가중치 업데이트
            epoch_loss += loss.item()
        if (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}")

# 모델 학습 실행
train_model(model, train_loader, criterion, optimizer, epochs=1000)
```


## 학습 결과

```{python}
# 평가 함수
def evaluate_model(model, test_loader):
    model.eval()
    predictions, actuals = [], []
    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            y_pred = model(x_batch)
            predictions.extend(y_pred.squeeze().numpy())
            actuals.extend(y_batch.squeeze().numpy())
    return np.array(predictions), np.array(actuals)

# 평가 실행
predictions, actuals = evaluate_model(model, test_loader)
```

학습된 로지스틱 회귀모형의 계수값을 사용하여 평가셋의 반응 변수가 1일 확률, 즉, 시험에 통과할 확률을 예측 할 수 있다.

예측한 확률값이 0.5가 넘을 경우, 학생이 시험에 통과를 할 수 있다고 예측을 하고, 그렇지 않을 경우 통과하지 못한다고 예측해보자.

```{python}
# 예측값 이진화
predicted_classes = (predictions >= 0.5).astype(int)
```

이렇게 예측한 값과 실제 평가셋에 들어있는 학생들의 시험 통과 여부값을 사용하여 결과 비교하여 표로 만들어보면 다음과 같다.

```{python}
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# 혼동 행렬 계산
cm = confusion_matrix(actuals, predicted_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fail", "Pass"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()
```


아래 그림에서처럼 로지스틱 함수는 입력값이 0일때 함수값이 0.5가 된다. 즉, 로지스틱 회귀에서 $X\beta$값이 로지스틱 함수에 입력이 되므로, $X\beta$ 값이 0이 되는 값들을 경계로 모델의 합격 불합격 예측값이 갈리는 것이다. 

```{python}
import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성
x = np.arange(-5, 5.1, 0.1)
y = 1 / (1 + np.exp(-x))  # 시그모이드 함수

# 시각화
plt.figure(figsize=(8, 6))
plt.plot(x, y, label="Sigmoid Function")
plt.axhline(0.5, color="red", linestyle="--", label="y = 0.5")
plt.axvline(0, color="red", linestyle="--", label="x = 0")
plt.axhline(0, color="black")
plt.axvline(0, color="black")
plt.text(-3, 0.55, "y = 0.5", color="red", fontsize=10)
plt.text(1.5, 0.05, "x = 0", color="red", fontsize=10)
plt.xlabel("x")
plt.ylabel("Sigmoid(x)")
plt.title("Sigmoid Function with Decision Boundary")
plt.legend()
plt.grid()
plt.show()
```


이러한 선을 의사결정선(decision boundary)라고 부른다.

$$
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2  = 0
$$

주어진 예제의 평가셋을 시각화 시키고, 학습한 계수를 바탕으로 의사결정선을 구해보면 다음과 같다.

```{python}
# 학습된 계수와 절편
learned_beta = model.fc.weight.detach().numpy().flatten()
learned_bias = model.fc.bias.detach().numpy().item()

# 의사결정선 계산 함수
def decision_boundary(beta, bias, x_vals):
    slope = -beta[0] / beta[1]
    intercept = -bias / beta[1]
    return slope * x_vals + intercept

# 평가 데이터 시각화 및 의사결정선 추가
plt.figure(figsize=(8, 6))
sns.scatterplot(
    data=test_data,
    x="study_time",
    y="n_question",
    hue="pass_exam",
    palette={0: "red", 1: "blue"},
    alpha=0.7,
    legend="full"
)

# 의사결정선 추가
x_vals = np.linspace(test_data["study_time"].min(), test_data["study_time"].max(), 100)
y_vals = decision_boundary(learned_beta, learned_bias, x_vals)
plt.plot(x_vals, y_vals, color="black", linestyle="--", label="Decision Boundary")

# 설정
plt.xlabel("Study Time")
plt.ylabel("Number of Questions")
plt.title("Test Data with Decision Boundary")
plt.legend(title="Exam Result", labels=["Fail", "Pass", "Decision Boundary"], loc="upper left")
plt.grid(alpha=0.3)
plt.show()
```


