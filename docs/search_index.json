[["index.html", "딥러닝 공략집 with R 들어가며 설치하기 기본 패키지", " 딥러닝 공략집 with R 슬기로운통계생활 2021-02-12 들어가며 Figure 0.1: deep-learning playbook 이제까지 R에서의 딥러닝은 Python의 라이브러리들을 reticulate 패키지를 이용하여 빌려온 형태였지만, torch for R 패키지는 C 라이브러리를 Torch를 기반으로 R을 wrapper 언어로서 사용하여 패키지를 만들었다. 즉, Torch + Python = PyTorch, Torch + R = Rtorch가 되는 셈이다. 설치하기 설치 역시 간단한다. 여느 R패키지와 같이 install.packages() 함수를 사용하면 된다. 서브 라이브러리인 torchaudio와 torchvision이 있으나, 책의 뒷부분에서 다루기로 한다. install.packages(&quot;torch&quot;) # 혹은 개발버전을 다운 받고 싶다면 # devtools::install_github(&quot;mlverse/torch&quot;) 기본 패키지 앞으로의 내용에 있어서 다음의 두 패키지는 기본으로 불러와서 사용하는 것을 약속으로 한다. library(tidyverse) library(torch) "],["intro.html", "Chapter 1 딥러닝 첫걸음, 텐서 (tensor) 만들기 1.1 torch와의 첫만남 1.2 텐서 (tensor) 만들기 1.3 고급기술: 영리하게 만들기 1.4 텐서와 행렬은 같을까?", " Chapter 1 딥러닝 첫걸음, 텐서 (tensor) 만들기 1.1 torch와의 첫만남 torch패키지를 설치했으니, 한번 만나봐야한다. 다음의 명령어를 통하여 torch를 불러보자. library(torch) 1.2 텐서 (tensor) 만들기 텐서가 무엇이냐! 무언가 대단한 것처럼 보이나, 결국 우리가 R을 배웠을때 사용했던 matrix의 개념을 확장시킨 것이라고 생각하면 된다. 결국 다차원 행렬, 혹은 Array인 것이다. 우리가 많이 쓰는 행렬도 Array에 속하지만, 보통 Array라는 용어는 3차원 이상의 행렬을 암시한다. 이름부터 멋있는 딥러닝인데 다른 용어들이 Array같이 다른 패키지에서 사용되는 것들이랑 동일하면 격이 떨어지므로, 텐서 (tensor) 라고 붙였다. 토치 설명서에 따르면 텐서는 R의 Array와 비슷하나, GPU 계산에도 쓸 수 있다고 나와있다. (응, 그냥 Array.) 또한, 프로그래밍 언어에서 어떤 변수를 만들때, 만든다고 하지않고, “선언한다\" 라고 한다. 따라서 앞으로 만든다는 말 대신”선언\"이라는 용어를 사용하겠다. 1.2.1 빈 텐서 만들기 속이 빈 5행 3열의 텐서은 다음과 같이 선언한다. 주의할 점은 우리가 이번에 만들 빈 (empty) 텐서 과 뒤에서 만들어 볼 0 텐서는 다르다; 빈 텐서은 0과 근접한 쓰레기값이 들어있는 반면에, 0 텐서에는 정말 0이 들어있다. x &lt;- torch_empty(5, 3) # 텐서 x값 확인 x ## torch_tensor ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## [ CPUFloatType{5,3} ] # 텐서 x의 크기 확인 dim(x) ## [1] 5 3 정말 5행 3열의 텐서가 만들어졌다. 이건 마치 우리가 R을 처음 시작하고 텐서을 만드는 것과 아주 유사해서, 실제 R을 만져본 사람이라면 뒤로 빨리 넘기고 싶어하는 욕구가 솓구칠 것이다. 결과값을 잘 살펴보자. CPUFloatType에서 우리는 현재 만든 rand_tensor는 CPU에서 접근이 가능하며, 실수 (float) 타입이라는 것을 이야기해준다. 1.2.2 랜덤 텐서 텐서의 각 자리에 0에서 1사이의 난수로 채워서 만드는 방법이다. torch_rand() 함수를 사용한다. rand_tensor &lt;- torch_rand(5, 3) rand_tensor ## torch_tensor ## 0.6406 0.7781 0.4124 ## 0.2323 0.4230 0.5581 ## 0.5231 0.1425 0.4484 ## 0.4961 0.3128 0.1683 ## 0.4265 0.2016 0.0520 ## [ CPUFloatType{5,3} ] 참고로 이렇게 만들어진 텐서에는 R에서 텐서과 어레이(array)에 접근할 때사용한 모든 문법들을 사용해서 접근할 수 있다. rand_tensor[,2] ## torch_tensor ## 0.7781 ## 0.4230 ## 0.1425 ## 0.3128 ## 0.2016 ## [ CPUFloatType{5} ] rand_tensor[1:3,] ## torch_tensor ## 0.6406 0.7781 0.4124 ## 0.2323 0.4230 0.5581 ## 0.5231 0.1425 0.4484 ## [ CPUFloatType{3,3} ] rand_tensor[3:4,c(1, 3)] ## torch_tensor ## 0.5231 0.4484 ## 0.4961 0.1683 ## [ CPUFloatType{2,2} ] 위의 예제에서 벌써부터 일부 R유저들은 감격의 눈물을 흘릴 수 있다. 그렇다, Rtorch에서 텐서의 첫번째 위치는 1부터 시작한다. 이 너무나도 당연한 진리는 파이썬에서는 통하지 않는다. 1.2.3 단위 텐서 4행 4열의 단위 텐서 (identity matrix)를 선언하는 방법은 다음과 같다. x &lt;- torch_eye(4) x ## torch_tensor ## 1 0 0 0 ## 0 1 0 0 ## 0 0 1 0 ## 0 0 0 1 ## [ CPUFloatType{4,4} ] 1.2.4 영(0) 텐서 텐서의 요소들이 모두 0으로 채워진 3행 5열의 텐서을 선언하는 것은 다음과 같이 torch_zeros() 함수를 사용한다. x &lt;- torch_zeros(3, 5) x ## torch_tensor ## 0 0 0 0 0 ## 0 0 0 0 0 ## 0 0 0 0 0 ## [ CPUFloatType{3,5} ] 1.3 고급기술: 영리하게 만들기 지금까지는 미리 정해진 값들, 난수나, 0과 1을 채워넣는 법을 배웠다. 하지만, 많은 경우 우리가 직접 정의한 텐서들을 다루게 될 것이다. 이번 섹션에서는 좀 더 영리하게 선언해보는 방법을 배워보자. 1.3.1 텐서 직접선언 가장 핵심적인 내용은 R에서 벡터와 행렬을 정의한 후 torch_tensor() 함수에 넣어주면, 그대로 가져다가 텐서로 바꿔준다는 사실이다. 다음의 예제는 2행 2열의 행렬을 정의한 후, 정의된 행렬을 사용하여 텐서를 만드는 코드이다. y &lt;- torch_tensor(matrix(c(1, 2, 3, 4, 5, 6), ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUFloatType{3,2} ] 1.3.2 : 연산자 사용 위의 코드가 잘 작동한다는 사실을 알게 되면, 우리가 너무나 익숙한 R의 기본 함수들을 사용하여 텐서를 자유롭게 만들 수 있을 것이다. 앞선 예제는 : 연산자를 통하여 다음과 같이 축약 할 수 있다. y &lt;- torch_tensor(matrix(1:6, ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPULongType{3,2} ] 1.3.3 seq() 함수 사용 seq() 함수는 좀 더 유연한 벡터를 만들 수 있도록 해주므로, 텐서를 만들때 유용하게 사용될 것이다. y &lt;- torch_tensor(matrix(seq(0.1, 1, by = 0.1), ncol = 2)) y ## torch_tensor ## 0.1000 0.6000 ## 0.2000 0.7000 ## 0.3000 0.8000 ## 0.4000 0.9000 ## 0.5000 1.0000 ## [ CPUFloatType{5,2} ] 위의 코드는 seq() 함수를 사용해서 벡터를 만들고, 2열을 갖는 행렬을 만든 후, 텐서로 변환을 시켰다. 단, by 옵션의 경우, 결과값이 홀수인지 짝수인지 체크해줘야 하므로, 특정 범위에서의 일정 간격 숫자를 뽑아 행렬로 만들땐 length.out 옵션이 편하다. y &lt;- torch_tensor(matrix(seq(0, 1, length.out = 10), ncol = 2)) y ## torch_tensor ## 0.0000 0.5556 ## 0.1111 0.6667 ## 0.2222 0.7778 ## 0.3333 0.8889 ## 0.4444 1.0000 ## [ CPUFloatType{5,2} ] 둘 다 0과 1사이의 벡터를 만들었지만, 결과는 다르다는 것에 주의하자. 텐서를 만드는 방법에 대한 핵심은 결국, 자신이 편한 방법으로 만들고 싶은 텐서와 대응되는 R 개체를 만들고, torch_tensor()에 입력 시켜주면 되는 것이다. 1.3.4 %&gt;% 연산자 사용 가끔 R에서 아주 많이 쓰이는 %&gt;% 파이프 연산자를 다른 라이브러리를 사용할 경우 적용할 생각을 못하는 경우가 있다. 왼쪽의 결과 값을 오른쪽의 입력값으로 넘겨주는 파이프 연산자 역시 torch 패키지에서 사용 가능하므로, 텐서 만드는 방법은 그야말로 무궁무진하다. library(magrittr) y2 &lt;- torch_tensor(1:5 %&gt;% diag()) y2 ## torch_tensor ## 1 0 0 0 0 ## 0 2 0 0 0 ## 0 0 3 0 0 ## 0 0 0 4 0 ## 0 0 0 0 5 ## [ CPULongType{5,5} ] 1.4 텐서와 행렬은 같을까? 앞에서 설명한 것처럼 텐서는 행렬의 개념을 확장시킨 것에 지나지 않겠지만, 그렇다고 같은 취급을 해서도 안된다. 그도 그럴것이 R에서 torch의 텐서와 행렬은 같지 않다. 이러한 사실은 다음과 같이 위에서 만든 텐서 x에 R의 기본 연산자인 행렬곱을 적용해보면 알 수 있다. x &lt;- torch_zeros(3, 5) x %*% t(x) ## Error in t.default(x): argument is not a matrix 위의 argument is not a matrix 에러에서 우리는 정말 텐서와 행렬은 다르게 취급된다는 것을 알 수 있다. 즉, R환경에서 텐서와 행렬은 근본이 다른 개체(object)라는 것을 알 수 있다. 그렇다면 ‘텐서끼리의 계산은 어떻게 할까?’ 자연스러운 의문이 든다. 다음 장에서는 텐서의 연산에 대하여 배워보자. "],["operation.html", "Chapter 2 텐서 (tensor) 연산 2.1 토치 (torch) 불러오기 및 준비물 준비 2.2 텐서의 연산", " Chapter 2 텐서 (tensor) 연산 지난 챕터에서 우리는 텐서가 행렬의 연산에 적용되는 %*%과 호환이 되지 않는 다는 것을 알게되었다. 이번 챕터에서는 텐서들의 연산에 대하여 알아보도록 하자. 2.1 토치 (torch) 불러오기 및 준비물 준비 토치 (torch) 를 불러오고, 이번 챕터에 사용될 텐서 A, B, 그리고 C를 준비하자. 지난 챕터에서 배운 난수를 이용한 텐서도 만들 예정이니 난수를 고정한다. library(torch) # 난수 생성 시드 고정 torch_manual_seed(2021) A &lt;- torch_tensor(1:6) B &lt;- torch_rand(2, 3) C &lt;- torch_rand(2, 3, 2) A; B; C ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPULongType{6} ] ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] ## torch_tensor ## (1,.,.) = ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## ## (2,.,.) = ## 0.1242 0.7489 ## 0.3608 0.5131 ## 0.2959 0.7834 ## [ CPUFloatType{2,3,2} ] 만들어진 세 개의 텐서 결과를 살펴보면 다음과 같다. 텐서 A: 정수들로 구성이 되어있고, 6개의 원소들이 벡터를 이루고 있다. 텐서 B: 실수들로 구성이 되어있고, 똑같이 6개의 원소들이 있지만, 모양이 4행 3열인 2차원 행렬의 모양을 하고 있다. 텐서 C: 실수들로 구성이 되어있고, 총 원소 갯수는 12개지만, 모양은 3행 2열의 행렬이 두개가 쌓여진 꼴의 3차원 배열 (array) 이다. 2.2 텐서의 연산 2.2.1 형(type) 변환 먼저 주목해야 할 것은 바로 텐서 A와 B의 자료형이 다르다는 것이다. 이게 무슨뜻이냐면 A에는 정수만이 담길 수 있고, B에는 실수만이 담길 수 있도록 설계가 되어있다는 것이다. 앞에서 확인한 자료형을 좀 더 명확하게 확인하기 위해서는 type() 사용한다. A$dtype ## torch_Long B$dtype ## torch_Float 텐서 A를 실수형 텐서로 바꿔보자. 텐서의 형을 변환할 때에는 A텐서 안에 속성으로 들어가있는 to() 함수를 사용 (좀 더 어려운 관점에서는 OOP의 method를 사용) 해서 바꿔줄 수 있다. A &lt;- A$to(dtype = torch_double()) A ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPUDoubleType{6} ] torch에는 정말 많은 자료형이 있는데, 그 목록은 다음을 참고하자. 2.2.2 모양 변환 앞에서 텐서 A를 B와 같은 실수를 담을 수 있는 형으로 바꾸었다. 그렇다면 이 두 개를 더할 수 있을까? 답은 “아니올시다.” 이다. 왜냐하면 모양이 다르기 때문이다. A + B ## Error in (function (self, other, alpha) : The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1 ## Exception raised from infer_size at ../aten/src/ATen/ExpandUtils.cpp:24 (most recent call first): ## frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x69 (0x7f418589fb89 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libc10.so) ## frame #1: at::infer_size(c10::ArrayRef&lt;long&gt;, c10::ArrayRef&lt;long&gt;) + 0x552 (0x7f41753ee382 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #2: at::TensorIterator::compute_shape(at::TensorIteratorConfig const&amp;) + 0xde (0x7f41758f0c2e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #3: at::TensorIterator::build(at::TensorIteratorConfig&amp;) + 0x64 (0x7f41758f31e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #4: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&amp;) + 0xdd (0x7f41758f399d in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #5: at::TensorIterator::binary_op(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;) + 0x130 (0x7f41758f3b30 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #6: at::native::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x53 (0x7f41755a6bc3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #7: &lt;unknown function&gt; + 0x13311bd (0x7f4175c0d1bd in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #8: &lt;unknown function&gt; + 0xaf2045 (0x7f41753ce045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #9: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7f4175db881f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #10: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7f4175caefd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #11: &lt;unknown function&gt; + 0x2a0f2bb (0x7f41772eb2bb in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #12: &lt;unknown function&gt; + 0xaf2045 (0x7f41753ce045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #13: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7f4175db881f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #14: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7f4175caefd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #15: _lantern_add_tensor_tensor_scalar + 0x64 (0x7f4185c210e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/liblantern.so) ## frame #16: cpp_torch_namespace_add_self_Tensor_other_Tensor(Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchScalar, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchScalar&gt;(XPtrTorchScalar*)), false&gt;) + 0x48 (0x7f4186566fe8 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #17: _torch_cpp_torch_namespace_add_self_Tensor_other_Tensor + 0x9c (0x7f41862ff00c in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #18: &lt;unknown function&gt; + 0xf9310 (0x7f4198ee8310 in /usr/lib/R/lib/libR.so) ## frame #19: &lt;unknown function&gt; + 0xf9826 (0x7f4198ee8826 in /usr/lib/R/lib/libR.so) ## frame #20: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #21: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #22: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #23: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #24: Rf_eval + 0x353 (0x7f4198f328c3 in /usr/lib/R/lib/libR.so) ## frame #25: &lt;unknown function&gt; + 0xc650d (0x7f4198eb550d in /usr/lib/R/lib/libR.so) ## frame #26: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #27: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #28: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #29: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #30: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #31: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #32: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #33: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #34: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #35: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #36: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #37: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #38: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #39: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #40: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #41: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #42: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #43: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #44: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #45: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #46: &lt;unknown function&gt; + 0x12d83b (0x7f4198f1c83b in /usr/lib/R/lib/libR.so) ## frame #47: &lt;unknown function&gt; + 0x9021b (0x7f4198e7f21b in /usr/lib/R/lib/libR.so) ## frame #48: Rf_eval + 0x706 (0x7f4198f32c76 in /usr/lib/R/lib/libR.so) ## frame #49: &lt;unknown function&gt; + 0x149782 (0x7f4198f38782 in /usr/lib/R/lib/libR.so) ## frame #50: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #51: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #52: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #53: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #54: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #55: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #56: &lt;unknown function&gt; + 0x1440ac (0x7f4198f330ac in /usr/lib/R/lib/libR.so) ## frame #57: Rf_eval + 0x454 (0x7f4198f329c4 in /usr/lib/R/lib/libR.so) ## frame #58: &lt;unknown function&gt; + 0x14a22c (0x7f4198f3922c in /usr/lib/R/lib/libR.so) ## frame #59: &lt;unknown function&gt; + 0x1871fd (0x7f4198f761fd in /usr/lib/R/lib/libR.so) ## frame #60: &lt;unknown function&gt; + 0x1353c4 (0x7f4198f243c4 in /usr/lib/R/lib/libR.so) ## frame #61: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #62: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #63: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) 모양이 다른 텐서를 더하려고 하면 R은 위에서 보듯 너무나 많은 에러를 쏟아낸다. 모양이 다른 두 텐서를 더하기 위해서는 모양을 같게 맞춰줘야 한다. A의 모양을 B의 모양과 같이 바꿔보도록 하자. 모양을 바꿀때는 view() 함수를 사용하고, 안에 모양의 형태를 벡터 형식으로 짚어 넣는다는 것을 기억하자. A &lt;- A$view(c(2, 3)) A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] 한가지 짚고 넘어가야하는 기능이 있는데, R에서 행렬을 정의할 때, 주어진 원소벡터를 넣고, 가로행과 세로열 중 하나만 입력을 해도 잘 정의가 되는 것을 기억할 것이다. view 함수 역시 비슷한 기능이 있는데, 바로 -1을 이용해서 모양을 변환시키는 방법이다. 앞선 예제에서 2행 3열이 텐서를 1행의 가로 텐서로 변환 시키려면 다음과 같이 view() 함수의 입력값을 조정할 수 있다. A$view(c(1, -1)) ## torch_tensor ## 1 2 3 4 5 6 ## [ CPUDoubleType{1,6} ] 2.2.3 덧셈과 뺄셈 앞에서 형(type)과 모양(shape)까지 맞춰놨으니, 텐서끼리의 덧셈과 뺄셈을 할 수 있다. A + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUDoubleType{2,3} ] A - B ## torch_tensor ## 0.4866 1.2574 2.2841 ## 3.4295 4.8347 5.9557 ## [ CPUDoubleType{2,3} ] 사실, 텐서끼리의 연산은 모양만 맞으면 가능하다. 즉, 다음의 연산이 성립한다. A_ &lt;- A$to(dtype = torch_long()) A_ + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUFloatType{2,3} ] 결과에서 알 수 있듯, 정수를 담을 수 있는 텐서와 실수를 담을 수 있는 텐서를 더하면, 결과는 실수를 담을 수 있는 텐서로 반환이 된다. 하지만, 필자는 이러한 코딩은 피해야 한다고 생각한다. 즉, 모든 연산을 할 경우, 명시적으로 형변환을 한 후 연산을 할 것을 권한다. 왜냐하면, 언제나 우리는 코드를 다른 사람이 보았을 때, 이해하기 쉽도록 짜는 것을 추구해야 한다. (코드는 하나의 자신의 생각을 적은 글이다.) 2.2.4 상수와의 연산 R에서와 마찬가지로, 텐서와 상수와의 사칙연산은 각 원소에 적용되는 것을 확인하자. A + 2 ## torch_tensor ## 3 4 5 ## 6 7 8 ## [ CPUDoubleType{2,3} ] B^2 ## torch_tensor ## 0.2636 0.5514 0.5125 ## 0.3254 0.0273 0.0020 ## [ CPUFloatType{2,3} ] A %/% 3 ## torch_tensor ## 0 0 1 ## 1 1 2 ## [ CPUDoubleType{2,3} ] A %% 3 ## torch_tensor ## 1 2 0 ## 1 2 0 ## [ CPUDoubleType{2,3} ] 2.2.5 제곱근과 로그 제곱근(square root)나 로그(log) 함수 역시 각 원소별 적용이 가능하다. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] torch_sqrt(A) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] 위의 연산이 에러가 나는 이유는 A가 정수를 담는 텐서였는데, 연산을 수행한 후에 실수가 담겨져서 나오는 에러이다. R과는 사뭇다른 예민한 아이 torch를 위해 형을 바꿔준 후에 연산을 실행하도록 하자. torch_sqrt(A$to(dtype = torch_double())) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] torch_log(B) ## torch_tensor ## -0.6667 -0.2977 -0.3342 ## -0.5613 -1.8002 -3.1166 ## [ CPUFloatType{2,3} ] 2.2.6 텐서의 곱셈 텐서의 곱셈 역시 모양이 맞아야 하므로, 3행 2열이 두개가 붙어있는 C에서 앞에 한장을 떼어내도록 하자. B ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] D &lt;- C[1,,] D ## torch_tensor ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## [ CPUFloatType{3,2} ] 텐서의 곱셈은 torch_matmul() 함수를 사용한다. # 파이프 사용해도 무방하다. # B %&gt;% torch_matmul(D) torch_matmul(B, D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] 토치의 텐서 곱셈은 다음과 같은 방법들도 있으니 알아두자. torch_mm(B, D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] B$mm(D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] B$matmul(D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] 2.2.7 텐서의 전치(transpose) 전치(transpose)는 주어진 텐서를 뒤집는 것인데, 다음의 문법 구조를 가지고 있다. torch_transpose(input, dim0, dim1) dim0, dim1는 바꿀 차원을 의미한다. ‘바꿀 차원은 두 개 밖에 없지 않나?’ 라고 생각할 수 있다. 2 차원 텐서의 경우에는 그렇다. 우리가 행렬을 전치하는 경우에는 transpose를 취하는 대상이 2차원이므로 지정해주는 차원이 정해져있다. 하지만, 텐서의 차원이 3차원 이상이 되면 전치를 해주는 차원을 지정해줘야한다. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] 위의 텐서 A의 차원은 행과 열, 즉, 2개이다. 다음의 코드들은 A 텐서의 첫번째 차원과 두번째 차원을 뒤집는 효과를 가져온다. 즉, 전치 텐서가 된다. torch_transpose(A, 1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] A$transpose(1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] A %&gt;% torch_transpose(1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] 3차원의 텐서를 살펴보자. C ## torch_tensor ## (1,.,.) = ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## ## (2,.,.) = ## 0.1242 0.7489 ## 0.3608 0.5131 ## 0.2959 0.7834 ## [ CPUFloatType{2,3,2} ] 텐서 C는 위와 같이 2차원 텐서가 두 개 포개져 있다고 생각하면 된다. 텐서의 결과물을 잘 살펴보면, 제일 앞에 위치한 1, 2가 나타내는 것이 우리가 흔히 생각하는 2차원 텐서들의 색인(index) 역할을 한다는 것을 알 수 있다. 앞으로는 편의를 위해서 3차원 텐서의 색인 역할을 하는 차원을 깊이(depth)라고 부르도록 하자. 앞에서 주어진 텐서 C 안의 포개져있는 2차원 텐서들을 전치하기 위해서는 이들을 관할(?)하는 두번째와 세번째 차원을 바꿔줘야 한다. torch_transpose(C, 2, 3) ## torch_tensor ## (1,.,.) = ## 0.9628 0.0992 0.0169 ## 0.2943 0.8096 0.8222 ## ## (2,.,.) = ## 0.1242 0.3608 0.2959 ## 0.7489 0.5131 0.7834 ## [ CPUFloatType{2,2,3} ] 결과를 살펴보면, 잘 바뀌어 있음을 알 수 있다. 2.2.8 R에서의 3차원 배열 앞에서 다룬 torch에서의 3차원 텐서 부분은 R에서 기본적으로 제공하는 array의 문법과 차이가 난다. 다음의 코드를 살펴보자. 먼저 R에서 2행 3열의 행렬을 두 개 포개어 놓은 3차원 배열을 만드는 코드이다. array(1:12, c(2, 3, 2)) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 필자는 참고로 matrix()를 만들때에도 byrow 옵션을 써서 만드는 것을 좋아하는데, array()에서 byrow 옵션 효과를 적용하려면 aperm() 함수를 사용해야 한다. 따라서, 좀 더 직관적으로 쓰기위해서 다음의 함수를 사용하자. array_3d_byrow &lt;- function(num_vec, nrow, ncol, ndeath){ aperm(array(num_vec, c(ncol, nrow, ndeath)), c(2, 1, 3)) } E &lt;- array_3d_byrow(1:12, 2, 3, 2) E ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 이러한 코드를 앞서 배웠던 torch_tensor() 함수에 넣어보자. E %&gt;% torch_tensor() ## torch_tensor ## (1,.,.) = ## 1 7 ## 2 8 ## 3 9 ## ## (2,.,.) = ## 4 10 ## 5 11 ## 6 12 ## [ CPULongType{2,3,2} ] 결과를 살펴보면, 우리가 예상했던 2행 3열의 텐서가 두개 겹쳐있는 텐서의 모양이 나오지 않는다는 것을 알 수 있다. 이유는 torch에서 정의된 3차원 텐서의 경우, 첫번째 차원이 텐서가 얼마나 겹쳐있는지를 나타내는 깊이(depth)를 나타내기 때문이다. 문제를 해결하기 위해서는 aperm() 사용해서 차원을 바꿔주면 된다. E %&gt;% aperm(c(3, 1, 2)) %&gt;% # 3 번째 차원을 맨 앞으로, 나머지는 그대로 torch_tensor() ## torch_tensor ## (1,.,.) = ## 1 2 3 ## 4 5 6 ## ## (2,.,.) = ## 7 8 9 ## 10 11 12 ## [ CPULongType{2,2,3} ] 위의 경우를 좀더 직관적인 함수명으로 바꿔서 사용하도록 하자. array_to_torch &lt;- function(mat, n_dim = 3){ torch_tensor(aperm(mat, c(n_dim:3, 1, 2))) } E &lt;- array_to_torch(E) E ## torch_tensor ## (1,.,.) = ## 1 2 3 ## 4 5 6 ## ## (2,.,.) = ## 7 8 9 ## 10 11 12 ## [ CPULongType{2,2,3} ] 2.2.9 다차원 텐서와 1차원 벡터 텐서의 연산 R에서 우리가 아주 애용하는 기능 중 하나가 바로 recycling 개념이다. 즉, 길이 혹은 모양이 맞지 않는 개체(object)들을 연산할 때, 자동으로 길이와 모양을 맞춰서 연산을 해주는 기능인데, torch에서도 이러한 기능을 제공한다. 다음의 코드를 살펴보자. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] A + torch_tensor(1:3) ## torch_tensor ## 2 4 6 ## 5 7 9 ## [ CPUDoubleType{2,3} ] A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] A + torch_tensor(matrix(2:3, ncol = 1)) ## torch_tensor ## 3 4 5 ## 7 8 9 ## [ CPUDoubleType{2,3} ] 2.2.10 1차원 텐서 끼리의 연산, 내적과 외적 1차원 텐서끼리의 연산도 2차원 텐서끼리의 연산과 마찬가지라고 생각하면 된다. 내적과 외적 역시 그냥 모양을 맞춰서 곱하면 된다. A_1 &lt;- A$view(c(1, -1)) A_1 ## torch_tensor ## 1 2 3 4 5 6 ## [ CPUDoubleType{1,6} ] A_2 &lt;- A$view(c(-1, 1)) A_2 ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPUDoubleType{6,1} ] A_1$mm(A_2) ## torch_tensor ## 91 ## [ CPUDoubleType{1,1} ] A_2$mm(A_1) ## torch_tensor ## 1 2 3 4 5 6 ## 2 4 6 8 10 12 ## 3 6 9 12 15 18 ## 4 8 12 16 20 24 ## 5 10 15 20 25 30 ## 6 12 18 24 30 36 ## [ CPUDoubleType{6,6} ] 한가지 주의할 점은 1차원 텐서끼리의 연산이더라도 꼭 차원을 선언해줘서 열벡터와 행벡터를 분명히 해줘야 한다는 점이다. A_3 &lt;- torch_tensor(1:6) A_1$mm(A_3) ## Error in (function (self, mat2) : mat2 must be a matrix ## Exception raised from mm_cpu at ../aten/src/ATen/native/LinearAlgebra.cpp:399 (most recent call first): ## frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x69 (0x7f418589fb89 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libc10.so) ## frame #1: at::native::mm_cpu(at::Tensor const&amp;, at::Tensor const&amp;) + 0x334 (0x7f41756ee194 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #2: &lt;unknown function&gt; + 0x133236d (0x7f4175c0e36d in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #3: &lt;unknown function&gt; + 0xaf1c34 (0x7f41753cdc34 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #4: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;) const + 0x1ce (0x7f4175db624e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #5: at::mm(at::Tensor const&amp;, at::Tensor const&amp;) + 0xb7 (0x7f4175c9c947 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #6: &lt;unknown function&gt; + 0x2a5db24 (0x7f4177339b24 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #7: &lt;unknown function&gt; + 0xaf1c34 (0x7f41753cdc34 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #8: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;) const + 0x1ce (0x7f4175db624e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #9: at::Tensor::mm(at::Tensor const&amp;) const + 0xb7 (0x7f4175f1fd67 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #10: _lantern_Tensor_mm_tensor_tensor + 0x4c (0x7f4185bdd79c in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/liblantern.so) ## frame #11: cpp_torch_method_mm_self_Tensor_mat2_Tensor(Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;) + 0x2c (0x7f418650f4fc in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #12: _torch_cpp_torch_method_mm_self_Tensor_mat2_Tensor + 0x82 (0x7f41862b4f22 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #13: &lt;unknown function&gt; + 0xf932c (0x7f4198ee832c in /usr/lib/R/lib/libR.so) ## frame #14: &lt;unknown function&gt; + 0xf9826 (0x7f4198ee8826 in /usr/lib/R/lib/libR.so) ## frame #15: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #16: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #17: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #18: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #19: Rf_eval + 0x353 (0x7f4198f328c3 in /usr/lib/R/lib/libR.so) ## frame #20: &lt;unknown function&gt; + 0xc650d (0x7f4198eb550d in /usr/lib/R/lib/libR.so) ## frame #21: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #22: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #23: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #24: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #25: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #26: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #27: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #28: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #29: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #30: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #31: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #32: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #33: Rf_eval + 0x353 (0x7f4198f328c3 in /usr/lib/R/lib/libR.so) ## frame #34: &lt;unknown function&gt; + 0x1470a2 (0x7f4198f360a2 in /usr/lib/R/lib/libR.so) ## frame #35: Rf_eval + 0x572 (0x7f4198f32ae2 in /usr/lib/R/lib/libR.so) ## frame #36: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #37: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #38: Rf_eval + 0x353 (0x7f4198f328c3 in /usr/lib/R/lib/libR.so) ## frame #39: &lt;unknown function&gt; + 0x149782 (0x7f4198f38782 in /usr/lib/R/lib/libR.so) ## frame #40: &lt;unknown function&gt; + 0x137106 (0x7f4198f26106 in /usr/lib/R/lib/libR.so) ## frame #41: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #42: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #43: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #44: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #45: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #46: &lt;unknown function&gt; + 0x1440ac (0x7f4198f330ac in /usr/lib/R/lib/libR.so) ## frame #47: Rf_eval + 0x454 (0x7f4198f329c4 in /usr/lib/R/lib/libR.so) ## frame #48: &lt;unknown function&gt; + 0x14a22c (0x7f4198f3922c in /usr/lib/R/lib/libR.so) ## frame #49: &lt;unknown function&gt; + 0x1871fd (0x7f4198f761fd in /usr/lib/R/lib/libR.so) ## frame #50: &lt;unknown function&gt; + 0x1353c4 (0x7f4198f243c4 in /usr/lib/R/lib/libR.so) ## frame #51: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #52: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #53: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #54: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #55: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #56: &lt;unknown function&gt; + 0x1440ac (0x7f4198f330ac in /usr/lib/R/lib/libR.so) ## frame #57: &lt;unknown function&gt; + 0x1444e4 (0x7f4198f334e4 in /usr/lib/R/lib/libR.so) ## frame #58: &lt;unknown function&gt; + 0x1377d4 (0x7f4198f267d4 in /usr/lib/R/lib/libR.so) ## frame #59: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) ## frame #60: &lt;unknown function&gt; + 0x14550f (0x7f4198f3450f in /usr/lib/R/lib/libR.so) ## frame #61: Rf_applyClosure + 0x1c7 (0x7f4198f352d7 in /usr/lib/R/lib/libR.so) ## frame #62: &lt;unknown function&gt; + 0x13a989 (0x7f4198f29989 in /usr/lib/R/lib/libR.so) ## frame #63: Rf_eval + 0x180 (0x7f4198f326f0 in /usr/lib/R/lib/libR.so) 위의 코드는 연산 에러가 나는데, 이유는 A_3의 모양이 A_1의 모양과 맞지 않기 때문이다. A_1$size() ## [1] 1 6 A_3$size() ## [1] 6 "],["텐서의-이동-cpu-leftrightarrow-gpu.html", "Chapter 3 텐서의 이동; CPU \\(\\leftrightarrow\\) GPU 3.1 GPU 사용 가능 체크 3.2 CPU to GPU 3.3 GPU to CPU", " Chapter 3 텐서의 이동; CPU \\(\\leftrightarrow\\) GPU 딥러닝(deep learning)에서는 네트워크의 구조가 조금만 복잡해져도, 필요한 계산량이 엄청나게 늘어나기 때문에 GPU는 사실 필수적이다. torch 패키지에서는 텐서를 다룰때에 현재 다루는 텐서가 어디에 저장되어있는가에 대한 일종의 태그를 달아놓는다. 다음의 코드를 살펴보자. a &lt;- torch_tensor(1:4) a ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] a는 3이라는 상수가 담겨있는 텐서이다. 이 a를 콘솔에서 돌렸을때에 나오는 결과 [ CPUFloatType{1} ]를 통해서 우리는 a가 현재 CPU의 메모리를 이용하고 있으며, 모양은 {1}인 실수을 담은 텐서라는 것을 알 수 있다. 3.1 GPU 사용 가능 체크 앞서 정의한 텐서 a를 GPU의 메모리로 옮기기 위해서는, 너무나 당연하게 GPU가 현재 시스템에서 접근 가능한지에 대하여 알아보아야한다. GPU 접근성은 cuda_is_available()을 사용한다. cuda_is_available() ## [1] TRUE 3.2 CPU to GPU 이미 정의된 텐서 a를 GPU로 옮기려면 다음과 같이 cuda() 함수를 이용하면 된다. a ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] a$cuda() ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] gpu &lt;- torch_device(&quot;cuda&quot;) a$to(device = gpu) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] 옮길 때에 dtype을 사용하여 다음과 같이 자료형을 바꿔줄 수도 있다. a$to(device = gpu, dtype = torch_double()) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDADoubleType{4} ] 3.3 GPU to CPU GPU 상에 직접 텐서를 만드는 방법은 다음과 같다. b &lt;- torch_tensor(1:4, device=gpu) b ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] 이전 섹션에서 CPU에서 GPU로 옮기는 방법과 비슷하게 다음의 코드가 작동한다. b$cpu() ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] # to 함수 이용 cpu &lt;- torch_device(&quot;cpu&quot;) a$to(device = cpu) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] "],["순전파-forward-propagation.html", "Chapter 4 순전파 (Forward propagation) 4.1 신경망의 구조 4.2 순전파(Forward propagation)", " Chapter 4 순전파 (Forward propagation) 4.1 신경망의 구조 딥러닝의 시작점인 신경망(Neural network)을 공부하기 위해서, 앞으로 우리가 다룰 모델 중 가장 간단하면서, 딥러닝에서 어떤 일이 벌어지고 있는지 상상이 가능한 신경망을 먼저 학습하기로 하자. 우리가 오늘 예로 생각할 신경망은 다음과 같다. Figure 4.1: 세상에서 가장 간단하지만 있을 건 다있는 신경망 위의 그림과 같은 신경망을 2단 신경망이라고 부른다. 일반적으로 단수를 셀 때 제일 처음 입력하는 층은 단수에 포함하지 않는 것에 주의하자. 각 녹색, 회색, 그리고 빨간색의 노드(node)들은 신경망의 요소를 이루는데, 각각의 이름은 다음과 같다. 입력층(input layer) - 2개의 녹색 노드(node) 은닉층(hidden layer) - 3개의 회색 노드(node) 출력층(output layer) - 1개의 빨강색 노드(node) 자 이제부터, 녹색 노드에는 무엇이 들어가는지, 그리고, 어떤 과정을 거쳐서 빨강색의 값이 나오는지에 대하여 알아보자. 딥러닝에서 녹색이 입력값을 넣어서 빨간색의 결과값을 얻는 과정을 순전파(Forward propagation)라고 부른다. propagation의 뜻은 증식, 혹은 번식인데, 식물이나 동물이 자라나는 것을 의미하는데, 녹색의 입력값들이 어떠한 과정을 거쳐 빨간색으로 자라나는지 한번 알아보자. 4.2 순전파(Forward propagation) 우리가 사용할 데이터 역시 아주 간단하다. \\[ X =\\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; 6 \\end{array}\\right) \\] 가로 행이 하나의 표본을 의미하고, 세로 열 각각은 변수를 의미한다. 즉, 위의 자료 행렬은 2개의 변수 정보가 들어있는 세 개의 표본들이 있는 자료을 의미한다. 4.2.1 표본 1개, 경로 1개만 생각해보기 주의할 것은, 우리가 그려놓은 신경망의 입력층의 노드는 2개이고, 자료 행렬은 3행 2열이라는 것이다. 우리가 그려놓은 신경망으로 샘플 하나 하나가 입력층에 각각 입력되어 표본별 결과값 생성되는 것이다. 따라서 신경망을 잘 이해하기 위해서 딱 하나의 표본, 그리고 딱 하나의 경로만을 생각해보자. &gt; 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 경로를 타고 어떻게 자라나는지 생각해보자. Figure 4.2: 예시 경로 1 그림에서 \\(\\beta\\)는 노드와 노드 사이를 지나갈 때 부여되는 웨이트들을 의미하고, \\(\\sigma()\\)는 다음의 시그모이드(sigmoid) 함수를 의미한다. \\[ \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x+1} \\] 자료 행렬을 위에 색칠된 경로로 보낸다는 의미는 다음과 같은 계산과정을 거친다는 것이다. set.seed(1234) # 데이터 매트릭스 # 3 by 2 X &lt;- torch_tensor(matrix(1:2, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## [ CPUDoubleType{1,2} ] # beta_1 벡터 # 2 by 1 # 1번째 레이어에 관한 웨이트 (베타) 중 # 다음 레이어의 1번째 노드에 대한 베타 벡터에 부여 # beta_1 = (beta_11, beta_12) beta_1 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_1 ## torch_tensor ## 0.1137 ## 0.6223 ## [ CPUDoubleType{2,1} ] # 2번째 레이어 1번째 노드 # 3 by 1 z_21 &lt;- X$mm(beta_1) z_21 ## torch_tensor ## 1.3583 ## [ CPUDoubleType{1,1} ] # 2번째 레이어 1번째 노드에서의 시그모이드 함수 통과 # 3 by 1 library(sigmoid) a_21 &lt;- sigmoid(z_21) a_21 ## torch_tensor ## 0.7955 ## [ CPUDoubleType{1,1} ] # 2번째 레이어에 관한 웨이트 (감마) 중 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # beta_1 상수 1 by 1 gamma_1 &lt;- runif(1) # 3번째 레이어 1번째 노드 # 3 by 1 z_31 &lt;- a_21 * gamma_1 z_31 ## torch_tensor ## 0.4847 ## [ CPUDoubleType{1,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 3 by 1 y_hat &lt;- sigmoid(z_31) y_hat ## torch_tensor ## 0.6188 ## [ CPUDoubleType{1,1} ] 즉, 우리가 생각하는 표본은 빨간색 노드에 도착하기 위해서 두번째 은닉층의 첫번째 노드를 통과하여 올 수 있다. 하지만 빨간색 노드에는 방금 우리가 생각한 경로 뿐만아니라 두 개의 선택지가 더 존재한다. 4.2.2 1개의 표본, 경로 한꺼번에 생각하기 세가지의 경로를 모두 생각해보면, 우리의 표본은 다음의 경로를 통해서 도착한다. &gt; 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 세가지 경로를 타고 어떻게 하나로 합쳐지는지 이해해보자. Figure 4.3: 3가지 경로 이 과정을 우리가 통계 시간에 배운 회귀분석에 연결지어 생각해보면, 다음의 해석이 가능하다. 두번째 은닉층의 각각의 노드들이 하나의 회귀분석 예측 모델들이라고 생각하면, 신경망은 세 개의 회귀분석을 한 대 모아놓은 거대한 회귀분석 집합체라고 생각할 수 있게 된다. 즉, 각 회귀분석 모델들이 예측한 표본에 대한 대응변수 예측값들을 은닉층에 저장한 후, 그 예측값들을 모두 모아 마지막 빨간색 노드에서 합치면서 좀 더 좋은 예측값을 만들어 내는 것이다. 이 때, \\(\\gamma\\) 벡터를 통해 가중치를 부여하는 것이라고 해석이 가능하다. 이 과정을 torch 텐서를 사용하여 깔끔하게 나타내보자. # 1개 표본 # 1 by 2 X &lt;- torch_tensor(matrix(1:2, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## [ CPUDoubleType{1,2} ] # 베타벡터가 세 개 존재함. # 2 by 3 beta_1 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_2 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_3 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) # 정의된 베타벡터를 cbind in torch beta &lt;- torch_cat(c(beta_1, beta_2, beta_3), 2) beta ## torch_tensor ## 0.6234 0.6403 0.2326 ## 0.8609 0.0095 0.6661 ## [ CPUDoubleType{2,3} ] # 2번째 레이어 z_2 # 1 by 3 z_2 &lt;- X$mm(beta) z_2 ## torch_tensor ## 2.3452 0.6593 1.5647 ## [ CPUDoubleType{1,3} ] # 2번째 레이어 sigmoid 함수 통과 # 1 by 3 a_2 &lt;- sigmoid(z_2) # 2번째 레이어에 관한 웨이트 (감마) 벡터 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # gamma vector 3 by 1 gamma_1 &lt;- runif(1) gamma_2 &lt;- runif(1) gamma_3 &lt;- runif(1) gamma &lt;- torch_tensor(matrix(c(gamma_1, gamma_2, gamma_3), ncol = 1), dtype = torch_double()) # 3번째 레이어 z_3 # 1 by 1 z_3 &lt;- a_2$mm(gamma) z_3 ## torch_tensor ## 1.3771 ## [ CPUDoubleType{1,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 1 by 1 y_hat &lt;- sigmoid(z_3) y_hat ## torch_tensor ## 0.7985 ## [ CPUDoubleType{1,1} ] R에서 우리가 즐겨쓰던 cbind()와 rbind()는 torch에서는 torch_cat() 하나의 함수으로 구현이 가능하다. 함수의 두번째 입력값은 숫자 1은 행방향(rbind)에, 2는 열방향(cbind)과 대응된다. 4.2.3 전체 표본, 경로 1개만 생각해보기 이제 자료 행렬 전체를 한꺼번에 넣는 방법을 생각해보자. 입력값이 자료 행렬 전체이므로, 결과값은 이에 대응하도록 행의 갯수와 같은 벡터 형식이 될 것이라는 것을 예상하고 코드를 따라오도록 하자. &gt; 목표: 전체 표본이 신경망을 통해서 예측되는 구조를 이해하자. # 데이터 텐서 # 3 by 2 X &lt;- torch_tensor(matrix(1:6, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## 3 4 ## 5 6 ## [ CPUDoubleType{3,2} ] # 베타벡터가 세 개 존재함. # 2 by 3 beta &lt;- torch_tensor(matrix(runif(6), ncol = 3), dtype = torch_double()) beta ## torch_tensor ## 0.2827 0.2923 0.2862 ## 0.9234 0.8373 0.2668 ## [ CPUDoubleType{2,3} ] # 2번째 레이어 z_2 # 3 by 3 z_2 &lt;- X$mm(beta) z_2 ## torch_tensor ## 2.1296 1.9669 0.8199 ## 4.5419 4.2261 1.9260 ## 6.9543 6.4854 3.0320 ## [ CPUDoubleType{3,3} ] # 2번째 레이어 sigmoid 함수 통과 # 3 by 3 a_2 &lt;- sigmoid(z_2) # 2번째 레이어에 관한 웨이트 (감마) 벡터 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # gamma vector 3 by 1 gamma &lt;- torch_tensor(matrix(runif(3), ncol = 1), dtype = torch_double()) # 3번째 레이어 z_3 # 3 by 1 z_3 &lt;- a_2$mm(gamma) z_3 ## torch_tensor ## 0.5904 ## 0.6900 ## 0.7205 ## [ CPUDoubleType{3,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 3 by 1 y_hat &lt;- sigmoid(z_3) y_hat ## torch_tensor ## 0.6435 ## 0.6660 ## 0.6727 ## [ CPUDoubleType{3,1} ] "],["r6와-텐서.html", "Chapter 5 R6와 텐서 5.1 시작하기 5.2 클래스(Class)와 멤버함수(Method), 그리고 필드(Field) 5.3 상속(Inheritance) - 클래스 물려받기 5.4 공개(Public)정보와 비공개(Private) 정보의 필요성 5.5 텐서와 R6의 관계 5.6 R6 관련자료", " Chapter 5 R6와 텐서 torch의 코드를 살펴보면 우리가 늘상 사용하던 R의 패키지들과는 어딘가 다른점이 있다고 느껴질 것이다. 이것의 근본적인 이유는 바로 torch 패키지가 계체지향언어 (Object Oriented Programming; OOP)를 할 수 있도록 해주는 R6 패키지를 기반으로 하고있기 때문이다. 좀 더 직접적으로 말하면, torch의 텐서와 신경망들이 R6 패키지의 클래스들로 정의되어 있기 때문에, 일반적인 R 패키지들보다 $을 통한 함수(OOP에서는 method 라고 부른다.) 접근이 가능하다. 어떤 이야기인지 한번 좀 더 깊게 들어가보자. 5.1 시작하기 여느 패키지와 다를바가 없다. R6 패키지를 설치하도록 하자. # install.packages(&quot;R6&quot;) library(R6) 5.2 클래스(Class)와 멤버함수(Method), 그리고 필드(Field) R6 패키지에는 딱 하나의 함수가 존재한다. 바로 R6Class() 함수이다. 이 함수의 입력값은 두가지 인데, 첫번째는 클래스 이름 clasename이고, 두번째는 공개될 정보들을 담을 public이라는 입력값이다. public에는 우리가 만들 클래스에서 사용이 가능한 멤버함수들(methods)과 변수(fields)들을 몽땅 다 떼려넣은 리스트(list) 형태가 들어간다. ExampleClass &lt;- R6Class(classname = &quot;Example&quot;, public = list( # 변수(fields) 정의 # 멤버함수(methods) 정의 )) ExampleClass ## &lt;Example&gt; object generator ## Public: ## clone: function (deep = FALSE) ## Parent env: &lt;environment: R_GlobalEnv&gt; ## Locked objects: TRUE ## Locked class: FALSE ## Portable: TRUE 한가지 꼭 짚고 넘어가야하는 것이 있는데, 바로 이름을 정하는 방식이다. 클래스의 이름은 UpperCamelCase 형식으로 짓는다. 즉, 클래스의 이름을 선언할 때 띄어쓰기를 하지않고, 대신 대문자를 사용한다. 두번째 리스트에 들어가는 요소들의 이름은 snake_case를 사용한다. 즉, 모두 소문자를 유지하고, 띄어쓰기 대신에 밑줄을 사용하여 선언한다. 이렇게 규칙을 따라서 작성하게 되면, 나중에 다른 사람이 짜놓은 코드를 보게 되더라도, 선언된 이름의 구조를 보고, 이게 클래스인지, 클래스 안에 정의된 함수 혹은 변수인지를 구분 할 수 있어서 좋다. 5.2.1 클래스는 왜 필요할까? 필자도 클래스의 개념을 처음 들었을때 대체 이게 무슨 소리인지.. 했던 기억이 있다. 심지어 필자의 경우 R밖에 모르던 터여서, OOP가 필요가 있는지에 대한 의문까지 들 정도였으니, (사실 지금도 생각이 많이 바뀌지 않았다.) 머리에 아예 들어오지를 않았다. 그런 필자를 클래스 개념에 대하여 한방에 이해시킨 예제가 바로 학생 클래스이다. 자고로 모든 개념은 예를 들어 설명을 하는 것이 아주 효과적이라고 필자는 믿고있다. &gt; 목표: OOP의 개념와 왜 사용을 하는지에 대하여 이해한다. 5.2.2 학생자료 입력 예제 다음의 코드를 생각해보자. student &lt;- function(){ list() } issac &lt;- student() bomi &lt;- student() issac ## list() bomi ## list() student라는 함수는 빈 리스트를 반환을 하는데, 우리가 이 함수를 사용하여 issac과 bomi라는 학생의 정보를 담는 리스트를 만들 수 있다. 만약 우리가 다음과 같은 추가 정보를 저장하려고 한다고 가정해보자. issac last name: Lee first name: Issac email: issac-lee@gmail.com midterm: 70 final: 50 bomi last name: Kim first name: Bomi email: bomi-kim@gmail.com midterm: 65 final: 80 위의 정보를 저장하기 위해서는 다음과 같이 $ 기호를 통하여 저장할 수 있다. issac$first &lt;- &quot;Issac&quot; issac$last &lt;- &quot;Lee&quot; issac$email &lt;- &quot;issac-lee@gmail.com&quot; issac$midterm &lt;- 70 issac$final &lt;- 50 bomi$first &lt;- &quot;Bomi&quot; bomi$last &lt;- &quot;Kim&quot; bomi$email &lt;- &quot;bomi-kim@gmail.com&quot; bomi$midterm &lt;- 65 bomi$final &lt;- 80 issac ## $first ## [1] &quot;Issac&quot; ## ## $last ## [1] &quot;Lee&quot; ## ## $email ## [1] &quot;issac-lee@gmail.com&quot; ## ## $midterm ## [1] 70 ## ## $final ## [1] 50 bomi ## $first ## [1] &quot;Bomi&quot; ## ## $last ## [1] &quot;Kim&quot; ## ## $email ## [1] &quot;bomi-kim@gmail.com&quot; ## ## $midterm ## [1] 65 ## ## $final ## [1] 80 위의 코드는 OOP관점에서 상당히 중복 코드가 많은 비효율적인 코드이다. 이러한 코드를 우리가 배운 R6Class()를 사용하여 어떻게 줄일 수 있는지 알아보자. 5.2.3 클래스(Class) 정의하기 앞에서 우리는 issac과 bomi라는 변수를 생성했는데, 둘의 공통점은 학생이라는 점이었다. 사실 앞선 코드를 작성을 한다는 것은 issac과 bomi 뿐 아니라 엄청 많은 수의 학생들에 대한 데이터를 다루고 있는 상황일 수도 있다. 우리들이 써놓은 코드를 잘 뜯어보니, 학생 데이터로 입력되는 각 개인들은 성과 이름, 이메일, 그리고, 중간, 기말고사 점수의 정보들을 가지고 있다. 즉, 학생, Student, 라는 클래스는 항상 성(last)과 이름(first), 중간(midterm), 기말고사(final) 성적이 저장되어 있고, 이메일의 경우 이름과 성을 이용해서 작성을 하되, 모두 소문자로 입력된 자료 형태를 가지고 있는 구조를 갖는 어떤 추상적인 개념이라는 것을 알 수 있다. 이러한 정보를 사용하여 우리는 다음과 같이 Student 클래스를 선언 할 수 있다. Student &lt;- R6Class(&quot;Student&quot;, list( # 필요한 변수 (field) 선언 first = NULL, last = NULL, email = NULL, midterm = NA, final = NA, # 클래스 안의 객체를 만들때 사용되는 initialize initialize = function(first, last, midterm, final){ self$first = first self$last = last self$email = glue::glue(&quot;{tolower(first)}-{tolower(last)}@gmail.com&quot;) self$midterm = midterm self$final = final } )) Student ## &lt;Student&gt; object generator ## Public: ## first: NULL ## last: NULL ## email: NULL ## midterm: NA ## final: NA ## initialize: function (first, last, midterm, final) ## clone: function (deep = FALSE) ## Parent env: &lt;environment: R_GlobalEnv&gt; ## Locked objects: TRUE ## Locked class: FALSE ## Portable: TRUE 결과값을 유심히 살펴보면, &lt;Student&gt; object generator 라는 부분이 있는데, Student 라는 클래스는 객체(object)들을 만들어내는 생성자(generator)라는 것을 알 수 있다. 우리가 만들 Student 생성자를 통해서 도장을 찍듯, new() 함수를 사용하여 issac과 bomi를 다음과 같이 만들 수 있다. issac &lt;- Student$new(&quot;Issac&quot;, &quot;Lee&quot;, 70, 50) bomi &lt;- Student$new(&quot;Bomi&quot;, &quot;Kim&quot;, 65, 80) issac ## &lt;Student&gt; ## Public: ## clone: function (deep = FALSE) ## email: issac-lee@gmail.com ## final: 50 ## first: Issac ## initialize: function (first, last, midterm, final) ## last: Lee ## midterm: 70 bomi ## &lt;Student&gt; ## Public: ## clone: function (deep = FALSE) ## email: bomi-kim@gmail.com ## final: 80 ## first: Bomi ## initialize: function (first, last, midterm, final) ## last: Kim ## midterm: 65 즉, OOP의 장점은 공을 들여 한번 클래스를 잘 만들어놓으면, 한번 작성된 함수나 변수들의 재 사용율이 엄청 좋아지는 것이다. 5.2.4 print()를 사용한 결과물 정리 정의된 클래스는 기본적으로 동작하는 함수들을 덮어서 쓸 수 있다. 예를들어 print()를 함수로 정의해버리면, base에 있는 print() 동작을 덮어서 쓸 수 있다. 즉, 기본 함수들 print(), plot() 같은 함수들을 우리가 정의한 클래스에서 나온 객체들에 적용했을때의 작동을 정해줄 수 있다는 것이다. Student &lt;- R6Class(&quot;Student&quot;, list( # 필요한 변수 (field) 선언 first = NULL, last = NULL, email = NULL, midterm = NA, final = NA, # 클래스 안의 객체를 만들때 사용되는 initialize initialize = function(first, last, midterm, final){ self$first = first self$last = last self$email = glue::glue(&quot;{tolower(first)}-{tolower(last)}@gmail.com&quot;) self$midterm = midterm self$final = final }, print = function(...){ cat(&quot;Student: \\n&quot;) cat(glue::glue(&quot; Name : {self$first} {self$last} E-mail: {self$email} Midterm Score : {self$midterm} Final Score: {self$final} &quot;)) invisible(self) } )) soony &lt;- Student$new(&quot;Soony&quot;, &quot;Kim&quot;, 70, 20) soony ## Student: ## Name : Soony Kim ## E-mail: soony-kim@gmail.com ## Midterm Score : 70 ## Final Score: 20 print() 멤버 함수를 추가한 후에 만들어진 soony의 정보는 클래스안에 정의된 print()를 통해서 보여진다는 것을 확인할 수 있다. 한가지 주의할 점은 print()가 클래스 안에 정의되어 있지 않은 채로 생성된 issac과 bomi의 경우는 print()가 작동하지 않는다는 것이다. 즉, 클래스에 정의된 함수들은 객체가 클래스로부터 생성될 때, 따라와서 붙는다. issac$print() ## Error in eval(expr, envir, enclos): attempt to apply non-function soony$print() ## Student: ## Name : Soony Kim ## E-mail: soony-kim@gmail.com ## Midterm Score : 70 ## Final Score: 20 5.2.5 set을 이용한 클래스 조정 앞에서 우리는 print() 함수를 추가하기 위하여 전체 클래스를 다시 정의하였다. 하지만, 이렇게 클래스안에 함수를 추가하기 위해서 전체 클래스를 다시 정의하기보단, set()을 이용해서 변수나 함수를 추가할 수 있다. Student$set(&quot;public&quot;, &quot;total&quot;, NA) Student$set(&quot;public&quot;, &quot;calculate_total&quot;, function(){ self$total &lt;- self$midterm + self$final invisible(self) }) invisible() 함수는 결과를 반환하되, 결과물을 보여주지 않는 것인데, 클래스에서 함수를 정의할 때에 반드시 invisible(self)를 반환해줘야만 한다. 따라서 함수이지만, 함수와는 다른 이 클래스 안의 함수들을 멤버함수 method()라고하여 일반 함수와 구분을 지어서 부른다. jelly &lt;- Student$new(&quot;Jelly&quot;, &quot;Lee&quot;, 35, 23) jelly ## Student: ## Name : Jelly Lee ## E-mail: jelly-lee@gmail.com ## Midterm Score : 35 ## Final Score: 23 jelly$total ## [1] NA jelly$calculate_total() jelly$total ## [1] 58 5.3 상속(Inheritance) - 클래스 물려받기 OOP가 코드의 중복을 되도록 피할 수 있도록 설계되어 있다는 것을 어렴풋이나마 앞의 예제를 통하여 알 수 있을 것이다. 이러한 OOP의 코드 재사용 관점에서 상속(Inheritance)의 개념은 꽃 중에 꽃이라 불릴 만하다. 단 한 줄의 코드로 미리 작성해놓은 함수들에 접근이 가능하기 때문이다. 상속(Inheritance)이라고 하면 뭔가 거창할 것 같지만, 그냥 미리 정의해둔 클래스의 정보(멤버함수과 필드)를 다른 클래스를 정의할 때 받아올 수 있다는 말이다. 예를 들어보자. 이제까지 사용해 온 학생 개념, Student 클래스를 좀 더 세분화를 한다면 학교별로 나눌 수 있을 것이다. Student 클래스를 상속받는 슬통대학교(University of Statistics Playbook; USP) 학생들을 위한 서브 클래스(sub class)는 다음과 같이 생성할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, average = NA, calculate_average = function(){ self$average &lt;- mean(c(self$midterm, self$final)) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ) ) sanghoon &lt;- UspStudent$new(&quot;Sanghoon&quot;, &quot;Park&quot;, 80, 56) sanghoon ## Student: ## Name : Sanghoon Park ## E-mail: sanghoon-park@gmail.com ## Midterm Score : 80 ## Final Score: 56 새로 정의된 UspStudent 클래스는 상위 클래스인 Student 클래스의 멤버함수들과 변수들을 그대로 물려받는다. 여기서 코드의 재사용성이 증가한다. 또한 상위 클래스가 가지고 있던 calculate_total() 멤버함수에 접근하여, 새롭게 고쳐서 사용하는 것도 가능하다. 다음은 정의된 멤버함수들을 사용하여 변수들에 계산을 해서 넣는 과정을 보여준다. sanghoon$university_name ## [1] &quot;University of Statistics Playbook&quot; sanghoon$calculate_average() sanghoon$average ## [1] 68 sanghoon$calculate_total() ## The total score of midterm and final exam is calculated. sanghoon$total ## [1] 136 5.4 공개(Public)정보와 비공개(Private) 정보의 필요성 앞에서 살펴본 R6Class() 함수의 두 가지 입력값은 클래스 이름(classname)과 공개정보(public) 였다. 클래스를 만들고 사용하다보면, 때로는 클래스 안의 함수들을 사용하기 위해서 만들어야하는 변수나 함수들이 있는데, 이러한 정보들은 굳이 클래스를 사용하는 사용자들에게 보여줄 필요가 없다. 우리네 인생도 그러하다. 우리는 때로는 너무 많은 정보 제공에 피로감과 불편을 겪는 경우가 많다. 따라서, 클래스에 대한 정보의 접근을 적절하게 조절할 필요가 있는데, 클래스의 정보들을 공개될 정보(public)와 비공개 정보(private)들로 분류함으로써 조절할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, calculate_average = function(){ private$.average &lt;- mean(c(self$midterm, self$final)) cat(&quot;Average score is&quot;, private$.average) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ), private = list( .average = NA ) ) taemo &lt;- UspStudent$new(&quot;Taemo&quot;, &quot;Bang&quot;, 80, 56) taemo$calculate_average() ## Average score is 68 위의 UspStudent 클래스에는 비공개 정보가 하나들어있다. 바로 중간 기말고사 점수의 평균을 저장하는 average 변수인데, 클래스의 정의시 private()에 감싸져서 입력이 되었음에 주목하자. average 변수는 클래스 안에서의 멤버함수를 통해서 접근할 땐 private$name 형식으로 접근이 가능함에 반하여, 클래스를 사용하는 사용자 입장에서는 가려져서 보이지 않는 정보에 해당한다. taemo$.average ## NULL 해들리 위캠의 말을 빌리면, 공개-비공개 정보의 구분은 큰 패키지나 클래스를 정의할 때 가장 중요한 단계가 된다. 왜냐하면 비공개 정보의 경우는 개발자의 입장에서 언제든지 수정할 수 있는 정보가 되지만, 공개된 멤버함수나 필드들에 대해서는 쉽게 바꿀 수가 없기 때문이다. &gt; 여기서 하나 짚고 넘어가면 좋은 것이 있는데, 바로 R에서의 이름 짓기 방식이다. R의 기본 함수들 중에서 .을 사용해서 지어진 경우가 있는데, 현재는 권장하지 않고 있다. 이유는 바로 비공개 정보를 갖는 변수나 함수들을 나타내는데에 .을 찍어서 나타내기 때문이다. `.average` 역시 변수의 이름에서 이 변수는 클래스 안에서만 접근이 가능하다는 것을 변수 이름만 보고도 알 수 있도록 만들어졌다. 5.4.1 활성 변수(active field)를 사용한 읽기 전용 변수 Advance R의 14장의 내용을 보면, R6의 접근성을 다루면서 active field의 개념이 나온다. 자세한 내용이 궁금한 독자들은 찾아보기 바란다. active field의 좋은 점은 이것을 사용해서 클래스 사용자들에게 읽기 전용 정보를 제공해줄수 있기 때문이다. 앞에서의 예를 들어보면 중간, 기말고사의 평균 정보는 클래스 사용자들에게 유요한 정보가 될 수 있다. 하지만, private으로 감싸버리면 사용자들은 이 정보에 접근을 할 수 없게 된다. 사용자는 평균 정보에 접근하고 싶어하지만, 개발자의 입장에서는 쉽게 공개정보로 바꾸기가 쉽지 않다. 왜냐하면 사용자들이 마음대로 평균 변수에 접근해서 정보를 변경시켜버리면 클래스에서 평균 정보를 가져다가 쓰는 멤버함수들이 잘 작동하지 않을 수 있기 때문이다. 이럴 경우 active field를 사용해서 average를 읽기전용으로만 접근 가능하도록 설계할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, ## active field active = list( average = function(value) { if (missing(value)) { private$.average } else { stop(&quot;`$average` is read only&quot;, call. = FALSE) } } ), public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, calculate_average = function(){ private$.average &lt;- mean(c(self$midterm, self$final)) cat(&quot;Average score is&quot;, private$.average) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ), private = list( .average = NA ) ) conie &lt;- UspStudent$new(&quot;Connie&quot;, &quot;&quot;, 78, 82) conie$calculate_average() ## Average score is 80 conie$average ## [1] 80 위에서 정의된 UspStudent 클래스에서는 사용자에게 평균값을 구하는 함수와 구한 평균값에 접근을 허용하지만, 사용자가 average값에 접근하여 바꾸려고 하면 에러를 뱉어내도록 설계가 되어있다. conie$average &lt;- 60 ## Error: `$average` is read only 5.5 텐서와 R6의 관계 5.6 R6 관련자료 R6에 대한 더 깊은 내용은 Hadley Wickham의 Advanced R과 R6 패키지의 웹사이트를 참고하도록 하자. "],["references.html", "References", " References "]]
