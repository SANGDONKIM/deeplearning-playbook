[["index.html", "딥러닝 공략집 with R 들어가며 설치하기 기본 패키지", " 딥러닝 공략집 with R 슬기로운통계생활 2021-02-15 들어가며 Figure 0.1: deep-learning playbook 이제까지 R에서의 딥러닝은 Python의 라이브러리들을 reticulate 패키지를 이용하여 빌려온 형태였지만, torch for R 패키지는 C 라이브러리를 Torch를 기반으로 R을 wrapper 언어로서 사용하여 패키지를 만들었다. 즉, Torch + Python = PyTorch, Torch + R = Rtorch가 되는 셈이다. 설치하기 설치 역시 간단한다. 여느 R패키지와 같이 install.packages() 함수를 사용하면 된다. 서브 라이브러리인 torchaudio와 torchvision이 있으나, 책의 뒷부분에서 다루기로 한다. install.packages(&quot;torch&quot;) # 혹은 개발버전을 다운 받고 싶다면 # devtools::install_github(&quot;mlverse/torch&quot;) 기본 패키지 앞으로의 내용에 있어서 다음의 두 패키지는 기본으로 불러와서 사용하는 것을 약속으로 한다. library(tidyverse) library(torch) "],["intro.html", "Chapter 1 딥러닝 첫걸음, 텐서 (tensor) 만들기 1.1 torch와의 첫만남 1.2 텐서 (tensor) 만들기 1.3 고급기술: 영리하게 만들기 1.4 텐서와 행렬은 같을까?", " Chapter 1 딥러닝 첫걸음, 텐서 (tensor) 만들기 1.1 torch와의 첫만남 torch패키지를 설치했으니, 한번 만나봐야한다. 다음의 명령어를 통하여 torch를 불러보자. library(torch) 1.2 텐서 (tensor) 만들기 텐서가 무엇이냐! 무언가 대단한 것처럼 보이나, 결국 우리가 R을 배웠을때 사용했던 matrix의 개념을 확장시킨 것이라고 생각하면 된다. 결국 다차원 행렬, 혹은 Array인 것이다. 우리가 많이 쓰는 행렬도 Array에 속하지만, 보통 Array라는 용어는 3차원 이상의 행렬을 암시한다. 이름부터 멋있는 딥러닝인데 다른 용어들이 Array같이 다른 패키지에서 사용되는 것들이랑 동일하면 격이 떨어지므로, 텐서 (tensor) 라고 붙였다. 토치 설명서에 따르면 텐서는 R의 Array와 비슷하나, GPU 계산에도 쓸 수 있다고 나와있다. (응, 그냥 Array.) 또한, 프로그래밍 언어에서 어떤 변수를 만들때, 만든다고 하지않고, “선언한다\" 라고 한다. 따라서 앞으로 만든다는 말 대신”선언\"이라는 용어를 사용하겠다. 1.2.1 빈 텐서 만들기 속이 빈 5행 3열의 텐서은 다음과 같이 선언한다. 주의할 점은 우리가 이번에 만들 빈 (empty) 텐서 과 뒤에서 만들어 볼 0 텐서는 다르다; 빈 텐서은 0과 근접한 쓰레기값이 들어있는 반면에, 0 텐서에는 정말 0이 들어있다. x &lt;- torch_empty(5, 3) # 텐서 x값 확인 x ## torch_tensor ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## [ CPUFloatType{5,3} ] # 텐서 x의 크기 확인 dim(x) ## [1] 5 3 정말 5행 3열의 텐서가 만들어졌다. 이건 마치 우리가 R을 처음 시작하고 텐서을 만드는 것과 아주 유사해서, 실제 R을 만져본 사람이라면 뒤로 빨리 넘기고 싶어하는 욕구가 솓구칠 것이다. 결과값을 잘 살펴보자. CPUFloatType에서 우리는 현재 만든 rand_tensor는 CPU에서 접근이 가능하며, 실수 (float) 타입이라는 것을 이야기해준다. 1.2.2 랜덤 텐서 텐서의 각 자리에 0에서 1사이의 난수로 채워서 만드는 방법이다. torch_rand() 함수를 사용한다. rand_tensor &lt;- torch_rand(5, 3) rand_tensor ## torch_tensor ## 0.1502 0.8582 0.9395 ## 0.4241 0.9650 0.8021 ## 0.3065 0.6665 0.4819 ## 0.1011 0.4421 0.1399 ## 0.0454 0.1024 0.2083 ## [ CPUFloatType{5,3} ] 참고로 이렇게 만들어진 텐서에는 R에서 텐서과 어레이(array)에 접근할 때사용한 모든 문법들을 사용해서 접근할 수 있다. rand_tensor[,2] ## torch_tensor ## 0.8582 ## 0.9650 ## 0.6665 ## 0.4421 ## 0.1024 ## [ CPUFloatType{5} ] rand_tensor[1:3,] ## torch_tensor ## 0.1502 0.8582 0.9395 ## 0.4241 0.9650 0.8021 ## 0.3065 0.6665 0.4819 ## [ CPUFloatType{3,3} ] rand_tensor[3:4,c(1, 3)] ## torch_tensor ## 0.3065 0.4819 ## 0.1011 0.1399 ## [ CPUFloatType{2,2} ] 위의 예제에서 벌써부터 일부 R유저들은 감격의 눈물을 흘릴 수 있다. 그렇다, Rtorch에서 텐서의 첫번째 위치는 1부터 시작한다. 이 너무나도 당연한 진리는 파이썬에서는 통하지 않는다. 1.2.3 단위 텐서 4행 4열의 단위 텐서 (identity matrix)를 선언하는 방법은 다음과 같다. x &lt;- torch_eye(4) x ## torch_tensor ## 1 0 0 0 ## 0 1 0 0 ## 0 0 1 0 ## 0 0 0 1 ## [ CPUFloatType{4,4} ] 1.2.4 영(0) 텐서 텐서의 요소들이 모두 0으로 채워진 3행 5열의 텐서을 선언하는 것은 다음과 같이 torch_zeros() 함수를 사용한다. x &lt;- torch_zeros(3, 5) x ## torch_tensor ## 0 0 0 0 0 ## 0 0 0 0 0 ## 0 0 0 0 0 ## [ CPUFloatType{3,5} ] 1.3 고급기술: 영리하게 만들기 지금까지는 미리 정해진 값들, 난수나, 0과 1을 채워넣는 법을 배웠다. 하지만, 많은 경우 우리가 직접 정의한 텐서들을 다루게 될 것이다. 이번 섹션에서는 좀 더 영리하게 선언해보는 방법을 배워보자. 1.3.1 텐서 직접선언 가장 핵심적인 내용은 R에서 벡터와 행렬을 정의한 후 torch_tensor() 함수에 넣어주면, 그대로 가져다가 텐서로 바꿔준다는 사실이다. 다음의 예제는 2행 2열의 행렬을 정의한 후, 정의된 행렬을 사용하여 텐서를 만드는 코드이다. y &lt;- torch_tensor(matrix(c(1, 2, 3, 4, 5, 6), ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUFloatType{3,2} ] 1.3.2 : 연산자 사용 위의 코드가 잘 작동한다는 사실을 알게 되면, 우리가 너무나 익숙한 R의 기본 함수들을 사용하여 텐서를 자유롭게 만들 수 있을 것이다. 앞선 예제는 : 연산자를 통하여 다음과 같이 축약 할 수 있다. y &lt;- torch_tensor(matrix(1:6, ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPULongType{3,2} ] 1.3.3 seq() 함수 사용 seq() 함수는 좀 더 유연한 벡터를 만들 수 있도록 해주므로, 텐서를 만들때 유용하게 사용될 것이다. y &lt;- torch_tensor(matrix(seq(0.1, 1, by = 0.1), ncol = 2)) y ## torch_tensor ## 0.1000 0.6000 ## 0.2000 0.7000 ## 0.3000 0.8000 ## 0.4000 0.9000 ## 0.5000 1.0000 ## [ CPUFloatType{5,2} ] 위의 코드는 seq() 함수를 사용해서 벡터를 만들고, 2열을 갖는 행렬을 만든 후, 텐서로 변환을 시켰다. 단, by 옵션의 경우, 결과값이 홀수인지 짝수인지 체크해줘야 하므로, 특정 범위에서의 일정 간격 숫자를 뽑아 행렬로 만들땐 length.out 옵션이 편하다. y &lt;- torch_tensor(matrix(seq(0, 1, length.out = 10), ncol = 2)) y ## torch_tensor ## 0.0000 0.5556 ## 0.1111 0.6667 ## 0.2222 0.7778 ## 0.3333 0.8889 ## 0.4444 1.0000 ## [ CPUFloatType{5,2} ] 둘 다 0과 1사이의 벡터를 만들었지만, 결과는 다르다는 것에 주의하자. 텐서를 만드는 방법에 대한 핵심은 결국, 자신이 편한 방법으로 만들고 싶은 텐서와 대응되는 R 개체를 만들고, torch_tensor()에 입력 시켜주면 되는 것이다. 1.3.4 %&gt;% 연산자 사용 가끔 R에서 아주 많이 쓰이는 %&gt;% 파이프 연산자를 다른 라이브러리를 사용할 경우 적용할 생각을 못하는 경우가 있다. 왼쪽의 결과 값을 오른쪽의 입력값으로 넘겨주는 파이프 연산자 역시 torch 패키지에서 사용 가능하므로, 텐서 만드는 방법은 그야말로 무궁무진하다. library(magrittr) y2 &lt;- torch_tensor(1:5 %&gt;% diag()) y2 ## torch_tensor ## 1 0 0 0 0 ## 0 2 0 0 0 ## 0 0 3 0 0 ## 0 0 0 4 0 ## 0 0 0 0 5 ## [ CPULongType{5,5} ] 1.4 텐서와 행렬은 같을까? 앞에서 설명한 것처럼 텐서는 행렬의 개념을 확장시킨 것에 지나지 않겠지만, 그렇다고 같은 취급을 해서도 안된다. 그도 그럴것이 R에서 torch의 텐서와 행렬은 같지 않다. 이러한 사실은 다음과 같이 위에서 만든 텐서 x에 R의 기본 연산자인 행렬곱을 적용해보면 알 수 있다. x &lt;- torch_zeros(3, 5) x %*% t(x) ## Error in t.default(x): argument is not a matrix 위의 argument is not a matrix 에러에서 우리는 정말 텐서와 행렬은 다르게 취급된다는 것을 알 수 있다. 즉, R환경에서 텐서와 행렬은 근본이 다른 개체(object)라는 것을 알 수 있다. 그렇다면 ‘텐서끼리의 계산은 어떻게 할까?’ 자연스러운 의문이 든다. 다음 장에서는 텐서의 연산에 대하여 배워보자. "],["operation.html", "Chapter 2 텐서 (tensor) 연산 2.1 토치 (torch) 불러오기 및 준비물 준비 2.2 텐서의 연산", " Chapter 2 텐서 (tensor) 연산 지난 챕터에서 우리는 텐서가 행렬의 연산에 적용되는 %*%과 호환이 되지 않는 다는 것을 알게되었다. 이번 챕터에서는 텐서들의 연산에 대하여 알아보도록 하자. 2.1 토치 (torch) 불러오기 및 준비물 준비 토치 (torch) 를 불러오고, 이번 챕터에 사용될 텐서 A, B, 그리고 C를 준비하자. 지난 챕터에서 배운 난수를 이용한 텐서도 만들 예정이니 난수를 고정한다. library(torch) # 난수 생성 시드 고정 torch_manual_seed(2021) A &lt;- torch_tensor(1:6) B &lt;- torch_rand(2, 3) C &lt;- torch_rand(2, 3, 2) A; B; C ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPULongType{6} ] ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] ## torch_tensor ## (1,.,.) = ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## ## (2,.,.) = ## 0.1242 0.7489 ## 0.3608 0.5131 ## 0.2959 0.7834 ## [ CPUFloatType{2,3,2} ] 만들어진 세 개의 텐서 결과를 살펴보면 다음과 같다. 텐서 A: 정수들로 구성이 되어있고, 6개의 원소들이 벡터를 이루고 있다. 텐서 B: 실수들로 구성이 되어있고, 똑같이 6개의 원소들이 있지만, 모양이 4행 3열인 2차원 행렬의 모양을 하고 있다. 텐서 C: 실수들로 구성이 되어있고, 총 원소 갯수는 12개지만, 모양은 3행 2열의 행렬이 두개가 쌓여진 꼴의 3차원 배열 (array) 이다. 2.2 텐서의 연산 2.2.1 형(type) 변환 먼저 주목해야 할 것은 바로 텐서 A와 B의 자료형이 다르다는 것이다. 이게 무슨뜻이냐면 A에는 정수만이 담길 수 있고, B에는 실수만이 담길 수 있도록 설계가 되어있다는 것이다. 앞에서 확인한 자료형을 좀 더 명확하게 확인하기 위해서는 type() 사용한다. A$dtype ## torch_Long B$dtype ## torch_Float 텐서 A를 실수형 텐서로 바꿔보자. 텐서의 형을 변환할 때에는 A텐서 안에 속성으로 들어가있는 to() 함수를 사용 (좀 더 어려운 관점에서는 OOP의 method를 사용) 해서 바꿔줄 수 있다. A &lt;- A$to(dtype = torch_double()) A ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPUDoubleType{6} ] torch에는 정말 많은 자료형이 있는데, 그 목록은 다음을 참고하자. 2.2.2 모양 변환 앞에서 텐서 A를 B와 같은 실수를 담을 수 있는 형으로 바꾸었다. 그렇다면 이 두 개를 더할 수 있을까? 답은 “아니올시다.” 이다. 왜냐하면 모양이 다르기 때문이다. A + B ## Error in (function (self, other, alpha) : The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1 ## Exception raised from infer_size at ../aten/src/ATen/ExpandUtils.cpp:24 (most recent call first): ## frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x69 (0x7f0dd8f17b89 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libc10.so) ## frame #1: at::infer_size(c10::ArrayRef&lt;long&gt;, c10::ArrayRef&lt;long&gt;) + 0x552 (0x7f0dc8a66382 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #2: at::TensorIterator::compute_shape(at::TensorIteratorConfig const&amp;) + 0xde (0x7f0dc8f68c2e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #3: at::TensorIterator::build(at::TensorIteratorConfig&amp;) + 0x64 (0x7f0dc8f6b1e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #4: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&amp;) + 0xdd (0x7f0dc8f6b99d in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #5: at::TensorIterator::binary_op(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;) + 0x130 (0x7f0dc8f6bb30 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #6: at::native::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x53 (0x7f0dc8c1ebc3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #7: &lt;unknown function&gt; + 0x13311bd (0x7f0dc92851bd in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #8: &lt;unknown function&gt; + 0xaf2045 (0x7f0dc8a46045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #9: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7f0dc943081f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #10: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7f0dc9326fd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #11: &lt;unknown function&gt; + 0x2a0f2bb (0x7f0dca9632bb in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #12: &lt;unknown function&gt; + 0xaf2045 (0x7f0dc8a46045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #13: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7f0dc943081f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #14: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7f0dc9326fd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #15: _lantern_add_tensor_tensor_scalar + 0x64 (0x7f0dd92990e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/liblantern.so) ## frame #16: cpp_torch_namespace_add_self_Tensor_other_Tensor(Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchScalar, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchScalar&gt;(XPtrTorchScalar*)), false&gt;) + 0x48 (0x7f0dd9bdefe8 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #17: _torch_cpp_torch_namespace_add_self_Tensor_other_Tensor + 0x9c (0x7f0dd997700c in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #18: &lt;unknown function&gt; + 0xf9310 (0x7f0dec560310 in /usr/lib/R/lib/libR.so) ## frame #19: &lt;unknown function&gt; + 0xf9826 (0x7f0dec560826 in /usr/lib/R/lib/libR.so) ## frame #20: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #21: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #22: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #23: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #24: Rf_eval + 0x353 (0x7f0dec5aa8c3 in /usr/lib/R/lib/libR.so) ## frame #25: &lt;unknown function&gt; + 0xc650d (0x7f0dec52d50d in /usr/lib/R/lib/libR.so) ## frame #26: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #27: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #28: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #29: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #30: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #31: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #32: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #33: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #34: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #35: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #36: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #37: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #38: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #39: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #40: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #41: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #42: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #43: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #44: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #45: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #46: &lt;unknown function&gt; + 0x12d83b (0x7f0dec59483b in /usr/lib/R/lib/libR.so) ## frame #47: &lt;unknown function&gt; + 0x9021b (0x7f0dec4f721b in /usr/lib/R/lib/libR.so) ## frame #48: Rf_eval + 0x706 (0x7f0dec5aac76 in /usr/lib/R/lib/libR.so) ## frame #49: &lt;unknown function&gt; + 0x149782 (0x7f0dec5b0782 in /usr/lib/R/lib/libR.so) ## frame #50: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #51: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #52: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #53: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #54: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #55: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #56: &lt;unknown function&gt; + 0x1440ac (0x7f0dec5ab0ac in /usr/lib/R/lib/libR.so) ## frame #57: Rf_eval + 0x454 (0x7f0dec5aa9c4 in /usr/lib/R/lib/libR.so) ## frame #58: &lt;unknown function&gt; + 0x14a22c (0x7f0dec5b122c in /usr/lib/R/lib/libR.so) ## frame #59: &lt;unknown function&gt; + 0x1871fd (0x7f0dec5ee1fd in /usr/lib/R/lib/libR.so) ## frame #60: &lt;unknown function&gt; + 0x1353c4 (0x7f0dec59c3c4 in /usr/lib/R/lib/libR.so) ## frame #61: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #62: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #63: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) 모양이 다른 텐서를 더하려고 하면 R은 위에서 보듯 너무나 많은 에러를 쏟아낸다. 모양이 다른 두 텐서를 더하기 위해서는 모양을 같게 맞춰줘야 한다. A의 모양을 B의 모양과 같이 바꿔보도록 하자. 모양을 바꿀때는 view() 함수를 사용하고, 안에 모양의 형태를 벡터 형식으로 짚어 넣는다는 것을 기억하자. A &lt;- A$view(c(2, 3)) A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] 한가지 짚고 넘어가야하는 기능이 있는데, R에서 행렬을 정의할 때, 주어진 원소벡터를 넣고, 가로행과 세로열 중 하나만 입력을 해도 잘 정의가 되는 것을 기억할 것이다. view 함수 역시 비슷한 기능이 있는데, 바로 -1을 이용해서 모양을 변환시키는 방법이다. 앞선 예제에서 2행 3열이 텐서를 1행의 가로 텐서로 변환 시키려면 다음과 같이 view() 함수의 입력값을 조정할 수 있다. A$view(c(1, -1)) ## torch_tensor ## 1 2 3 4 5 6 ## [ CPUDoubleType{1,6} ] 2.2.3 덧셈과 뺄셈 앞에서 형(type)과 모양(shape)까지 맞춰놨으니, 텐서끼리의 덧셈과 뺄셈을 할 수 있다. A + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUDoubleType{2,3} ] A - B ## torch_tensor ## 0.4866 1.2574 2.2841 ## 3.4295 4.8347 5.9557 ## [ CPUDoubleType{2,3} ] 사실, 텐서끼리의 연산은 모양만 맞으면 가능하다. 즉, 다음의 연산이 성립한다. A_ &lt;- A$to(dtype = torch_long()) A_ + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUFloatType{2,3} ] 결과에서 알 수 있듯, 정수를 담을 수 있는 텐서와 실수를 담을 수 있는 텐서를 더하면, 결과는 실수를 담을 수 있는 텐서로 반환이 된다. 하지만, 필자는 이러한 코딩은 피해야 한다고 생각한다. 즉, 모든 연산을 할 경우, 명시적으로 형변환을 한 후 연산을 할 것을 권한다. 왜냐하면, 언제나 우리는 코드를 다른 사람이 보았을 때, 이해하기 쉽도록 짜는 것을 추구해야 한다. (코드는 하나의 자신의 생각을 적은 글이다.) 2.2.4 상수와의 연산 R에서와 마찬가지로, 텐서와 상수와의 사칙연산은 각 원소에 적용되는 것을 확인하자. A + 2 ## torch_tensor ## 3 4 5 ## 6 7 8 ## [ CPUDoubleType{2,3} ] B^2 ## torch_tensor ## 0.2636 0.5514 0.5125 ## 0.3254 0.0273 0.0020 ## [ CPUFloatType{2,3} ] A %/% 3 ## torch_tensor ## 0 0 1 ## 1 1 2 ## [ CPUDoubleType{2,3} ] A %% 3 ## torch_tensor ## 1 2 0 ## 1 2 0 ## [ CPUDoubleType{2,3} ] 2.2.5 제곱근과 로그 제곱근(square root)나 로그(log) 함수 역시 각 원소별 적용이 가능하다. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] torch_sqrt(A) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] 위의 연산이 에러가 나는 이유는 A가 정수를 담는 텐서였는데, 연산을 수행한 후에 실수가 담겨져서 나오는 에러이다. R과는 사뭇다른 예민한 아이 torch를 위해 형을 바꿔준 후에 연산을 실행하도록 하자. torch_sqrt(A$to(dtype = torch_double())) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] torch_log(B) ## torch_tensor ## -0.6667 -0.2977 -0.3342 ## -0.5613 -1.8002 -3.1166 ## [ CPUFloatType{2,3} ] 2.2.6 텐서의 곱셈 텐서의 곱셈 역시 모양이 맞아야 하므로, 3행 2열이 두개가 붙어있는 C에서 앞에 한장을 떼어내도록 하자. B ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] D &lt;- C[1,,] D ## torch_tensor ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## [ CPUFloatType{3,2} ] 텐서의 곱셈은 torch_matmul() 함수를 사용한다. # 파이프 사용해도 무방하다. # B %&gt;% torch_matmul(D) torch_matmul(B, D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] 토치의 텐서 곱셈은 다음과 같은 방법들도 있으니 알아두자. torch_mm(B, D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] B$mm(D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] B$matmul(D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] 2.2.7 텐서의 전치(transpose) 전치(transpose)는 주어진 텐서를 뒤집는 것인데, 다음의 문법 구조를 가지고 있다. torch_transpose(input, dim0, dim1) dim0, dim1는 바꿀 차원을 의미한다. ‘바꿀 차원은 두 개 밖에 없지 않나?’ 라고 생각할 수 있다. 2 차원 텐서의 경우에는 그렇다. 우리가 행렬을 전치하는 경우에는 transpose를 취하는 대상이 2차원이므로 지정해주는 차원이 정해져있다. 하지만, 텐서의 차원이 3차원 이상이 되면 전치를 해주는 차원을 지정해줘야한다. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] 위의 텐서 A의 차원은 행과 열, 즉, 2개이다. 다음의 코드들은 A 텐서의 첫번째 차원과 두번째 차원을 뒤집는 효과를 가져온다. 즉, 전치 텐서가 된다. torch_transpose(A, 1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] A$transpose(1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] A %&gt;% torch_transpose(1, 2) ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUDoubleType{3,2} ] 3차원의 텐서를 살펴보자. C ## torch_tensor ## (1,.,.) = ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## ## (2,.,.) = ## 0.1242 0.7489 ## 0.3608 0.5131 ## 0.2959 0.7834 ## [ CPUFloatType{2,3,2} ] 텐서 C는 위와 같이 2차원 텐서가 두 개 포개져 있다고 생각하면 된다. 텐서의 결과물을 잘 살펴보면, 제일 앞에 위치한 1, 2가 나타내는 것이 우리가 흔히 생각하는 2차원 텐서들의 색인(index) 역할을 한다는 것을 알 수 있다. 앞으로는 편의를 위해서 3차원 텐서의 색인 역할을 하는 차원을 깊이(depth)라고 부르도록 하자. 앞에서 주어진 텐서 C 안의 포개져있는 2차원 텐서들을 전치하기 위해서는 이들을 관할(?)하는 두번째와 세번째 차원을 바꿔줘야 한다. torch_transpose(C, 2, 3) ## torch_tensor ## (1,.,.) = ## 0.9628 0.0992 0.0169 ## 0.2943 0.8096 0.8222 ## ## (2,.,.) = ## 0.1242 0.3608 0.2959 ## 0.7489 0.5131 0.7834 ## [ CPUFloatType{2,2,3} ] 결과를 살펴보면, 잘 바뀌어 있음을 알 수 있다. 2.2.8 R에서의 3차원 배열 앞에서 다룬 torch에서의 3차원 텐서 부분은 R에서 기본적으로 제공하는 array의 문법과 차이가 난다. 다음의 코드를 살펴보자. 먼저 R에서 2행 3열의 행렬을 두 개 포개어 놓은 3차원 배열을 만드는 코드이다. array(1:12, c(2, 3, 2)) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 필자는 참고로 matrix()를 만들때에도 byrow 옵션을 써서 만드는 것을 좋아하는데, array()에서 byrow 옵션 효과를 적용하려면 aperm() 함수를 사용해야 한다. 따라서, 좀 더 직관적으로 쓰기위해서 다음의 함수를 사용하자. array_3d_byrow &lt;- function(num_vec, nrow, ncol, ndeath){ aperm(array(num_vec, c(ncol, nrow, ndeath)), c(2, 1, 3)) } E &lt;- array_3d_byrow(1:12, 2, 3, 2) E ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 이러한 코드를 앞서 배웠던 torch_tensor() 함수에 넣어보자. E %&gt;% torch_tensor() ## torch_tensor ## (1,.,.) = ## 1 7 ## 2 8 ## 3 9 ## ## (2,.,.) = ## 4 10 ## 5 11 ## 6 12 ## [ CPULongType{2,3,2} ] 결과를 살펴보면, 우리가 예상했던 2행 3열의 텐서가 두개 겹쳐있는 텐서의 모양이 나오지 않는다는 것을 알 수 있다. 이유는 torch에서 정의된 3차원 텐서의 경우, 첫번째 차원이 텐서가 얼마나 겹쳐있는지를 나타내는 깊이(depth)를 나타내기 때문이다. 문제를 해결하기 위해서는 aperm() 사용해서 차원을 바꿔주면 된다. E %&gt;% aperm(c(3, 1, 2)) %&gt;% # 3 번째 차원을 맨 앞으로, 나머지는 그대로 torch_tensor() ## torch_tensor ## (1,.,.) = ## 1 2 3 ## 4 5 6 ## ## (2,.,.) = ## 7 8 9 ## 10 11 12 ## [ CPULongType{2,2,3} ] 위의 경우를 좀더 직관적인 함수명으로 바꿔서 사용하도록 하자. array_to_torch &lt;- function(mat, n_dim = 3){ torch_tensor(aperm(mat, c(n_dim:3, 1, 2))) } E &lt;- array_to_torch(E) E ## torch_tensor ## (1,.,.) = ## 1 2 3 ## 4 5 6 ## ## (2,.,.) = ## 7 8 9 ## 10 11 12 ## [ CPULongType{2,2,3} ] 2.2.9 다차원 텐서와 1차원 벡터 텐서의 연산 R에서 우리가 아주 애용하는 기능 중 하나가 바로 recycling 개념이다. 즉, 길이 혹은 모양이 맞지 않는 개체(object)들을 연산할 때, 자동으로 길이와 모양을 맞춰서 연산을 해주는 기능인데, torch에서도 이러한 기능을 제공한다. 다음의 코드를 살펴보자. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] A + torch_tensor(1:3) ## torch_tensor ## 2 4 6 ## 5 7 9 ## [ CPUDoubleType{2,3} ] A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] A + torch_tensor(matrix(2:3, ncol = 1)) ## torch_tensor ## 3 4 5 ## 7 8 9 ## [ CPUDoubleType{2,3} ] 2.2.10 1차원 텐서 끼리의 연산, 내적과 외적 1차원 텐서끼리의 연산도 2차원 텐서끼리의 연산과 마찬가지라고 생각하면 된다. 내적과 외적 역시 그냥 모양을 맞춰서 곱하면 된다. A_1 &lt;- A$view(c(1, -1)) A_1 ## torch_tensor ## 1 2 3 4 5 6 ## [ CPUDoubleType{1,6} ] A_2 &lt;- A$view(c(-1, 1)) A_2 ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPUDoubleType{6,1} ] A_1$mm(A_2) ## torch_tensor ## 91 ## [ CPUDoubleType{1,1} ] A_2$mm(A_1) ## torch_tensor ## 1 2 3 4 5 6 ## 2 4 6 8 10 12 ## 3 6 9 12 15 18 ## 4 8 12 16 20 24 ## 5 10 15 20 25 30 ## 6 12 18 24 30 36 ## [ CPUDoubleType{6,6} ] 한가지 주의할 점은 1차원 텐서끼리의 연산이더라도 꼭 차원을 선언해줘서 열벡터와 행벡터를 분명히 해줘야 한다는 점이다. A_3 &lt;- torch_tensor(1:6) A_1$mm(A_3) ## Error in (function (self, mat2) : mat2 must be a matrix ## Exception raised from mm_cpu at ../aten/src/ATen/native/LinearAlgebra.cpp:399 (most recent call first): ## frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x69 (0x7f0dd8f17b89 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libc10.so) ## frame #1: at::native::mm_cpu(at::Tensor const&amp;, at::Tensor const&amp;) + 0x334 (0x7f0dc8d66194 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #2: &lt;unknown function&gt; + 0x133236d (0x7f0dc928636d in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #3: &lt;unknown function&gt; + 0xaf1c34 (0x7f0dc8a45c34 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #4: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;) const + 0x1ce (0x7f0dc942e24e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #5: at::mm(at::Tensor const&amp;, at::Tensor const&amp;) + 0xb7 (0x7f0dc9314947 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #6: &lt;unknown function&gt; + 0x2a5db24 (0x7f0dca9b1b24 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #7: &lt;unknown function&gt; + 0xaf1c34 (0x7f0dc8a45c34 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #8: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;) const + 0x1ce (0x7f0dc942e24e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #9: at::Tensor::mm(at::Tensor const&amp;) const + 0xb7 (0x7f0dc9597d67 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #10: _lantern_Tensor_mm_tensor_tensor + 0x4c (0x7f0dd925579c in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/liblantern.so) ## frame #11: cpp_torch_method_mm_self_Tensor_mat2_Tensor(Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;) + 0x2c (0x7f0dd9b874fc in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #12: _torch_cpp_torch_method_mm_self_Tensor_mat2_Tensor + 0x82 (0x7f0dd992cf22 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #13: &lt;unknown function&gt; + 0xf932c (0x7f0dec56032c in /usr/lib/R/lib/libR.so) ## frame #14: &lt;unknown function&gt; + 0xf9826 (0x7f0dec560826 in /usr/lib/R/lib/libR.so) ## frame #15: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #16: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #17: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #18: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #19: Rf_eval + 0x353 (0x7f0dec5aa8c3 in /usr/lib/R/lib/libR.so) ## frame #20: &lt;unknown function&gt; + 0xc650d (0x7f0dec52d50d in /usr/lib/R/lib/libR.so) ## frame #21: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #22: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #23: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #24: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #25: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #26: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #27: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #28: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #29: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #30: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #31: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #32: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #33: Rf_eval + 0x353 (0x7f0dec5aa8c3 in /usr/lib/R/lib/libR.so) ## frame #34: &lt;unknown function&gt; + 0x1470a2 (0x7f0dec5ae0a2 in /usr/lib/R/lib/libR.so) ## frame #35: Rf_eval + 0x572 (0x7f0dec5aaae2 in /usr/lib/R/lib/libR.so) ## frame #36: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #37: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #38: Rf_eval + 0x353 (0x7f0dec5aa8c3 in /usr/lib/R/lib/libR.so) ## frame #39: &lt;unknown function&gt; + 0x149782 (0x7f0dec5b0782 in /usr/lib/R/lib/libR.so) ## frame #40: &lt;unknown function&gt; + 0x137106 (0x7f0dec59e106 in /usr/lib/R/lib/libR.so) ## frame #41: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #42: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #43: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #44: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #45: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #46: &lt;unknown function&gt; + 0x1440ac (0x7f0dec5ab0ac in /usr/lib/R/lib/libR.so) ## frame #47: Rf_eval + 0x454 (0x7f0dec5aa9c4 in /usr/lib/R/lib/libR.so) ## frame #48: &lt;unknown function&gt; + 0x14a22c (0x7f0dec5b122c in /usr/lib/R/lib/libR.so) ## frame #49: &lt;unknown function&gt; + 0x1871fd (0x7f0dec5ee1fd in /usr/lib/R/lib/libR.so) ## frame #50: &lt;unknown function&gt; + 0x1353c4 (0x7f0dec59c3c4 in /usr/lib/R/lib/libR.so) ## frame #51: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #52: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #53: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #54: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #55: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #56: &lt;unknown function&gt; + 0x1440ac (0x7f0dec5ab0ac in /usr/lib/R/lib/libR.so) ## frame #57: &lt;unknown function&gt; + 0x1444e4 (0x7f0dec5ab4e4 in /usr/lib/R/lib/libR.so) ## frame #58: &lt;unknown function&gt; + 0x1377d4 (0x7f0dec59e7d4 in /usr/lib/R/lib/libR.so) ## frame #59: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) ## frame #60: &lt;unknown function&gt; + 0x14550f (0x7f0dec5ac50f in /usr/lib/R/lib/libR.so) ## frame #61: Rf_applyClosure + 0x1c7 (0x7f0dec5ad2d7 in /usr/lib/R/lib/libR.so) ## frame #62: &lt;unknown function&gt; + 0x13a989 (0x7f0dec5a1989 in /usr/lib/R/lib/libR.so) ## frame #63: Rf_eval + 0x180 (0x7f0dec5aa6f0 in /usr/lib/R/lib/libR.so) 위의 코드는 연산 에러가 나는데, 이유는 A_3의 모양이 A_1의 모양과 맞지 않기 때문이다. A_1$size() ## [1] 1 6 A_3$size() ## [1] 6 "],["텐서의-이동-cpu-leftrightarrow-gpu.html", "Chapter 3 텐서의 이동; CPU \\(\\leftrightarrow\\) GPU 3.1 GPU 사용 가능 체크 3.2 CPU to GPU 3.3 GPU to CPU", " Chapter 3 텐서의 이동; CPU \\(\\leftrightarrow\\) GPU 딥러닝(deep learning)에서는 네트워크의 구조가 조금만 복잡해져도, 필요한 계산량이 엄청나게 늘어나기 때문에 GPU는 사실 필수적이다. torch 패키지에서는 텐서를 다룰때에 현재 다루는 텐서가 어디에 저장되어있는가에 대한 일종의 태그를 달아놓는다. 다음의 코드를 살펴보자. a &lt;- torch_tensor(1:4) a ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] a는 3이라는 상수가 담겨있는 텐서이다. 이 a를 콘솔에서 돌렸을때에 나오는 결과 [ CPUFloatType{1} ]를 통해서 우리는 a가 현재 CPU의 메모리를 이용하고 있으며, 모양은 {1}인 실수을 담은 텐서라는 것을 알 수 있다. 3.1 GPU 사용 가능 체크 앞서 정의한 텐서 a를 GPU의 메모리로 옮기기 위해서는, 너무나 당연하게 GPU가 현재 시스템에서 접근 가능한지에 대하여 알아보아야한다. GPU 접근성은 cuda_is_available()을 사용한다. cuda_is_available() ## [1] TRUE 3.2 CPU to GPU 이미 정의된 텐서 a를 GPU로 옮기려면 다음과 같이 cuda() 함수를 이용하면 된다. a ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] a$cuda() ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] gpu &lt;- torch_device(&quot;cuda&quot;) a$to(device = gpu) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] 옮길 때에 dtype을 사용하여 다음과 같이 자료형을 바꿔줄 수도 있다. a$to(device = gpu, dtype = torch_double()) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDADoubleType{4} ] 3.3 GPU to CPU GPU 상에 직접 텐서를 만드는 방법은 다음과 같다. b &lt;- torch_tensor(1:4, device=gpu) b ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CUDALongType{4} ] 이전 섹션에서 CPU에서 GPU로 옮기는 방법과 비슷하게 다음의 코드가 작동한다. b$cpu() ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] # to 함수 이용 cpu &lt;- torch_device(&quot;cpu&quot;) a$to(device = cpu) ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## [ CPULongType{4} ] "],["r6.html", "Chapter 4 R6와 텐서 4.1 시작하기 4.2 클래스(Class)와 멤버함수(Method), 그리고 필드(Field) 4.3 상속(Inheritance) - 클래스 물려받기 4.4 공개(Public)정보와 비공개(Private) 정보의 필요성 4.5 텐서와 R6의 관계 4.6 R6 관련자료", " Chapter 4 R6와 텐서 torch의 코드를 살펴보면 우리가 늘상 사용하던 R의 패키지들과는 어딘가 다른점이 있다고 느껴질 것이다. 이것의 근본적인 이유는 바로 torch 패키지가 객체지향언어 (Object Oriented Programming; OOP)를 할 수 있도록 해주는 R6 패키지를 기반으로 하고있기 때문이다. 좀 더 직접적으로 말하면, torch의 텐서와 신경망들이 R6 패키지의 클래스들로 정의되어 있기 때문에, 일반적인 R 패키지들보다 $을 통한 함수(OOP에서는 method 라고 부른다.) 접근이 가능하다. 어떤 이야기인지 한번 좀 더 깊게 들어가보자. 4.1 시작하기 여느 패키지와 다를바가 없다. R6 패키지를 설치하도록 하자. # install.packages(&quot;R6&quot;) library(R6) 4.2 클래스(Class)와 멤버함수(Method), 그리고 필드(Field) R6 패키지에는 딱 하나의 함수가 존재한다. 바로 R6Class() 함수이다. 이 함수의 입력값은 두가지 인데, 첫번째는 클래스 이름 clasename이고, 두번째는 공개될 정보들을 담을 public이라는 입력값이다. public에는 우리가 만들 클래스에서 사용이 가능한 멤버함수들(methods)과 변수(fields)들을 몽땅 다 떼려넣은 리스트(list) 형태가 들어간다. ExampleClass &lt;- R6Class(classname = &quot;Example&quot;, public = list( # 변수(fields) 정의 # 멤버함수(methods) 정의 )) ExampleClass ## &lt;Example&gt; object generator ## Public: ## clone: function (deep = FALSE) ## Parent env: &lt;environment: R_GlobalEnv&gt; ## Locked objects: TRUE ## Locked class: FALSE ## Portable: TRUE 한가지 꼭 짚고 넘어가야하는 것이 있는데, 바로 이름을 정하는 방식이다. 클래스의 이름은 UpperCamelCase 형식으로 짓는다. 즉, 클래스의 이름을 선언할 때 띄어쓰기를 하지않고, 대신 대문자를 사용한다. 두번째 리스트에 들어가는 요소들의 이름은 snake_case를 사용한다. 즉, 모두 소문자를 유지하고, 띄어쓰기 대신에 밑줄을 사용하여 선언한다. 이렇게 규칙을 따라서 작성하게 되면, 나중에 다른 사람이 짜놓은 코드를 보게 되더라도, 선언된 이름의 구조를 보고, 이게 클래스인지, 클래스 안에 정의된 함수 혹은 변수인지를 구분 할 수 있어서 좋다. 4.2.1 클래스는 왜 필요할까? 필자도 클래스의 개념을 처음 들었을때 대체 이게 무슨 소리인지.. 했던 기억이 있다. 심지어 필자의 경우 R밖에 모르던 터여서, OOP가 필요가 있는지에 대한 의문까지 들 정도였으니, (사실 지금도 생각이 많이 바뀌지 않았다.) 머리에 아예 들어오지를 않았다. 그런 필자를 클래스 개념에 대하여 한방에 이해시킨 예제가 바로 학생 클래스이다. 자고로 모든 개념은 예를 들어 설명을 하는 것이 아주 효과적이라고 필자는 믿고있다. &gt; 목표: OOP의 개념와 왜 사용을 하는지에 대하여 이해한다. 4.2.2 학생자료 입력 예제 다음의 코드를 생각해보자. student &lt;- function(){ list() } issac &lt;- student() bomi &lt;- student() issac ## list() bomi ## list() student라는 함수는 빈 리스트를 반환을 하는데, 우리가 이 함수를 사용하여 issac과 bomi라는 학생의 정보를 담는 리스트를 만들 수 있다. 만약 우리가 다음과 같은 추가 정보를 저장하려고 한다고 가정해보자. issac last name: Lee first name: Issac email: issac-lee@gmail.com midterm: 70 final: 50 bomi last name: Kim first name: Bomi email: bomi-kim@gmail.com midterm: 65 final: 80 위의 정보를 저장하기 위해서는 다음과 같이 $ 기호를 통하여 저장할 수 있다. issac$first &lt;- &quot;Issac&quot; issac$last &lt;- &quot;Lee&quot; issac$email &lt;- &quot;issac-lee@gmail.com&quot; issac$midterm &lt;- 70 issac$final &lt;- 50 bomi$first &lt;- &quot;Bomi&quot; bomi$last &lt;- &quot;Kim&quot; bomi$email &lt;- &quot;bomi-kim@gmail.com&quot; bomi$midterm &lt;- 65 bomi$final &lt;- 80 issac ## $first ## [1] &quot;Issac&quot; ## ## $last ## [1] &quot;Lee&quot; ## ## $email ## [1] &quot;issac-lee@gmail.com&quot; ## ## $midterm ## [1] 70 ## ## $final ## [1] 50 bomi ## $first ## [1] &quot;Bomi&quot; ## ## $last ## [1] &quot;Kim&quot; ## ## $email ## [1] &quot;bomi-kim@gmail.com&quot; ## ## $midterm ## [1] 65 ## ## $final ## [1] 80 위의 코드는 OOP관점에서 상당히 중복 코드가 많은 비효율적인 코드이다. 이러한 코드를 우리가 배운 R6Class()를 사용하여 어떻게 줄일 수 있는지 알아보자. 4.2.3 클래스(Class) 정의하기 앞에서 우리는 issac과 bomi라는 변수를 생성했는데, 둘의 공통점은 학생이라는 점이었다. 사실 앞선 코드를 작성을 한다는 것은 issac과 bomi 뿐 아니라 엄청 많은 수의 학생들에 대한 데이터를 다루고 있는 상황일 수도 있다. 우리들이 써놓은 코드를 잘 뜯어보니, 학생 데이터로 입력되는 각 개인들은 성과 이름, 이메일, 그리고, 중간, 기말고사 점수의 정보들을 가지고 있다. 즉, 학생, Student, 라는 클래스는 항상 성(last)과 이름(first), 중간(midterm), 기말고사(final) 성적이 저장되어 있고, 이메일의 경우 이름과 성을 이용해서 작성을 하되, 모두 소문자로 입력된 자료 형태를 가지고 있는 구조를 갖는 어떤 추상적인 개념이라는 것을 알 수 있다. 이러한 정보를 사용하여 우리는 다음과 같이 Student 클래스를 선언 할 수 있다. Student &lt;- R6Class(&quot;Student&quot;, list( # 필요한 변수 (field) 선언 first = NULL, last = NULL, email = NULL, midterm = NA, final = NA, # 클래스 안의 객체를 만들때 사용되는 initialize initialize = function(first, last, midterm, final){ self$first = first self$last = last self$email = glue::glue(&quot;{tolower(first)}-{tolower(last)}@gmail.com&quot;) self$midterm = midterm self$final = final } )) Student ## &lt;Student&gt; object generator ## Public: ## first: NULL ## last: NULL ## email: NULL ## midterm: NA ## final: NA ## initialize: function (first, last, midterm, final) ## clone: function (deep = FALSE) ## Parent env: &lt;environment: R_GlobalEnv&gt; ## Locked objects: TRUE ## Locked class: FALSE ## Portable: TRUE 결과값을 유심히 살펴보면, &lt;Student&gt; object generator 라는 부분이 있는데, Student 라는 클래스는 객체(object)들을 만들어내는 생성자(generator)라는 것을 알 수 있다. 우리가 만들 Student 생성자를 통해서 도장을 찍듯, new() 함수를 사용하여 issac과 bomi를 다음과 같이 만들 수 있다. issac &lt;- Student$new(&quot;Issac&quot;, &quot;Lee&quot;, 70, 50) bomi &lt;- Student$new(&quot;Bomi&quot;, &quot;Kim&quot;, 65, 80) issac ## &lt;Student&gt; ## Public: ## clone: function (deep = FALSE) ## email: issac-lee@gmail.com ## final: 50 ## first: Issac ## initialize: function (first, last, midterm, final) ## last: Lee ## midterm: 70 bomi ## &lt;Student&gt; ## Public: ## clone: function (deep = FALSE) ## email: bomi-kim@gmail.com ## final: 80 ## first: Bomi ## initialize: function (first, last, midterm, final) ## last: Kim ## midterm: 65 즉, OOP의 장점은 공을 들여 한번 클래스를 잘 만들어놓으면, 한번 작성된 함수나 변수들의 재 사용율이 엄청 좋아지는 것이다. 4.2.4 print()를 사용한 결과물 정리 정의된 클래스는 기본적으로 동작하는 함수들을 덮어서 쓸 수 있다. 예를들어 print()를 함수로 정의해버리면, base에 있는 print() 동작을 덮어서 쓸 수 있다. 즉, 기본 함수들 print(), plot() 같은 함수들을 우리가 정의한 클래스에서 나온 객체들에 적용했을때의 작동을 정해줄 수 있다는 것이다. Student &lt;- R6Class(&quot;Student&quot;, list( # 필요한 변수 (field) 선언 first = NULL, last = NULL, email = NULL, midterm = NA, final = NA, # 클래스 안의 객체를 만들때 사용되는 initialize initialize = function(first, last, midterm, final){ self$first = first self$last = last self$email = glue::glue(&quot;{tolower(first)}-{tolower(last)}@gmail.com&quot;) self$midterm = midterm self$final = final }, print = function(...){ cat(&quot;Student: \\n&quot;) cat(glue::glue(&quot; Name : {self$first} {self$last} E-mail: {self$email} Midterm Score : {self$midterm} Final Score: {self$final} &quot;)) invisible(self) } )) soony &lt;- Student$new(&quot;Soony&quot;, &quot;Kim&quot;, 70, 20) soony ## Student: ## Name : Soony Kim ## E-mail: soony-kim@gmail.com ## Midterm Score : 70 ## Final Score: 20 print() 멤버 함수를 추가한 후에 만들어진 soony의 정보는 클래스안에 정의된 print()를 통해서 보여진다는 것을 확인할 수 있다. 한가지 주의할 점은 print()가 클래스 안에 정의되어 있지 않은 채로 생성된 issac과 bomi의 경우는 print()가 작동하지 않는다는 것이다. 즉, 클래스에 정의된 함수들은 객체가 클래스로부터 생성될 때, 따라와서 붙는다. issac$print() ## Error in eval(expr, envir, enclos): attempt to apply non-function soony$print() ## Student: ## Name : Soony Kim ## E-mail: soony-kim@gmail.com ## Midterm Score : 70 ## Final Score: 20 4.2.5 set을 이용한 클래스 조정 앞에서 우리는 print() 함수를 추가하기 위하여 전체 클래스를 다시 정의하였다. 하지만, 이렇게 클래스안에 함수를 추가하기 위해서 전체 클래스를 다시 정의하기보단, set()을 이용해서 변수나 함수를 추가할 수 있다. Student$set(&quot;public&quot;, &quot;total&quot;, NA) Student$set(&quot;public&quot;, &quot;calculate_total&quot;, function(){ self$total &lt;- self$midterm + self$final invisible(self) }) invisible() 함수는 결과를 반환하되, 결과물을 보여주지 않는 것인데, 클래스에서 함수를 정의할 때에 반드시 invisible(self)를 반환해줘야만 한다. 따라서 함수이지만, 함수와는 다른 이 클래스 안의 함수들을 멤버함수 method()라고하여 일반 함수와 구분을 지어서 부른다. jelly &lt;- Student$new(&quot;Jelly&quot;, &quot;Lee&quot;, 35, 23) jelly ## Student: ## Name : Jelly Lee ## E-mail: jelly-lee@gmail.com ## Midterm Score : 35 ## Final Score: 23 jelly$total ## [1] NA jelly$calculate_total() jelly$total ## [1] 58 4.3 상속(Inheritance) - 클래스 물려받기 OOP가 코드의 중복을 되도록 피할 수 있도록 설계되어 있다는 것을 어렴풋이나마 앞의 예제를 통하여 알 수 있을 것이다. 이러한 OOP의 코드 재사용 관점에서 상속(Inheritance)의 개념은 꽃 중에 꽃이라 불릴 만하다. 단 한 줄의 코드로 미리 작성해놓은 함수들에 접근이 가능하기 때문이다. 상속(Inheritance)이라고 하면 뭔가 거창할 것 같지만, 그냥 미리 정의해둔 클래스의 정보(멤버함수과 필드)를 다른 클래스를 정의할 때 받아올 수 있다는 말이다. 예를 들어보자. 이제까지 사용해 온 학생 개념, Student 클래스를 좀 더 세분화를 한다면 학교별로 나눌 수 있을 것이다. Student 클래스를 상속받는 슬통대학교(University of Statistics Playbook; USP) 학생들을 위한 서브 클래스(sub class)는 다음과 같이 생성할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, average = NA, calculate_average = function(){ self$average &lt;- mean(c(self$midterm, self$final)) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ) ) sanghoon &lt;- UspStudent$new(&quot;Sanghoon&quot;, &quot;Park&quot;, 80, 56) sanghoon ## Student: ## Name : Sanghoon Park ## E-mail: sanghoon-park@gmail.com ## Midterm Score : 80 ## Final Score: 56 새로 정의된 UspStudent 클래스는 상위 클래스인 Student 클래스의 멤버함수들과 변수들을 그대로 물려받는다. 여기서 코드의 재사용성이 증가한다. 또한 상위 클래스가 가지고 있던 calculate_total() 멤버함수에 접근하여, 새롭게 고쳐서 사용하는 것도 가능하다. 다음은 정의된 멤버함수들을 사용하여 변수들에 계산을 해서 넣는 과정을 보여준다. sanghoon$university_name ## [1] &quot;University of Statistics Playbook&quot; sanghoon$calculate_average() sanghoon$average ## [1] 68 sanghoon$calculate_total() ## The total score of midterm and final exam is calculated. sanghoon$total ## [1] 136 4.4 공개(Public)정보와 비공개(Private) 정보의 필요성 앞에서 살펴본 R6Class() 함수의 두 가지 입력값은 클래스 이름(classname)과 공개정보(public) 였다. 클래스를 만들고 사용하다보면, 때로는 클래스 안의 함수들을 사용하기 위해서 만들어야하는 변수나 함수들이 있는데, 이러한 정보들은 굳이 클래스를 사용하는 사용자들에게 보여줄 필요가 없다. 우리네 인생도 그러하다. 우리는 때로는 너무 많은 정보 제공에 피로감과 불편을 겪는 경우가 많다. 따라서, 클래스에 대한 정보의 접근을 적절하게 조절할 필요가 있는데, 클래스의 정보들을 공개될 정보(public)와 비공개 정보(private)들로 분류함으로써 조절할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, calculate_average = function(){ private$.average &lt;- mean(c(self$midterm, self$final)) cat(&quot;Average score is&quot;, private$.average) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ), private = list( .average = NA ) ) taemo &lt;- UspStudent$new(&quot;Taemo&quot;, &quot;Bang&quot;, 80, 56) taemo$calculate_average() ## Average score is 68 위의 UspStudent 클래스에는 비공개 정보가 하나들어있다. 바로 중간 기말고사 점수의 평균을 저장하는 average 변수인데, 클래스의 정의시 private()에 감싸져서 입력이 되었음에 주목하자. average 변수는 클래스 안에서의 멤버함수를 통해서 접근할 땐 private$name 형식으로 접근이 가능함에 반하여, 클래스를 사용하는 사용자 입장에서는 가려져서 보이지 않는 정보에 해당한다. taemo$.average ## NULL 해들리 위캠의 말을 빌리면, 공개-비공개 정보의 구분은 큰 패키지나 클래스를 정의할 때 가장 중요한 단계가 된다. 왜냐하면 비공개 정보의 경우는 개발자의 입장에서 언제든지 수정할 수 있는 정보가 되지만, 공개된 멤버함수나 필드들에 대해서는 쉽게 바꿀 수가 없기 때문이다. &gt; 여기서 하나 짚고 넘어가면 좋은 것이 있는데, 바로 R에서의 이름 짓기 방식이다. R의 기본 함수들 중에서 .을 사용해서 지어진 경우가 있는데, 현재는 권장하지 않고 있다. 이유는 바로 비공개 정보를 갖는 변수나 함수들을 나타내는데에 .을 찍어서 나타내기 때문이다. `.average` 역시 변수의 이름에서 이 변수는 클래스 안에서만 접근이 가능하다는 것을 변수 이름만 보고도 알 수 있도록 만들어졌다. 4.4.1 활성 변수(active field)를 사용한 읽기 전용 변수 Advance R의 14장의 내용을 보면, R6의 접근성을 다루면서 active field의 개념이 나온다. 자세한 내용이 궁금한 독자들은 찾아보기 바란다. active field의 좋은 점은 이것을 사용해서 클래스 사용자들에게 읽기 전용 정보를 제공해줄수 있기 때문이다. 앞에서의 예를 들어보면 중간, 기말고사의 평균 정보는 클래스 사용자들에게 유요한 정보가 될 수 있다. 하지만, private으로 감싸버리면 사용자들은 이 정보에 접근을 할 수 없게 된다. 사용자는 평균 정보에 접근하고 싶어하지만, 개발자의 입장에서는 쉽게 공개정보로 바꾸기가 쉽지 않다. 왜냐하면 사용자들이 마음대로 평균 변수에 접근해서 정보를 변경시켜버리면 클래스에서 평균 정보를 가져다가 쓰는 멤버함수들이 잘 작동하지 않을 수 있기 때문이다. 이럴 경우 active field를 사용해서 average를 읽기전용으로만 접근 가능하도록 설계할 수 있다. UspStudent &lt;- R6Class(&quot;UspStudent&quot;, inherit = Student, ## active field active = list( average = function(value) { if (missing(value)) { private$.average } else { stop(&quot;`$average` is read only&quot;, call. = FALSE) } } ), public = list( university_name = &quot;University of Statistics Playbook&quot;, class_year = NA, calculate_average = function(){ private$.average &lt;- mean(c(self$midterm, self$final)) cat(&quot;Average score is&quot;, private$.average) invisible(self) }, calculate_total = function(){ cat(&quot;The total score of midterm and final exam is calculated. \\n&quot;) super$calculate_total() } ), private = list( .average = NA ) ) conie &lt;- UspStudent$new(&quot;Connie&quot;, &quot;&quot;, 78, 82) conie$calculate_average() ## Average score is 80 conie$average ## [1] 80 위에서 정의된 UspStudent 클래스에서는 사용자에게 평균값을 구하는 함수와 구한 평균값에 접근을 허용하지만, 사용자가 average값에 접근하여 바꾸려고 하면 에러를 뱉어내도록 설계가 되어있다. conie$average &lt;- 60 ## Error: `$average` is read only 4.5 텐서와 R6의 관계 4.6 R6 관련자료 R6에 대한 더 깊은 내용은 Hadley Wickham의 Advanced R과 R6 패키지의 웹사이트를 참고하도록 하자. "],["forward.html", "Chapter 5 순전파 (Forward propagation) 5.1 신경망의 구조 5.2 순전파(Forward propagation)", " Chapter 5 순전파 (Forward propagation) 5.1 신경망의 구조 딥러닝의 시작점인 신경망(Neural network)을 공부하기 위해서, 앞으로 우리가 다룰 모델 중 가장 간단하면서, 딥러닝에서 어떤 일이 벌어지고 있는지 상상이 가능한 신경망을 먼저 학습하기로 하자. 우리가 오늘 예로 생각할 신경망은 다음과 같다. Figure 5.1: 세상에서 가장 간단하지만 있을 건 다있는 신경망 위의 그림과 같은 신경망을 2단 신경망이라고 부른다. 일반적으로 단수를 셀 때 제일 처음 입력하는 층은 단수에 포함하지 않는 것에 주의하자. 각 녹색, 회색, 그리고 빨간색의 노드(node)들은 신경망의 요소를 이루는데, 각각의 이름은 다음과 같다. 입력층(input layer) - 2개의 녹색 노드(node) 은닉층(hidden layer) - 3개의 회색 노드(node) 출력층(output layer) - 1개의 빨강색 노드(node) 자 이제부터, 녹색 노드에는 무엇이 들어가는지, 그리고, 어떤 과정을 거쳐서 빨강색의 값이 나오는지에 대하여 알아보자. 딥러닝에서 녹색이 입력값을 넣어서 빨간색의 결과값을 얻는 과정을 순전파(Forward propagation)라고 부른다. propagation의 뜻은 증식, 혹은 번식인데, 식물이나 동물이 자라나는 것을 의미하는데, 녹색의 입력값들이 어떠한 과정을 거쳐 빨간색으로 자라나는지 한번 알아보자. 5.2 순전파(Forward propagation) 우리가 사용할 데이터 역시 아주 간단하다. \\[ X =\\left(\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; 6 \\end{array}\\right) \\] 가로 행이 하나의 표본을 의미하고, 세로 열 각각은 변수를 의미한다. 즉, 위의 자료 행렬은 2개의 변수 정보가 들어있는 세 개의 표본들이 있는 자료을 의미한다. 5.2.1 표본 1개, 경로 1개만 생각해보기 주의할 것은, 우리가 그려놓은 신경망의 입력층의 노드는 2개이고, 자료 행렬은 3행 2열이라는 것이다. 우리가 그려놓은 신경망으로 샘플 하나 하나가 입력층에 각각 입력되어 표본별 결과값 생성되는 것이다. 따라서 신경망을 잘 이해하기 위해서 딱 하나의 표본, 그리고 딱 하나의 경로만을 생각해보자. &gt; 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 경로를 타고 어떻게 자라나는지 생각해보자. Figure 5.2: 예시 경로 1 그림에서 \\(\\beta\\)는 노드와 노드 사이를 지나갈 때 부여되는 웨이트들을 의미하고, \\(\\sigma()\\)는 다음의 시그모이드(sigmoid) 함수를 의미한다. \\[ \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x+1} \\] 자료 행렬을 위에 색칠된 경로로 보낸다는 의미는 다음과 같은 계산과정을 거친다는 것이다. set.seed(1234) # 데이터 매트릭스 # 3 by 2 X &lt;- torch_tensor(matrix(1:2, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## [ CPUDoubleType{1,2} ] # beta_1 벡터 # 2 by 1 # 1번째 레이어에 관한 웨이트 (베타) 중 # 다음 레이어의 1번째 노드에 대한 베타 벡터에 부여 # beta_1 = (beta_11, beta_12) beta_1 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_1 ## torch_tensor ## 0.1137 ## 0.6223 ## [ CPUDoubleType{2,1} ] # 2번째 레이어 1번째 노드 # 3 by 1 z_21 &lt;- X$mm(beta_1) z_21 ## torch_tensor ## 1.3583 ## [ CPUDoubleType{1,1} ] # 2번째 레이어 1번째 노드에서의 시그모이드 함수 통과 # 3 by 1 library(sigmoid) a_21 &lt;- sigmoid(z_21) a_21 ## torch_tensor ## 0.7955 ## [ CPUDoubleType{1,1} ] # 2번째 레이어에 관한 웨이트 (감마) 중 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # beta_1 상수 1 by 1 gamma_1 &lt;- runif(1) # 3번째 레이어 1번째 노드 # 3 by 1 z_31 &lt;- a_21 * gamma_1 z_31 ## torch_tensor ## 0.4847 ## [ CPUDoubleType{1,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 3 by 1 y_hat &lt;- sigmoid(z_31) y_hat ## torch_tensor ## 0.6188 ## [ CPUDoubleType{1,1} ] 즉, 우리가 생각하는 표본은 빨간색 노드에 도착하기 위해서 두번째 은닉층의 첫번째 노드를 통과하여 올 수 있다. 하지만 빨간색 노드에는 방금 우리가 생각한 경로 뿐만아니라 두 개의 선택지가 더 존재한다. 5.2.2 1개의 표본, 경로 한꺼번에 생각하기 세가지의 경로를 모두 생각해보면, 우리의 표본은 다음의 경로를 통해서 도착한다. &gt; 목표: 첫번째 표본인 $(1, 2)$가 다음과 같은 세가지 경로를 타고 어떻게 하나로 합쳐지는지 이해해보자. Figure 5.3: 3가지 경로 이 과정을 우리가 통계 시간에 배운 회귀분석에 연결지어 생각해보면, 다음의 해석이 가능하다. 두번째 은닉층의 각각의 노드들이 하나의 회귀분석 예측 모델들이라고 생각하면, 신경망은 세 개의 회귀분석을 한 대 모아놓은 거대한 회귀분석 집합체라고 생각할 수 있게 된다. 즉, 각 회귀분석 모델들이 예측한 표본에 대한 대응변수 예측값들을 은닉층에 저장한 후, 그 예측값들을 모두 모아 마지막 빨간색 노드에서 합치면서 좀 더 좋은 예측값을 만들어 내는 것이다. 이 때, \\(\\gamma\\) 벡터를 통해 가중치를 부여하는 것이라고 해석이 가능하다. 이 과정을 torch 텐서를 사용하여 깔끔하게 나타내보자. # 1개 표본 # 1 by 2 X &lt;- torch_tensor(matrix(1:2, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## [ CPUDoubleType{1,2} ] # 베타벡터가 세 개 존재함. # 2 by 3 beta_1 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_2 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) beta_3 &lt;- torch_tensor(matrix(runif(2), ncol = 1), dtype = torch_double()) # 정의된 베타벡터를 cbind in torch beta &lt;- torch_cat(c(beta_1, beta_2, beta_3), 2) beta ## torch_tensor ## 0.6234 0.6403 0.2326 ## 0.8609 0.0095 0.6661 ## [ CPUDoubleType{2,3} ] # 2번째 레이어 z_2 # 1 by 3 z_2 &lt;- X$mm(beta) z_2 ## torch_tensor ## 2.3452 0.6593 1.5647 ## [ CPUDoubleType{1,3} ] # 2번째 레이어 sigmoid 함수 통과 # 1 by 3 a_2 &lt;- sigmoid(z_2) # 2번째 레이어에 관한 웨이트 (감마) 벡터 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # gamma vector 3 by 1 gamma_1 &lt;- runif(1) gamma_2 &lt;- runif(1) gamma_3 &lt;- runif(1) gamma &lt;- torch_tensor(matrix(c(gamma_1, gamma_2, gamma_3), ncol = 1), dtype = torch_double()) # 3번째 레이어 z_3 # 1 by 1 z_3 &lt;- a_2$mm(gamma) z_3 ## torch_tensor ## 1.3771 ## [ CPUDoubleType{1,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 1 by 1 y_hat &lt;- sigmoid(z_3) y_hat ## torch_tensor ## 0.7985 ## [ CPUDoubleType{1,1} ] R에서 우리가 즐겨쓰던 cbind()와 rbind()는 torch에서는 torch_cat() 하나의 함수으로 구현이 가능하다. 함수의 두번째 입력값은 숫자 1은 행방향(rbind)에, 2는 열방향(cbind)과 대응된다. 5.2.3 전체 표본, 경로 전체 생각해보기 이제 자료 행렬 전체를 한꺼번에 넣는 방법을 생각해보자. 입력값이 자료 행렬 전체이므로, 결과값은 이에 대응하도록 행의 갯수와 같은 벡터 형식이 될 것이라는 것을 예상하고 코드를 따라오도록 하자. &gt; 목표: 전체 표본이 신경망을 통해서 예측되는 구조를 이해하자. # 데이터 텐서 # 3 by 2 X &lt;- torch_tensor(matrix(1:6, ncol = 2, byrow = T), dtype = torch_double()) X ## torch_tensor ## 1 2 ## 3 4 ## 5 6 ## [ CPUDoubleType{3,2} ] # 베타벡터가 세 개 존재함. # 2 by 3 beta &lt;- torch_tensor(matrix(runif(6), ncol = 3), dtype = torch_double()) beta ## torch_tensor ## 0.2827 0.2923 0.2862 ## 0.9234 0.8373 0.2668 ## [ CPUDoubleType{2,3} ] # 2번째 레이어 z_2 # 3 by 3 z_2 &lt;- X$mm(beta) z_2 ## torch_tensor ## 2.1296 1.9669 0.8199 ## 4.5419 4.2261 1.9260 ## 6.9543 6.4854 3.0320 ## [ CPUDoubleType{3,3} ] # 2번째 레이어 sigmoid 함수 통과 # 3 by 3 a_2 &lt;- sigmoid(z_2) # 2번째 레이어에 관한 웨이트 (감마) 벡터 # 다음 레이어의 1번째 노드에 대한 베타값에 임의의 값을 부여 # gamma vector 3 by 1 gamma &lt;- torch_tensor(matrix(runif(3), ncol = 1), dtype = torch_double()) # 3번째 레이어 z_3 # 3 by 1 z_3 &lt;- a_2$mm(gamma) z_3 ## torch_tensor ## 0.5904 ## 0.6900 ## 0.7205 ## [ CPUDoubleType{3,1} ] # 마지막 레이어에서 시그모이드 함수 통과 # 3 by 1 y_hat &lt;- sigmoid(z_3) y_hat ## torch_tensor ## 0.6435 ## 0.6660 ## 0.6727 ## [ CPUDoubleType{3,1} ] "],["미분-자동추적-기능-autograd-에-대하여.html", "Chapter 6 미분 자동추적 기능 (Autograd) 에 대하여 6.1 예제 함수 6.2 데이터 생성 6.3 함수 만들기 및 오차 그래프 6.4 Autograd 기능 없이 기울기 구하기 6.5 자동미분(Autograd) 기능 6.6 자동 미분 관련 함수들 6.7 경사하강법", " Chapter 6 미분 자동추적 기능 (Autograd) 에 대하여 이번 장에서는 torch 및 다른 딥러닝 라이브러리의 근본을 이루는 기능인 미분 자동 추적 기능에 대하여 알아보도록 하자. 예를 들어 설명하는 것을 좋아하므로, 이번 챕터에 쓸 예제 함수를 먼저 정의하자. 6.1 예제 함수 \\(n\\)개의 데이터 \\(x_1, ..., x_n\\)이 주어졌다고 할 때, 우리는 다음의 함수 \\(f\\)를 정의 할 수 있다. \\[ f(\\mu) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2 \\] 위의 함수는 다음과 같이 해석해 볼 수 있다. \\(x\\) 데이터에 담겨있는 정보를 단 하나의 지표 \\(\\mu\\)로 압축해서 나타낸다고 할 때, 함수 \\(f\\)는 각 관찰값에 대한 오차들, \\(x_i - \\mu\\),의 제곱의 평균을 나타낸다. 통계학에서는 나름 유명한 함수인데, 왜냐하면 위의 함수값을 최소화시키는 \\(\\mu\\)를 찾게되면 표본평균(\\(\\bar{x}\\)) 나오기 때문이다. 오늘은 이 함수를 통하여 torch의 자동 미분 기능에 대하여 알아보고자 한다. 6.2 데이터 생성 torch패키지를 불러 임의로 난수를 발생시킨 후, 텐서 x에 집어넣도록 하자. library(tidyverse) library(torch) # set seed in torch torch_manual_seed(2021) x &lt;- torch_rand(7) * 10 x ## torch_tensor ## 5.1339 ## 7.4256 ## 7.1589 ## 5.7047 ## 1.6527 ## 0.4431 ## 9.6277 ## [ CPUFloatType{7} ] 위의 코드에서 쓰인 함수 두 개를 알아보자. torch_manual_seed(): base 패키지의 set.seed() 함수와 같다. 시뮬레이션 할 때 시드를 고정하는 역할을 한다. torch_rand(): base 패키지에서 runif() 함수와 같다. 균등분포(Uniform distribution) 분포에서 원하는 갯수만큼 표본을 뽑는다. 6.3 함수 만들기 및 오차 그래프 앞에서 살펴본 함수 \\(f\\)는 모수(\\(\\mu\\))를 입력값으로 하는 함수이므로, 다음과 같이 함수를 정의 할 수 있다. f &lt;- function(mu){ mean((x - mu)^2) } f(2) ## torch_tensor ## 20.0462 ## [ CPUFloatType{} ] 위에서 알 수 있듯, \\(\\mu\\) 값이 2인 경우에 대한 오차들의 제곱의 평균값은 20.0462이다. 여러 \\(\\mu\\) 값에 대하여 f 함수의 값을 구해보자. mu_vec &lt;- seq(0, 10, by = 0.02) result &lt;- map_dbl(mu_vec, ~as.numeric(f(mu = .x))) head(result) ## [1] 37.27285 37.06098 36.84991 36.63964 36.43018 36.22152 위의 두 정보를 이용해서 f의 모양이 어떻게 생겼는지 그려보면 다음과 같이 2차원 곡선을 띄고있다는 것을 알 수 있다. library(ggthemes) library(latex2exp) theme_set(theme_igray()) plot_data &lt;- tibble(x = mu_vec, y = result) p &lt;- ggplot(data = plot_data, aes(x = x, y = y)) + geom_line() + labs(x = TeX(&quot;$\\\\mu$&quot;), y = TeX(&quot;$f(\\\\mu)$&quot;)) p Figure 6.1: \\(\\mu\\) 값에 따른 myf 함수값의 변화 우리의 목표는 바로 저 곡선을 최소로 만드는 \\(\\mu\\) 값이 무엇인지 찾아내는 것이다. 이 최소값을 찾기위해서는 경사하강법 같은 방법을 사용해야 하는데, 이러한 알고리즘들의 핵심은 바로 주어진 \\(\\mu\\)값에 대응하는 기울기값을 구하는 것이다. 우리가 임의로 정한 시작점 \\(\\mu_i\\)에서 목표인 \\(\\mu_{*}\\)까지 찾아가기 위해서 경사하강법을 통하면 다음의 과정을 \\(\\mu\\)값이 수렴할 때까지 반복하면 된다. \\[ {\\displaystyle \\mathbf {\\mu} _{i+1}=\\mathbf {\\mu} _{i}-\\gamma _{i}\\nabla f(\\mathbf {\\mu} _{i})}, \\quad i \\in \\mathbb{N} (\\#eq:grad_decent) \\] 위의 수식에서 \\(\\gamma _{i}\\)은 탐색을 할 때 움직이는 거리 (step size)라고 부르고, 딥러닝 분야에서는 나중에 학습률(learning rate)의 개념이 된다. 또한, \\(\\nabla f(\\mathbf {\\mu} _{i})\\) 부분이 바로 기울기값을 나타내는 부분이다. 6.4 Autograd 기능 없이 기울기 구하기 먼저 torch의 자동기울기 기능를 사용해서 기울기값 계산을 하기에 앞서, 계산 결과를 구해보자. \\(y\\)를 \\(\\beta\\)에 대하여 미분하면 다음과 같다. \\[ \\begin{align*} f&#39;(\\beta) &amp; =\\frac{d}{d\\beta}\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\beta\\right)^{2}\\right)\\\\ &amp; =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{d}{d\\beta}\\left(x_{i}-\\beta\\right)^{2}\\\\ &amp; =-\\frac{1}{n}\\sum_{i=1}^{n}2\\left(x_{i}-\\beta\\right) \\end{align*} \\] 따라서 mu값이 2.5로 주어졌을때, 기울기 값은 다음과 같다. f_prime &lt;- function(mu){ -mean(2*(x - mu)) } f_prime(2.5) ## torch_tensor ## -5.61331 ## [ CPUFloatType{} ] 이것이 실제로 그러한지 그림을 그려보자. mu &lt;- 2.5 my_slope &lt;- as.numeric(f_prime(mu)) my_intercept &lt;- as.numeric(f(mu) - f_prime(mu) * mu) p + geom_abline(slope = my_slope, intercept = my_intercept, col = &quot;red&quot;) 6.5 자동미분(Autograd) 기능 torch에는 우리가 계산한 기울기 구하는 과정들을 자동으로 해주는 기능이 있다. 바로 자동미분 (Auto gradient) 기능이다. 기울기값 계산을 위해서 해야할 일은 기울기 계산기능을 activate 해주는 옵션을 실행시켜주기만 하면 된다. 함수는 \\(\\mu\\)에 대한 함수이므로, 기울기값을 추적할 텐서 \\(\\mu\\)를 선언할 때 requires_grad = TRUE 옵션을 붙여줘서 선언하면 끝이다. 이 옵션이 활성화 되면 torch는 이 변수와 관련된 다른 변수들에 대하여 기울기값을 자동으로 추적한다. 추후 복잡한 신경망을 다루는 딥러닝 분야에서는 기울기를 구하는 것이 학습에 아주 핵심적인 기능이고, 이러한 기울기를 구하는 이러한 기울기를 계산하는 방법을 역전파 (backpropagation)라고 부른다. mu &lt;- torch_tensor(2.5, requires_grad=TRUE) mu ## torch_tensor ## 2.5000 ## [ CPUFloatType{1} ] mu 텐서가 기울기 추적 옵션을 달고 있어서, 이와 관련되어 생성되는 모든 텐서에 기울기 추적 옵션 grad_fn 태그가 달려서 생성된다. 다음과 같이 y를 정의를 하면, y에도 역시 grad_fn이 붙어서 생성되는 것을 알 수 있다. y &lt;- mean((x - mu)^2) y$grad_fn ## MeanBackward0 기울기 값 계산을 위해서 해야 할 일은 기울기 계산을 activate 해주는 함수를 실행시켜주기만 하면 된다. y에 대한 베타의 기울기 값을 구하는 것이므로, 다음과 같이 backward()를 이용하여 역전파(backward propagation)를 통하여 기울기 계산을 한다. y$backward() 자동 기울기 추적 기능을 사용한 auto grad가 구한 베타의 기울기값이 우리가 구한 값과 동일한지 확인해보자. f_prime(2.5) ## torch_tensor ## -5.61331 ## [ CPUFloatType{} ] mu$grad ## torch_tensor ## -5.6133 ## [ CPUFloatType{1} ] 앞에서 구한 f_prime(2.5)값이 동일하게 mu$grad에 담겨 있다는 것을 알 수 있다. 6.6 자동 미분 관련 함수들 기울기 자동 추적기능을 사용한다는 것은 그것을 돌리는 컴퓨터의 메모리를 많이 차지한다는 이야기이다. 따라서 우리가 생각하는 변수에 대한 것에만 추적 옵션을 붙여야 하고, 더 이상 필요가 없어지면 기능을 꺼주기도 해야 할 것이다. 이러한 자동 미분 추척 기능들을 자유자재로 다루기 위해서 알아두어야 할 함수들이 있다. 6.6.1 $detach() 현재 y는 기울기 자동추적 기능이 붙어있다. 우리가 다음과 같이 y를 사용해서 텐서 z를 생성하면 그 역시 옵션이 딸려 생성이 될 테지만, y 텐서 이후 부터는 추적 기능을 사용하고 싶지 않을때, $detach()를 사용해서 추적기를 떼어낼 수 있다. y$grad_fn ## MeanBackward0 z &lt;- y^2 z$grad_fn ## PowBackward0 z$detach_() ## torch_tensor ## 288.646 ## [ CPUFloatType{} ] z$grad_fn ## NULL 6.6.2 $requires_grad 변수와 $requires_grad_(TRUE) 이 함수는 이미 선언된 텐서에 미분 추적기능을 붙이고 싶을 때, $requires_grad_(TRUE)을 사용할 수 있다. 일반 텐서 a를 생성하도록 하자. a &lt;- torch_tensor(c(1, 2)) a ## torch_tensor ## 1 ## 2 ## [ CPUFloatType{2} ] a$requires_grad ## [1] FALSE a$requires_grad 값이 FALSE라는 말은 a에 대한 추적 옵션은 현재 꺼져있는 상태이다. 자동 추적 기능이 없이 생성된 텐서에 추적 기능을 붙일 때에는 a$requires_grad을 TRUE로 바꿔주면 된다. TRUE를 직접 할당해도 되고, $requires_grad_(TRUE)을 사용하여 바꿔줘도 된다. # a$requires_grad &lt;- TRUE a$requires_grad_(TRUE) ## torch_tensor ## 1 ## 2 ## [ CPUFloatType{2} ] 6.6.3 with_no_grad({}) 만약 특정 코드를 실행함에 있어서 추적 기능을 떼고 계산하고 싶은 경우, with_no_grad({})가 유용하다. y ## torch_tensor ## 16.9896 ## [ CPUFloatType{} ] y$grad_fn ## MeanBackward0 with_no_grad({ y y$grad_fn }) ## MeanBackward0 6.7 경사하강법 이왕 자동 미분기능을 알았으니, 이 기능을 이용하여 식 @ref(eq:grad_decent)의 경사하강법으로 함수값을 최소로 만드는 \\(\\mu\\) 값을 찾아보도록 하자. learning_rate &lt;- 0.1 # 시작값 0.5 mu &lt;- torch_tensor(0.5, requires_grad=TRUE) result &lt;- rep(0, 100) result[1] &lt;- as.numeric(mu) for (i in 2:100) { result[i] &lt;- as.numeric(mu) y &lt;- mean((x - mu)^2) y$backward() with_no_grad({ mu$sub_(learning_rate * mu$grad) mu$grad$zero_() }) } tail(result) ## [1] 5.306654 5.306654 5.306654 5.306654 5.306654 5.306654 mu$grad$zero_() 부분은 미분값을 초기화 해주는 부분이라고 이해하면 좋다. 그렇지 않을 경우, 이전의 값이 남아있어서 계속 누적되므로 주의하자. 6.7.1 시각화 mu_points &lt;- tibble(x = result, y = map_dbl(result, ~as.numeric(f(mu = .x)))) p + geom_point(data = mu_points, aes(x = x, y = y), col = &quot;blue&quot;) 이 챕터의 제일 첫부분에서 말했든 이론적인 정답은 데이터의 표본평균이 함수값을 최소로 만드는 값이다. 실제로 그렇게 나왔는지 확인해보면 두 값이 같다는 것을 알 수 있다. result[100] ## [1] 5.306654 x$mean() ## torch_tensor ## 5.30665 ## [ CPUFloatType{} ] 이것으로 자동 미분 기능에 대하여 알아보았다. 이 기능을 활용하면 훨씬 복잡한 구조의 함수(예를 들어 딥러닝에서의 신경망 같은)에 대한 미분값 역시도 쉽게 구할 수 있다. 응용 코드들은 신경망 예제에서 다루기로 하자. "],["torch-nn-모듈로-첫-신경망-정의하기.html", "Chapter 7 torch_nn 모듈로 첫 신경망 정의하기 7.1 신경망 정의 (Custom nn Modules) 7.2 nn_linear 클래스 7.3 순전파(Forward propagation) 정의", " Chapter 7 torch_nn 모듈로 첫 신경망 정의하기 이제까지 torch의 자동미분(auto grad) 기능과 순전파(forward propagation)에 대하여 알아보았다. 오늘은 드디어, torch 라이브러리에서 제공하는 함수들을 이용해서 챕터 5 에서 정의해본 신경망을 정의해 보도록 한다. Figure 7.1: 다시 두두등장! 세상 간단한 신경망 7.1 신경망 정의 (Custom nn Modules) 토치를 사용해서 신경망을 정의할 때 사용하는 함수가 있다. 바로 nn_module()이라는 함수인데, torch에서 신경망을 정의할 때, 이 함수를 사용해서 “클래스”를 만들어 정의한다! 왜 우리가 챕터 4에서 R6관련 클래스 내용을 그렇게도 공부했었는지에 대한 답을 바로 이 챕터에서 찾을 수 있을 것이다. 7.1.1 nn_module과 클래스 상속 nn_module이 어떤 역할을 하는지에 대하여 알아보기 위해 가장 간단한 신경망을 작성해보도록 하자. 바로 우리가 앞서 살펴본 2단 레이어 네트워크 예제에서 사용한 데이터를 만들어 보자. library(torch) X &lt;- torch_tensor(matrix(1:6, ncol = 2, byrow = T), dtype = torch_float()) X ## torch_tensor ## 1 2 ## 3 4 ## 5 6 ## [ CPUFloatType{3,2} ] 먼저, TwoLayerNet이라는 이름의 신경망 클래스를 정의한다(기억하시나? 클래스의 이름은 카멜 형식이다!). nn_module() 함수는 클래스를 정의하는 함수인데, 이 함수를 사용해서 만들어진 클래스는 자동으로 신경망과 관련한 클래스인 basic-nn-module 클래스를 상속하게 만든다. 즉, nn_module안에는 신경망 관련 클래스들 속에는 신경망과 관련한 많은 함수가 정의되어 있을 것이고, 이것을 다 상속받아서 클래스가 만들어지는 것이다. 다음의 코드는 위의 신경망을 정의한 코드이다. TwoLayerNet &lt;- nn_module( classname = &quot;TowLayerNet&quot;, initialize = function(data_in, hidden, data_out){ cat(&quot;Initiation complete!&quot;) self$hidden_layer &lt;- nn_linear(data_in, hidden, bias=FALSE) self$output_layer &lt;- nn_linear(hidden, data_out, bias=FALSE) } ) myfirst_model &lt;- TwoLayerNet(2, 3, 1) ## Initiation complete! myfirst_model ## An `nn_module` containing 9 parameters. ## ## ── Modules ───────────────────────────────────────────────────────────────────── ## ● hidden_layer: &lt;nn_linear&gt; #6 parameters ## ● output_layer: &lt;nn_linear&gt; #3 parameters 결과를 살펴보면 TwoLayerNet 클래스에 의하여 만들어진 myfirst_model는 두 개의 층이 들어있는 것을 확인할 수 있다. 이 두개 층에 관련한 모수 갯수를 그림과 한번 연결 시켜보면 잘 정의가 되어있다는 것을 알 수 있다. hidden_layer: 그림에서 첫번째와 두번째 층을 연결하는 다리가 6개라는 것을 주목하자. 모수의 갯수는 그래서 6개! output_layer: 그림에서 두번째와 마지막 층을 연결하는 다리는 3개이므로, 모수의 갯수는 3개가 된다. 7.2 nn_linear 클래스 nn_linear의 입력값은 입력변수의 갯수, 출력변수의 갯수, 그리고 bias 항의 유무를 나타내는 옵션 이렇게 세개가 된다. 예제의 경우, 데이터 텐서 \\(X\\)의 features 갯수가 2개이므로, 히든 레이어의 입력값 갯수가 2개가 되어야 한다. 또한 히든 레이어의 노드 갯수가 3개이므로 결과 행력의 features 갯수가 3개가 되어야 한다. 7.2.1 bias 없는 경우 우리가 예전에 다루었던 예제에서는 bias 항이 없었으므로, bias=FALSE를 해주어야 함에 주의하자. mat_op &lt;- nn_linear(2, 3, bias = FALSE) mat_op$weight ## torch_tensor ## 0.2524 0.6096 ## 0.2502 0.0472 ## 0.3940 -0.1165 ## [ CPUFloatType{3,2} ] mat_op을 nn.Linear(2, 3) 클래스로 만들어진 클래스 생성자로 이해 할 수 있다. 그리고 이것의 수학적 의미는 행렬 연산으로 이해할 수 있겠다. mat_op가 생성될 때 임의의 weight 텐서, \\(W\\), 와 bias, \\(b\\),가 생성이 되고, 입력값으로 들어오는 X에 대하여 다음의 연산을 수행한 후 결괏값을 내보낸다. \\[ y = X\\beta = XW^T \\] 결과를 코드로 확인해보자. X$mm(mat_op$weight$t()) ## torch_tensor ## 1.4715 0.3445 0.1610 ## 3.1954 0.9392 0.7160 ## 4.9193 1.5339 1.2709 ## [ CPUFloatType{3,3} ] mat_op(X) ## torch_tensor ## 1.4715 0.3445 0.1610 ## 3.1954 0.9392 0.7160 ## 4.9193 1.5339 1.2709 ## [ CPUFloatType{3,3} ] 7.2.2 bias 있는 경우 bias=TRUE를 해주면 weight 텐서 \\(W\\)와 더불어 bias 텐서가 생성이 된다. mat_op2 &lt;- nn_linear(2, 3, bias = TRUE) mat_op2$weight ## torch_tensor ## 0.5929 -0.3510 ## -0.4910 -0.2437 ## 0.3812 -0.6701 ## [ CPUFloatType{3,2} ] mat_op2$bias ## torch_tensor ## 0.6401 ## -0.3302 ## -0.2438 ## [ CPUFloatType{3} ] 따라서 정의된 신경망의 연산 역시 다음과 같이 바뀐다. \\[ y = X\\beta + b = XW^T + b \\] X$mm(mat_op2$weight$t()) + mat_op2$bias ## torch_tensor ## 0.5309 -1.3086 -1.2027 ## 1.0145 -2.7780 -1.7804 ## 1.4982 -4.2474 -2.3580 ## [ CPUFloatType{3,3} ] mat_op2(X) ## torch_tensor ## 0.5309 -1.3086 -1.2027 ## 1.0145 -2.7780 -1.7804 ## 1.4982 -4.2474 -2.3580 ## [ CPUFloatType{3,3} ] 7.3 순전파(Forward propagation) 정의 torch를 공부하면서 신기한 걸 많이 배우고 있다. 그 중 한가지가 바로 객체지향 프로그래밍을 사용해서 신경망을 정의한다는 것이다. 앞선 예제를 이어가보면, 우리는 신경망의 순전파를 구현해야 한다. 순전파의 경우 다음과 같이 forward 멤버 함수를 정의해서 구현할 수 있다. TwoLayerNet &lt;- nn_module( classname = &quot;TowLayerNet&quot;, initialize = function(data_in, hidden, data_out){ cat(&quot;Initiation complete!&quot;) self$hidden_layer &lt;- nn_linear(data_in, hidden, bias=FALSE) self$output_layer &lt;- nn_linear(hidden, data_out, bias=FALSE) self$sigmoid &lt;- nn_sigmoid() }, # 순전파 멤버함수 forward 정의 부분 forward = function(X) { z1 &lt;- self$hidden_layer(X) a1 &lt;- self$sigmoid(z1) z2 &lt;- self$output_layer(a1) y_hat &lt;- self$sigmoid(z2) return(y_hat) } ) library(zeallot) c(D_in, H, D_out) %&lt;-% c(2, 3, 1) my_net &lt;- TwoLayerNet(D_in, H, D_out) ## Initiation complete! my_net(X) ## torch_tensor ## 0.4476 ## 0.4347 ## 0.4347 ## [ CPUFloatType{3,1} ] 위의 코드를 한번 살펴보자. 먼저 zeallot 패키지는 %&lt;-%를 포함하는 패키지인데, 여러 개의 변수에 한꺼번에 값을 부여하는 연산자이기 때문에 알아두면 편한 패키지 이다. 새로 정의된 TwoLayerNet 클래스에는 7.1의 2단 신경망의 순전파(forward propagation)가 구현된 멤버함수 forward가 정의되어 있다. 이 함수는 입력 텐서 X가 신경망으로 들어오게 되면, 은닉층(hidden_layer) \\(\\rightarrow\\) 활성함수 (activation function; 여기서는 nn_sigmoid 함수) \\(\\rightarrow\\) 출력층(output_layer) \\(\\rightarrow\\) 활성함수 순으로 내보내게 된다. "],["references.html", "References", " References "]]
