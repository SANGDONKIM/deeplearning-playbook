[["index.html", "딥러닝 공략집 with R Chapter 1 들어가며 1.1 설치하기", " 딥러닝 공략집 with R 슬기로운통계생활 2021-02-09 Chapter 1 들어가며 Figure 1.1: deep-learning playbook 이제까지 R에서의 딥러닝은 Python의 라이브러리들을 reticulate 패키지를 이용하여 빌려온 형태였지만, torch for R 패키지는 C 라이브러리를 Torch를 기반으로 R을 wrapper 언어로서 사용하여 패키지를 만들었다. 즉, Torch + Python = PyTorch, Torch + R = Rtorch가 되는 셈이다. 1.1 설치하기 설치 역시 간단한다. 여느 R패키지와 같이 install.packages() 함수를 사용하면 된다. 서브 라이브러리인 torchaudio와 torchvision이 있으나, 책의 뒷부분에서 다루기로 한다. install.packages(&quot;torch&quot;) # 혹은 개발버전을 다운 받고 싶다면 # devtools::install_github(&quot;mlverse/torch&quot;) "],["intro.html", "Chapter 2 딥러닝 첫걸음, 텐서 (tensor) 만들기 2.1 torch와의 첫만남 2.2 텐서 (tensor) 만들기 2.3 고급기술: 영리하게 만들기 2.4 텐서와 행렬은 같을까?", " Chapter 2 딥러닝 첫걸음, 텐서 (tensor) 만들기 2.1 torch와의 첫만남 torch패키지를 설치했으니, 한번 만나봐야한다. 다음의 명령어를 통하여 torch를 불러보자. library(torch) 2.2 텐서 (tensor) 만들기 텐서가 무엇이냐! 무언가 대단한 것처럼 보이나, 결국 우리가 R을 배웠을때 사용했던 matrix의 개념을 확장시킨 것이라고 생각하면 된다. 결국 다차원 행렬, 혹은 Array인 것이다. 우리가 많이 쓰는 행렬도 Array에 속하지만, 보통 Array라는 용어는 3차원 이상의 행렬을 암시한다. 이름부터 멋있는 딥러닝인데 다른 용어들이 Array같이 다른 패키지에서 사용되는 것들이랑 동일하면 격이 떨어지므로, 텐서 (tensor) 라고 붙였다. 토치 설명서에 따르면 텐서는 R의 Array와 비슷하나, GPU 계산에도 쓸 수 있다고 나와있다. (응, 그냥 Array.) 또한, 프로그래밍 언어에서 어떤 변수를 만들때, 만든다고 하지않고, “선언한다\" 라고 한다. 따라서 앞으로 만든다는 말 대신”선언\"이라는 용어를 사용하겠다. 2.2.1 빈 텐서 만들기 속이 빈 5행 3열의 텐서은 다음과 같이 선언한다. 주의할 점은 우리가 이번에 만들 빈 (empty) 텐서 과 뒤에서 만들어 볼 0 텐서는 다르다; 빈 텐서은 0과 근접한 쓰레기값이 들어있는 반면에, 0 텐서에는 정말 0이 들어있다. x &lt;- torch_empty(5, 3) # 텐서 x값 확인 x ## torch_tensor ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## 0 0 0 ## [ CPUFloatType{5,3} ] # 텐서 x의 크기 확인 dim(x) ## [1] 5 3 정말 5행 3열의 텐서가 만들어졌다. 이건 마치 우리가 R을 처음 시작하고 텐서을 만드는 것과 아주 유사해서, 실제 R을 만져본 사람이라면 뒤로 빨리 넘기고 싶어하는 욕구가 솓구칠 것이다. 결과값을 잘 살펴보자. CPUFloatType에서 우리는 현재 만든 rand_tensor는 CPU에서 접근이 가능하며, 실수 (float) 타입이라는 것을 이야기해준다. 2.2.2 랜덤 텐서 텐서의 각 자리에 0에서 1사이의 난수로 채워서 만드는 방법이다. torch_rand() 함수를 사용한다. rand_tensor &lt;- torch_rand(5, 3) rand_tensor ## torch_tensor ## 0.8918 0.8378 0.9646 ## 0.4432 0.2102 0.3342 ## 0.0045 0.8908 0.7716 ## 0.0803 0.9383 0.0508 ## 0.4391 0.7506 0.9748 ## [ CPUFloatType{5,3} ] 참고로 이렇게 만들어진 텐서에는 R에서 텐서과 어레이(array)에 접근할 때사용한 모든 문법들을 사용해서 접근할 수 있다. rand_tensor[,2] ## torch_tensor ## 0.8378 ## 0.2102 ## 0.8908 ## 0.9383 ## 0.7506 ## [ CPUFloatType{5} ] rand_tensor[1:3,] ## torch_tensor ## 0.8918 0.8378 0.9646 ## 0.4432 0.2102 0.3342 ## 0.0045 0.8908 0.7716 ## [ CPUFloatType{3,3} ] rand_tensor[3:4,c(1, 3)] ## torch_tensor ## 0.0045 0.7716 ## 0.0803 0.0508 ## [ CPUFloatType{2,2} ] 위의 예제에서 벌써부터 일부 R유저들은 감격의 눈물을 흘릴 수 있다. 그렇다, Rtorch에서 텐서의 첫번째 위치는 1부터 시작한다. 이 너무나도 당연한 진리는 파이썬에서는 통하지 않는다. 2.2.3 단위 텐서 4행 4열의 단위 텐서 (identity matrix)를 선언하는 방법은 다음과 같다. x &lt;- torch_eye(4) x ## torch_tensor ## 1 0 0 0 ## 0 1 0 0 ## 0 0 1 0 ## 0 0 0 1 ## [ CPUFloatType{4,4} ] 2.2.4 영(0) 텐서 텐서의 요소들이 모두 0으로 채워진 3행 5열의 텐서을 선언하는 것은 다음과 같이 torch_zeros() 함수를 사용한다. x &lt;- torch_zeros(3, 5) x ## torch_tensor ## 0 0 0 0 0 ## 0 0 0 0 0 ## 0 0 0 0 0 ## [ CPUFloatType{3,5} ] 2.3 고급기술: 영리하게 만들기 지금까지는 미리 정해진 값들, 난수나, 0과 1을 채워넣는 법을 배웠다. 하지만, 많은 경우 우리가 직접 정의한 텐서들을 다루게 될 것이다. 이번 섹션에서는 좀 더 영리하게 선언해보는 방법을 배워보자. 2.3.1 텐서 직접선언 가장 핵심적인 내용은 R에서 벡터와 행렬을 정의한 후 torch_tensor() 함수에 넣어주면, 그대로 가져다가 텐서로 바꿔준다는 사실이다. 다음의 예제는 2행 2열의 행렬을 정의한 후, 정의된 행렬을 사용하여 텐서를 만드는 코드이다. y &lt;- torch_tensor(matrix(c(1, 2, 3, 4, 5, 6), ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPUFloatType{3,2} ] 2.3.2 : 연산자 사용 위의 코드가 잘 작동한다는 사실을 알게 되면, 우리가 너무나 익숙한 R의 기본 함수들을 사용하여 텐서를 자유롭게 만들 수 있을 것이다. 앞선 예제는 : 연산자를 통하여 다음과 같이 축약 할 수 있다. y &lt;- torch_tensor(matrix(1:6, ncol = 2)) y ## torch_tensor ## 1 4 ## 2 5 ## 3 6 ## [ CPULongType{3,2} ] 2.3.3 seq() 함수 사용 seq() 함수는 좀 더 유연한 벡터를 만들 수 있도록 해주므로, 텐서를 만들때 유용하게 사용될 것이다. y &lt;- torch_tensor(matrix(seq(0.1, 1, by = 0.1), ncol = 2)) y ## torch_tensor ## 0.1000 0.6000 ## 0.2000 0.7000 ## 0.3000 0.8000 ## 0.4000 0.9000 ## 0.5000 1.0000 ## [ CPUFloatType{5,2} ] 위의 코드는 seq() 함수를 사용해서 벡터를 만들고, 2열을 갖는 행렬을 만든 후, 텐서로 변환을 시켰다. 단, by 옵션의 경우, 결과값이 홀수인지 짝수인지 체크해줘야 하므로, 특정 범위에서의 일정 간격 숫자를 뽑아 행렬로 만들땐 length.out 옵션이 편하다. y &lt;- torch_tensor(matrix(seq(0, 1, length.out = 10), ncol = 2)) y ## torch_tensor ## 0.0000 0.5556 ## 0.1111 0.6667 ## 0.2222 0.7778 ## 0.3333 0.8889 ## 0.4444 1.0000 ## [ CPUFloatType{5,2} ] 둘 다 0과 1사이의 벡터를 만들었지만, 결과는 다르다는 것에 주의하자. 텐서를 만드는 방법에 대한 핵심은 결국, 자신이 편한 방법으로 만들고 싶은 텐서와 대응되는 R 개체를 만들고, torch_tensor()에 입력 시켜주면 되는 것이다. 2.3.4 %&gt;% 연산자 사용 가끔 R에서 아주 많이 쓰이는 %&gt;% 파이프 연산자를 다른 라이브러리를 사용할 경우 적용할 생각을 못하는 경우가 있다. 왼쪽의 결과 값을 오른쪽의 입력값으로 넘겨주는 파이프 연산자 역시 torch 패키지에서 사용 가능하므로, 텐서 만드는 방법은 그야말로 무궁무진하다. library(magrittr) y2 &lt;- torch_tensor(1:5 %&gt;% diag()) y2 ## torch_tensor ## 1 0 0 0 0 ## 0 2 0 0 0 ## 0 0 3 0 0 ## 0 0 0 4 0 ## 0 0 0 0 5 ## [ CPULongType{5,5} ] 2.4 텐서와 행렬은 같을까? 앞에서 설명한 것처럼 텐서는 행렬의 개념을 확장시킨 것에 지나지 않겠지만, 그렇다고 같은 취급을 해서도 안된다. 그도 그럴것이 R에서 torch의 텐서와 행렬은 같지 않다. 이러한 사실은 다음과 같이 위에서 만든 텐서 x에 R의 기본 연산자인 행렬곱을 적용해보면 알 수 있다. x &lt;- torch_zeros(3, 5) x %*% t(x) ## Error in t.default(x): argument is not a matrix 위의 argument is not a matrix 에러에서 우리는 정말 텐서와 행렬은 다르게 취급된다는 것을 알 수 있다. 즉, R환경에서 텐서와 행렬은 근본이 다른 개체(object)라는 것을 알 수 있다. 그렇다면 ‘텐서끼리의 계산은 어떻게 할까?’ 자연스러운 의문이 든다. 다음 장에서는 텐서의 연산에 대하여 배워보자. "],["operation.html", "Chapter 3 텐서 (tensor) 연산 3.1 토치 (torch) 불러오기 및 준비물 준비 3.2 텐서의 연산", " Chapter 3 텐서 (tensor) 연산 지난 챕터에서 우리는 텐서가 행렬의 연산에 적용되는 %*%과 호환이 되지 않는 다는 것을 알게되었다. 이번 챕터에서는 텐서들의 연산에 대하여 알아보도록 하자. 3.1 토치 (torch) 불러오기 및 준비물 준비 토치 (torch) 를 불러오고, 이번 챕터에 사용될 텐서 A, B, 그리고 C를 준비하자. 지난 챕터에서 배운 난수를 이용한 텐서도 만들 예정이니 난수를 고정한다. library(torch) # 난수 생성 시드 고정 torch_manual_seed(2021) A &lt;- torch_tensor(1:6) B &lt;- torch_rand(2, 3) C &lt;- torch_rand(2, 3, 2) A; B; C ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPULongType{6} ] ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] ## torch_tensor ## (1,.,.) = ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## ## (2,.,.) = ## 0.1242 0.7489 ## 0.3608 0.5131 ## 0.2959 0.7834 ## [ CPUFloatType{2,3,2} ] 만들어진 세 개의 텐서 결과를 살펴보면 다음과 같다. 텐서 A: 정수들로 구성이 되어있고, 6개의 원소들이 벡터를 이루고 있다. 텐서 B: 실수들로 구성이 되어있고, 똑같이 6개의 원소들이 있지만, 모양이 4행 3열인 2차원 행렬의 모양을 하고 있다. 텐서 C: 실수들로 구성이 되어있고, 총 원소 갯수는 12개지만, 모양은 3행 2열의 행렬이 두개가 쌓여진 꼴의 3차원 배열 (array) 이다. 3.2 텐서의 연산 3.2.1 형(type) 변환 먼저 주목해야 할 것은 바로 텐서 A와 B의 자료형이 다르다는 것이다. 이게 무슨뜻이냐면 A에는 정수만이 담길 수 있고, B에는 실수만이 담길 수 있도록 설계가 되어있다는 것이다. 앞에서 확인한 자료형을 좀 더 명확하게 확인하기 위해서는 type() 사용한다. A$dtype ## torch_Long B$dtype ## torch_Float 텐서 A를 실수형 텐서로 바꿔보자. 텐서의 형을 변환할 때에는 A텐서 안에 속성으로 들어가있는 to() 함수를 사용 (좀 더 어려운 관점에서는 OOP의 method를 사용) 해서 바꿔줄 수 있다. A &lt;- A$to(dtype = torch_double()) A ## torch_tensor ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## [ CPUDoubleType{6} ] torch에는 정말 많은 자료형이 있는데, 그 목록은 다음을 참고하자. 3.2.2 모양 변환 앞에서 텐서 A를 B와 같은 실수를 담을 수 있는 형으로 바꾸었다. 그렇다면 이 두 개를 더할 수 있을까? 답은 “아니올시다.” 이다. 왜냐하면 모양이 다르기 때문이다. A + B ## Error in (function (self, other, alpha) : The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1 ## Exception raised from infer_size at ../aten/src/ATen/ExpandUtils.cpp:24 (most recent call first): ## frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0x69 (0x7fa87de62b89 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libc10.so) ## frame #1: at::infer_size(c10::ArrayRef&lt;long&gt;, c10::ArrayRef&lt;long&gt;) + 0x552 (0x7fa86d9b1382 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #2: at::TensorIterator::compute_shape(at::TensorIteratorConfig const&amp;) + 0xde (0x7fa86deb3c2e in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #3: at::TensorIterator::build(at::TensorIteratorConfig&amp;) + 0x64 (0x7fa86deb61e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #4: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&amp;) + 0xdd (0x7fa86deb699d in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #5: at::TensorIterator::binary_op(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;) + 0x130 (0x7fa86deb6b30 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #6: at::native::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x53 (0x7fa86db69bc3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #7: &lt;unknown function&gt; + 0x13311bd (0x7fa86e1d01bd in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #8: &lt;unknown function&gt; + 0xaf2045 (0x7fa86d991045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #9: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7fa86e37b81f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #10: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7fa86e271fd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #11: &lt;unknown function&gt; + 0x2a0f2bb (0x7fa86f8ae2bb in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #12: &lt;unknown function&gt; + 0xaf2045 (0x7fa86d991045 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #13: at::Tensor c10::Dispatcher::callWithDispatchKey&lt;at::Tensor, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar&gt;(c10::TypedOperatorHandle&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar)&gt; const&amp;, c10::DispatchKey, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) const + 0x27f (0x7fa86e37b81f in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #14: at::add(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x123 (0x7fa86e271fd3 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/./libtorch_cpu.so) ## frame #15: _lantern_add_tensor_tensor_scalar + 0x64 (0x7fa87e1e40e4 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/deps/liblantern.so) ## frame #16: cpp_torch_namespace_add_self_Tensor_other_Tensor(Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchTensor, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchTensor&gt;(XPtrTorchTensor*)), false&gt;, Rcpp::XPtr&lt;XPtrTorchScalar, Rcpp::PreserveStorage, &amp;(void Rcpp::standard_delete_finalizer&lt;XPtrTorchScalar&gt;(XPtrTorchScalar*)), false&gt;) + 0x48 (0x7fa87eb29fe8 in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #17: _torch_cpp_torch_namespace_add_self_Tensor_other_Tensor + 0x9c (0x7fa87e8c200c in /home/issac/R/x86_64-pc-linux-gnu-library/4.0/torch/libs/torchpkg.so) ## frame #18: &lt;unknown function&gt; + 0xf9310 (0x7fa8914ab310 in /usr/lib/R/lib/libR.so) ## frame #19: &lt;unknown function&gt; + 0xf9826 (0x7fa8914ab826 in /usr/lib/R/lib/libR.so) ## frame #20: &lt;unknown function&gt; + 0x137106 (0x7fa8914e9106 in /usr/lib/R/lib/libR.so) ## frame #21: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #22: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #23: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #24: Rf_eval + 0x353 (0x7fa8914f58c3 in /usr/lib/R/lib/libR.so) ## frame #25: &lt;unknown function&gt; + 0xc650d (0x7fa89147850d in /usr/lib/R/lib/libR.so) ## frame #26: &lt;unknown function&gt; + 0x137106 (0x7fa8914e9106 in /usr/lib/R/lib/libR.so) ## frame #27: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #28: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #29: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #30: &lt;unknown function&gt; + 0x13a989 (0x7fa8914ec989 in /usr/lib/R/lib/libR.so) ## frame #31: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #32: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #33: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #34: &lt;unknown function&gt; + 0x13a989 (0x7fa8914ec989 in /usr/lib/R/lib/libR.so) ## frame #35: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #36: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #37: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #38: &lt;unknown function&gt; + 0x13a989 (0x7fa8914ec989 in /usr/lib/R/lib/libR.so) ## frame #39: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #40: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #41: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #42: &lt;unknown function&gt; + 0x13a989 (0x7fa8914ec989 in /usr/lib/R/lib/libR.so) ## frame #43: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #44: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #45: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #46: &lt;unknown function&gt; + 0x12d83b (0x7fa8914df83b in /usr/lib/R/lib/libR.so) ## frame #47: &lt;unknown function&gt; + 0x9021b (0x7fa89144221b in /usr/lib/R/lib/libR.so) ## frame #48: Rf_eval + 0x706 (0x7fa8914f5c76 in /usr/lib/R/lib/libR.so) ## frame #49: &lt;unknown function&gt; + 0x149782 (0x7fa8914fb782 in /usr/lib/R/lib/libR.so) ## frame #50: &lt;unknown function&gt; + 0x137106 (0x7fa8914e9106 in /usr/lib/R/lib/libR.so) ## frame #51: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #52: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #53: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) ## frame #54: &lt;unknown function&gt; + 0x13a989 (0x7fa8914ec989 in /usr/lib/R/lib/libR.so) ## frame #55: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #56: &lt;unknown function&gt; + 0x1440ac (0x7fa8914f60ac in /usr/lib/R/lib/libR.so) ## frame #57: Rf_eval + 0x454 (0x7fa8914f59c4 in /usr/lib/R/lib/libR.so) ## frame #58: &lt;unknown function&gt; + 0x14a22c (0x7fa8914fc22c in /usr/lib/R/lib/libR.so) ## frame #59: &lt;unknown function&gt; + 0x1871fd (0x7fa8915391fd in /usr/lib/R/lib/libR.so) ## frame #60: &lt;unknown function&gt; + 0x1353c4 (0x7fa8914e73c4 in /usr/lib/R/lib/libR.so) ## frame #61: Rf_eval + 0x180 (0x7fa8914f56f0 in /usr/lib/R/lib/libR.so) ## frame #62: &lt;unknown function&gt; + 0x14550f (0x7fa8914f750f in /usr/lib/R/lib/libR.so) ## frame #63: Rf_applyClosure + 0x1c7 (0x7fa8914f82d7 in /usr/lib/R/lib/libR.so) 모양이 다른 텐서를 더하려고 하면 R은 위에서 보듯 너무나 많은 에러를 쏟아낸다. 모양이 다른 두 텐서를 더하기 위해서는 모양을 같게 맞춰줘야 한다. A의 모양을 B의 모양과 같이 바꿔보도록 하자. 모양을 바꿀때는 view() 함수를 사용하고, 안에 모양의 형태를 벡터 형식으로 짚어 넣는다는 것을 기억하자. A &lt;- A$view(c(2, 3)) A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] 3.2.3 덧셈과 뺄셈 앞에서 형(type)과 모양(shape)까지 맞춰놨으니, 텐서끼리의 덧셈과 뺄셈을 할 수 있다. A + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUDoubleType{2,3} ] A - B ## torch_tensor ## 0.4866 1.2574 2.2841 ## 3.4295 4.8347 5.9557 ## [ CPUDoubleType{2,3} ] 사실, 텐서끼리의 연산은 모양만 맞으면 가능하다. 즉, 다음의 연산이 성립한다. A_ &lt;- A$to(dtype = torch_long()) A_ + B ## torch_tensor ## 1.5134 2.7426 3.7159 ## 4.5705 5.1653 6.0443 ## [ CPUFloatType{2,3} ] 결과에서 알 수 있듯, 정수를 담을 수 있는 텐서와 실수를 담을 수 있는 텐서를 더하면, 결과는 실수를 담을 수 있는 텐서로 반환이 된다. 하지만, 필자는 이러한 코딩은 피해야 한다고 생각한다. 즉, 모든 연산을 할 경우, 명시적으로 형변환을 한 후 연산을 할 것을 권한다. 왜냐하면, 언제나 우리는 코드를 다른 사람이 보았을 때, 이해하기 쉽도록 짜는 것을 추구해야 한다. (코드는 하나의 자신의 생각을 적은 글이다.) 3.2.4 상수와의 연산 R에서와 마찬가지로, 텐서와 상수와의 사칙연산은 각 원소에 적용되는 것을 확인하자. A + 2 ## torch_tensor ## 3 4 5 ## 6 7 8 ## [ CPUDoubleType{2,3} ] B^2 ## torch_tensor ## 0.2636 0.5514 0.5125 ## 0.3254 0.0273 0.0020 ## [ CPUFloatType{2,3} ] A %/% 3 ## torch_tensor ## 0 0 1 ## 1 1 2 ## [ CPUDoubleType{2,3} ] A %% 3 ## torch_tensor ## 1 2 0 ## 1 2 0 ## [ CPUDoubleType{2,3} ] 3.2.5 제곱근과 로그 제곱근(square root)나 로그(log) 함수 역시 각 원소별 적용이 가능하다. A ## torch_tensor ## 1 2 3 ## 4 5 6 ## [ CPUDoubleType{2,3} ] torch_sqrt(A) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] 위의 연산이 에러가 나는 이유는 A가 정수를 담는 텐서였는데, 연산을 수행한 후에 실수가 담겨져서 나오는 에러이다. R과는 사뭇다른 예민한 아이 torch를 위해 형을 바꿔준 후에 연산을 실행하도록 하자. torch_sqrt(A$to(dtype = torch_double())) ## torch_tensor ## 1.0000 1.4142 1.7321 ## 2.0000 2.2361 2.4495 ## [ CPUDoubleType{2,3} ] torch_log(B) ## torch_tensor ## -0.6667 -0.2977 -0.3342 ## -0.5613 -1.8002 -3.1166 ## [ CPUFloatType{2,3} ] 3.2.6 텐서의 곱셈 텐서의 곱셈 역시 모양이 맞아야 하므로, 3행 2열이 두개가 붙어있는 C에서 앞에 한장을 떼어내도록 하자. B ## torch_tensor ## 0.5134 0.7426 0.7159 ## 0.5705 0.1653 0.0443 ## [ CPUFloatType{2,3} ] D &lt;- C[1,,] D ## torch_tensor ## 0.9628 0.2943 ## 0.0992 0.8096 ## 0.0169 0.8222 ## [ CPUFloatType{3,2} ] 텐서의 곱셈은 torch_matmul # 파이프 사용해도 무방하다. # B %&gt;% torch_matmul(D) torch_matmul(B, D) ## torch_tensor ## 0.5800 1.3409 ## 0.5664 0.3381 ## [ CPUFloatType{2,2} ] "],["references.html", "References", " References "]]
