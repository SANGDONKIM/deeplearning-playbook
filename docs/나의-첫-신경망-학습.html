<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 나의 첫 신경망 학습 | 딥러닝 공략집 with R</title>
  <meta name="description" content="딥러닝 라이브러리 Rtorch를 사용하여 딥러닝의 끝판왕을 정복해보자. 본격 R 딥러닝 공략집" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 나의 첫 신경망 학습 | 딥러닝 공략집 with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="딥러닝 라이브러리 Rtorch를 사용하여 딥러닝의 끝판왕을 정복해보자. 본격 R 딥러닝 공략집" />
  <meta name="github-repo" content="statisticsplaybook/r-torch-playbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 나의 첫 신경망 학습 | 딥러닝 공략집 with R" />
  
  <meta name="twitter:description" content="딥러닝 라이브러리 Rtorch를 사용하여 딥러닝의 끝판왕을 정복해보자. 본격 R 딥러닝 공략집" />
  

<meta name="author" content="슬기로운통계생활" />


<meta name="date" content="2021-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="torch-nn-모듈로-첫-신경망-정의하기.html"/>
<link rel="next" href="dataset과-dataloader-클래스.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<meta name="google-site-verification" content="z9CiKKDExNMW8gi4-dN3X6zGa1-OXeSaIpjGFgXgHEg" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6MYZBEL4H2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6MYZBEL4H2');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="dlplaybook.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">딥러닝 공략집 with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>들어가며</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#설치하기"><i class="fa fa-check"></i>설치하기</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#기본-패키지"><i class="fa fa-check"></i>기본 패키지</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> 딥러닝 첫걸음, 텐서 (tensor) 만들기</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#torch와의-첫만남"><i class="fa fa-check"></i><b>1.1</b> <code>torch</code>와의 첫만남</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#텐서-tensor-만들기"><i class="fa fa-check"></i><b>1.2</b> 텐서 (tensor) 만들기</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#빈-텐서-만들기"><i class="fa fa-check"></i><b>1.2.1</b> 빈 텐서 만들기</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#랜덤-텐서"><i class="fa fa-check"></i><b>1.2.2</b> 랜덤 텐서</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#단위-텐서"><i class="fa fa-check"></i><b>1.2.3</b> 단위 텐서</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#영0-텐서"><i class="fa fa-check"></i><b>1.2.4</b> 영(0) 텐서</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#고급기술-영리하게-만들기"><i class="fa fa-check"></i><b>1.3</b> 고급기술: 영리하게 만들기</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#텐서-직접선언"><i class="fa fa-check"></i><b>1.3.1</b> 텐서 직접선언</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#연산자-사용"><i class="fa fa-check"></i><b>1.3.2</b> <code>:</code> 연산자 사용</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#seq-함수-사용"><i class="fa fa-check"></i><b>1.3.3</b> <code>seq()</code> 함수 사용</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#연산자-사용-1"><i class="fa fa-check"></i><b>1.3.4</b> <code>%&gt;%</code> 연산자 사용</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#텐서와-행렬은-같을까"><i class="fa fa-check"></i><b>1.4</b> 텐서와 행렬은 같을까?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>2</b> 텐서 (tensor) 연산</a>
<ul>
<li class="chapter" data-level="2.1" data-path="operation.html"><a href="operation.html#토치-torch-불러오기-및-준비물-준비"><i class="fa fa-check"></i><b>2.1</b> 토치 (torch) 불러오기 및 준비물 준비</a></li>
<li class="chapter" data-level="2.2" data-path="operation.html"><a href="operation.html#텐서의-연산"><i class="fa fa-check"></i><b>2.2</b> 텐서의 연산</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="operation.html"><a href="operation.html#형type-변환"><i class="fa fa-check"></i><b>2.2.1</b> 형(type) 변환</a></li>
<li class="chapter" data-level="2.2.2" data-path="operation.html"><a href="operation.html#모양-변환"><i class="fa fa-check"></i><b>2.2.2</b> 모양 변환</a></li>
<li class="chapter" data-level="2.2.3" data-path="operation.html"><a href="operation.html#덧셈과-뺄셈"><i class="fa fa-check"></i><b>2.2.3</b> 덧셈과 뺄셈</a></li>
<li class="chapter" data-level="2.2.4" data-path="operation.html"><a href="operation.html#상수와의-연산"><i class="fa fa-check"></i><b>2.2.4</b> 상수와의 연산</a></li>
<li class="chapter" data-level="2.2.5" data-path="operation.html"><a href="operation.html#제곱근과-로그"><i class="fa fa-check"></i><b>2.2.5</b> 제곱근과 로그</a></li>
<li class="chapter" data-level="2.2.6" data-path="operation.html"><a href="operation.html#텐서의-곱셈"><i class="fa fa-check"></i><b>2.2.6</b> 텐서의 곱셈</a></li>
<li class="chapter" data-level="2.2.7" data-path="operation.html"><a href="operation.html#텐서의-전치transpose"><i class="fa fa-check"></i><b>2.2.7</b> 텐서의 전치(transpose)</a></li>
<li class="chapter" data-level="2.2.8" data-path="operation.html"><a href="operation.html#r에서의-3차원-배열"><i class="fa fa-check"></i><b>2.2.8</b> R에서의 3차원 배열</a></li>
<li class="chapter" data-level="2.2.9" data-path="operation.html"><a href="operation.html#다차원-텐서와-1차원-벡터-텐서의-연산"><i class="fa fa-check"></i><b>2.2.9</b> 다차원 텐서와 1차원 벡터 텐서의 연산</a></li>
<li class="chapter" data-level="2.2.10" data-path="operation.html"><a href="operation.html#차원-텐서-끼리의-연산-내적과-외적"><i class="fa fa-check"></i><b>2.2.10</b> 1차원 텐서 끼리의 연산, 내적과 외적</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="텐서의-이동-cpu-leftrightarrow-gpu.html"><a href="텐서의-이동-cpu-leftrightarrow-gpu.html"><i class="fa fa-check"></i><b>3</b> 텐서의 이동; CPU <span class="math inline">\(\leftrightarrow\)</span> GPU</a>
<ul>
<li class="chapter" data-level="3.1" data-path="텐서의-이동-cpu-leftrightarrow-gpu.html"><a href="텐서의-이동-cpu-leftrightarrow-gpu.html#gpu-사용-가능-체크"><i class="fa fa-check"></i><b>3.1</b> GPU 사용 가능 체크</a></li>
<li class="chapter" data-level="3.2" data-path="텐서의-이동-cpu-leftrightarrow-gpu.html"><a href="텐서의-이동-cpu-leftrightarrow-gpu.html#cpu-to-gpu"><i class="fa fa-check"></i><b>3.2</b> CPU to GPU</a></li>
<li class="chapter" data-level="3.3" data-path="텐서의-이동-cpu-leftrightarrow-gpu.html"><a href="텐서의-이동-cpu-leftrightarrow-gpu.html#gpu-to-cpu"><i class="fa fa-check"></i><b>3.3</b> GPU to CPU</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r6.html"><a href="r6.html"><i class="fa fa-check"></i><b>4</b> R6와 텐서</a>
<ul>
<li class="chapter" data-level="4.1" data-path="r6.html"><a href="r6.html#시작하기"><i class="fa fa-check"></i><b>4.1</b> 시작하기</a></li>
<li class="chapter" data-level="4.2" data-path="r6.html"><a href="r6.html#클래스class와-멤버함수method-그리고-필드field"><i class="fa fa-check"></i><b>4.2</b> 클래스(Class)와 멤버함수(Method), 그리고 필드(Field)</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="r6.html"><a href="r6.html#클래스는-왜-필요할까"><i class="fa fa-check"></i><b>4.2.1</b> 클래스는 왜 필요할까?</a></li>
<li class="chapter" data-level="4.2.2" data-path="r6.html"><a href="r6.html#학생자료-입력-예제"><i class="fa fa-check"></i><b>4.2.2</b> 학생자료 입력 예제</a></li>
<li class="chapter" data-level="4.2.3" data-path="r6.html"><a href="r6.html#클래스class-정의하기"><i class="fa fa-check"></i><b>4.2.3</b> 클래스(Class) 정의하기</a></li>
<li class="chapter" data-level="4.2.4" data-path="r6.html"><a href="r6.html#print를-사용한-결과물-정리"><i class="fa fa-check"></i><b>4.2.4</b> print()를 사용한 결과물 정리</a></li>
<li class="chapter" data-level="4.2.5" data-path="r6.html"><a href="r6.html#set을-이용한-클래스-조정"><i class="fa fa-check"></i><b>4.2.5</b> set을 이용한 클래스 조정</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="r6.html"><a href="r6.html#상속inheritance---클래스-물려받기"><i class="fa fa-check"></i><b>4.3</b> 상속(Inheritance) - 클래스 물려받기</a></li>
<li class="chapter" data-level="4.4" data-path="r6.html"><a href="r6.html#공개public정보와-비공개private-정보의-필요성"><i class="fa fa-check"></i><b>4.4</b> 공개(Public)정보와 비공개(Private) 정보의 필요성</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="r6.html"><a href="r6.html#활성-변수active-field를-사용한-읽기-전용-변수"><i class="fa fa-check"></i><b>4.4.1</b> 활성 변수(active field)를 사용한 읽기 전용 변수</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="r6.html"><a href="r6.html#텐서와-r6의-관계"><i class="fa fa-check"></i><b>4.5</b> 텐서와 R6의 관계</a></li>
<li class="chapter" data-level="4.6" data-path="r6.html"><a href="r6.html#r6-관련자료"><i class="fa fa-check"></i><b>4.6</b> R6 관련자료</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="forward.html"><a href="forward.html"><i class="fa fa-check"></i><b>5</b> 순전파 (Forward propagation)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="forward.html"><a href="forward.html#신경망의-구조"><i class="fa fa-check"></i><b>5.1</b> 신경망의 구조</a></li>
<li class="chapter" data-level="5.2" data-path="forward.html"><a href="forward.html#순전파forward-propagation"><i class="fa fa-check"></i><b>5.2</b> 순전파(Forward propagation)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="forward.html"><a href="forward.html#표본-1개-경로-1개만-생각해보기"><i class="fa fa-check"></i><b>5.2.1</b> 표본 1개, 경로 1개만 생각해보기</a></li>
<li class="chapter" data-level="5.2.2" data-path="forward.html"><a href="forward.html#개의-표본-경로-한꺼번에-생각하기"><i class="fa fa-check"></i><b>5.2.2</b> 1개의 표본, 경로 한꺼번에 생각하기</a></li>
<li class="chapter" data-level="5.2.3" data-path="forward.html"><a href="forward.html#전체-표본-경로-전체-생각해보기"><i class="fa fa-check"></i><b>5.2.3</b> 전체 표본, 경로 전체 생각해보기</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html"><i class="fa fa-check"></i><b>6</b> 미분 자동추적 기능 (Autograd) 에 대하여</a>
<ul>
<li class="chapter" data-level="6.1" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#예제-함수"><i class="fa fa-check"></i><b>6.1</b> 예제 함수</a></li>
<li class="chapter" data-level="6.2" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#데이터-생성"><i class="fa fa-check"></i><b>6.2</b> 데이터 생성</a></li>
<li class="chapter" data-level="6.3" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#함수-만들기-및-오차-그래프"><i class="fa fa-check"></i><b>6.3</b> 함수 만들기 및 오차 그래프</a></li>
<li class="chapter" data-level="6.4" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#autograd-기능-없이-기울기-구하기"><i class="fa fa-check"></i><b>6.4</b> Autograd 기능 없이 기울기 구하기</a></li>
<li class="chapter" data-level="6.5" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#자동미분autograd-기능"><i class="fa fa-check"></i><b>6.5</b> 자동미분(Autograd) 기능</a></li>
<li class="chapter" data-level="6.6" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#자동-미분-관련-함수들"><i class="fa fa-check"></i><b>6.6</b> 자동 미분 관련 함수들</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#detach"><i class="fa fa-check"></i><b>6.6.1</b> <code>$detach()</code></a></li>
<li class="chapter" data-level="6.6.2" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#requires_grad-변수와-requires_grad_true"><i class="fa fa-check"></i><b>6.6.2</b> <code>$requires_grad</code> 변수와 <code>$requires_grad_(TRUE)</code></a></li>
<li class="chapter" data-level="6.6.3" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#with_no_grad"><i class="fa fa-check"></i><b>6.6.3</b> <code>with_no_grad({})</code></a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#경사하강법"><i class="fa fa-check"></i><b>6.7</b> 경사하강법</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="미분-자동추적-기능-autograd-에-대하여.html"><a href="미분-자동추적-기능-autograd-에-대하여.html#시각화"><i class="fa fa-check"></i><b>6.7.1</b> 시각화</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html"><i class="fa fa-check"></i><b>7</b> <code>torch_nn</code> 모듈로 첫 신경망 정의하기</a>
<ul>
<li class="chapter" data-level="7.1" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#신경망-정의-custom-nn-modules"><i class="fa fa-check"></i><b>7.1</b> 신경망 정의 (Custom nn Modules)</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#nn_module과-클래스-상속"><i class="fa fa-check"></i><b>7.1.1</b> <code>nn_module</code>과 클래스 상속</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#nn_linear-클래스"><i class="fa fa-check"></i><b>7.2</b> <code>nn_linear</code> 클래스</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#bias-없는-경우"><i class="fa fa-check"></i><b>7.2.1</b> bias 없는 경우</a></li>
<li class="chapter" data-level="7.2.2" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#bias-있는-경우"><i class="fa fa-check"></i><b>7.2.2</b> bias 있는 경우</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="torch-nn-모듈로-첫-신경망-정의하기.html"><a href="torch-nn-모듈로-첫-신경망-정의하기.html#순전파forward-propagation-정의"><i class="fa fa-check"></i><b>7.3</b> 순전파(Forward propagation) 정의</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html"><i class="fa fa-check"></i><b>8</b> 나의 첫 신경망 학습</a>
<ul>
<li class="chapter" data-level="8.1" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#학습-준비---데이터-만들기"><i class="fa fa-check"></i><b>8.1</b> 학습 준비 - 데이터 만들기</a></li>
<li class="chapter" data-level="8.2" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#신경망과-블랙박스black-box"><i class="fa fa-check"></i><b>8.2</b> 신경망과 블랙박스(Black-box)</a></li>
<li class="chapter" data-level="8.3" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#신경망-학습"><i class="fa fa-check"></i><b>8.3</b> 신경망 학습</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#손실함수와-최적화-방법-선택"><i class="fa fa-check"></i><b>8.3.1</b> 손실함수와 최적화 방법 선택</a></li>
<li class="chapter" data-level="8.3.2" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#학습-구현"><i class="fa fa-check"></i><b>8.3.2</b> 학습 구현</a></li>
<li class="chapter" data-level="8.3.3" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#시각화-1"><i class="fa fa-check"></i><b>8.3.3</b> 시각화</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="나의-첫-신경망-학습.html"><a href="나의-첫-신경망-학습.html#과적합overfitting과의-싸움"><i class="fa fa-check"></i><b>8.4</b> 과적합(overfitting)과의 싸움</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dataset과-dataloader-클래스.html"><a href="dataset과-dataloader-클래스.html"><i class="fa fa-check"></i><b>9</b> <code>Dataset</code>과 <code>Dataloader</code> 클래스</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/statisticsplaybook/r-torch-playbook" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">딥러닝 공략집 with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="나의-첫-신경망-학습" class="section level1" number="8">
<h1><span class="header-section-number">Chapter 8</span> 나의 첫 신경망 학습</h1>
<p>저번 시간 우리는 토치에서 신경망을 정의하는 방법에 대하여 알아보았다. 오늘은 정의한 신경망을 어떻게 학습하는가에 대하여 알아보도록 하자.</p>
<div id="학습-준비---데이터-만들기" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> 학습 준비 - 데이터 만들기</h2>
<p>필자는 <a href="https://youtube.com/playlist?list=PLKtLBdGREmMnLbQnqGEfpCBtkGj2g_d-B">유튜브에 R을 사용한 통계관련 수업</a>들을 올려놓았다. 이 수업에서 큰 축을 이루는 것 중 하나가 바로 회귀분석이다. 회귀분석은 주어진 데이터를 모델링할 때 신경망의 가장 큰 장점은 회귀직선과 같은 선형모형들이 가지는 한계를 넘어서, 비선형 모델링을 할 수 있게 해준다는 것이다. 이러한 장점들을 잘 확인해보기 위해서 비선형 모델에서 관찰값을 뽑아 모의 데이터로 만들어 보도록 하자.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="나의-첫-신경망-학습.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb303-2"><a href="나의-첫-신경망-학습.html#cb303-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-3"><a href="나의-첫-신경망-학습.html#cb303-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 재현 가능을 위한 시드 고정</span></span>
<span id="cb303-4"><a href="나의-첫-신경망-학습.html#cb303-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb303-5"><a href="나의-첫-신경망-학습.html#cb303-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-6"><a href="나의-첫-신경망-학습.html#cb303-6" aria-hidden="true" tabindex="-1"></a><span class="co"># x 자리 임의 생성</span></span>
<span id="cb303-7"><a href="나의-첫-신경망-학습.html#cb303-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">100</span>))</span>
<span id="cb303-8"><a href="나의-첫-신경망-학습.html#cb303-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-9"><a href="나의-첫-신경망-학습.html#cb303-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델을 위한 f 함수 정의</span></span>
<span id="cb303-10"><a href="나의-첫-신경망-학습.html#cb303-10" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb303-11"><a href="나의-첫-신경망-학습.html#cb303-11" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">+</span> <span class="dv">30</span> <span class="sc">*</span> <span class="fu">sin</span>(<span class="fl">0.1</span> <span class="sc">*</span> x)</span>
<span id="cb303-12"><a href="나의-첫-신경망-학습.html#cb303-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb303-13"><a href="나의-첫-신경망-학습.html#cb303-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-14"><a href="나의-첫-신경망-학습.html#cb303-14" aria-hidden="true" tabindex="-1"></a><span class="co"># noise을 가미한 관찰값 생성</span></span>
<span id="cb303-15"><a href="나의-첫-신경망-학습.html#cb303-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">f</span>(x) <span class="sc">+</span> <span class="dv">5</span> <span class="sc">*</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb303-16"><a href="나의-첫-신경망-학습.html#cb303-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb303-17"><a href="나의-첫-신경망-학습.html#cb303-17" aria-hidden="true" tabindex="-1"></a>obs_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb303-18"><a href="나의-첫-신경망-학습.html#cb303-18" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(obs_data)</span></code></pre></div>
<pre><code>#&gt; # A tibble: 6 x 2
#&gt;       x     y
#&gt;   &lt;int&gt; &lt;dbl&gt;
#&gt; 1     1 10.1 
#&gt; 2     2  7.89
#&gt; 3     3  2.88
#&gt; 4     4 17.0 
#&gt; 5     5 21.9 
#&gt; 6     6 19.1</code></pre>
<p>관찰값 <span class="math inline">\(y\)</span>가 발생되는 코드를 살펴보면, <span class="math inline">\(y\)</span>는 발생되는 실제 함수 <span class="math inline">\(f\)</span>는 다음과 같이 비선형성을 가지고 있고, 거기에 잡음이 섞여서 관찰되는 형태를 띄고 있다.</p>
<p><span class="math display" id="eq:model-sample">\[
\begin{equation}
\begin{aligned} 
f(x) &amp;= x + 30 sin(0.1 x), \\
y &amp; = f(x) + \epsilon, \quad \epsilon \sim \mathbb{N}(0, 5^2)
\end{aligned}
\tag{8.1}
\end{equation}
\]</span></p>
<p>관찰값과 모델 함수 그려보도록 하자. 모델 함수의 경우 점선으로 표시했다.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="나의-첫-신경망-학습.html#cb305-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggthemes)</span>
<span id="cb305-2"><a href="나의-첫-신경망-학습.html#cb305-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(latex2exp)</span>
<span id="cb305-3"><a href="나의-첫-신경망-학습.html#cb305-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-4"><a href="나의-첫-신경망-학습.html#cb305-4" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_igray</span>())</span>
<span id="cb305-5"><a href="나의-첫-신경망-학습.html#cb305-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-6"><a href="나의-첫-신경망-학습.html#cb305-6" aria-hidden="true" tabindex="-1"></a>x_true <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb305-7"><a href="나의-첫-신경망-학습.html#cb305-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-8"><a href="나의-첫-신경망-학습.html#cb305-8" aria-hidden="true" tabindex="-1"></a>model_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> x_true, <span class="at">y =</span> <span class="fu">f</span>(x_true))</span>
<span id="cb305-9"><a href="나의-첫-신경망-학습.html#cb305-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-10"><a href="나의-첫-신경망-학습.html#cb305-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 관찰값 시각화</span></span>
<span id="cb305-11"><a href="나의-첫-신경망-학습.html#cb305-11" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> obs_data <span class="sc">%&gt;%</span> </span>
<span id="cb305-12"><a href="나의-첫-신경망-학습.html#cb305-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb305-13"><a href="나의-첫-신경망-학습.html#cb305-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;#E69F00&quot;</span>) <span class="sc">+</span></span>
<span id="cb305-14"><a href="나의-첫-신경망-학습.html#cb305-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;x&quot;</span>, <span class="at">y =</span> <span class="st">&quot;f(x)&quot;</span>,</span>
<span id="cb305-15"><a href="나의-첫-신경망-학습.html#cb305-15" aria-hidden="true" tabindex="-1"></a>         <span class="at">caption =</span> <span class="st">&quot;https://www.youtube.com/c/statisticsplaybook&quot;</span>)</span>
<span id="cb305-16"><a href="나의-첫-신경망-학습.html#cb305-16" aria-hidden="true" tabindex="-1"></a>p <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data =</span> model_data,</span>
<span id="cb305-17"><a href="나의-첫-신경망-학습.html#cb305-17" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y),</span>
<span id="cb305-18"><a href="나의-첫-신경망-학습.html#cb305-18" aria-hidden="true" tabindex="-1"></a>              <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="co"># 모델 함수</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:data-vis"></span>
<img src="08-train-mynn_files/figure-html/data-vis-1.png" alt="샘플 데이터 시각화. 비선형성이 잘 드러나있다." width="100%" />
<p class="caption">
그림 8.1: 샘플 데이터 시각화. 비선형성이 잘 드러나있다.
</p>
</div>
<p>위에 주어진 관찰값을 사용해서 회귀직선을 구해보면 다음과 같이 회색의 직선을 구할 수 있다.</p>
<div class="figure" style="text-align: center"><span id="fig:add-regression"></span>
<img src="08-train-mynn_files/figure-html/add-regression-1.png" alt="회귀직선(회색 직선)은 자료를 가장 잘 설명하는 선형모델로 볼 수 있다." width="100%" />
<p class="caption">
그림 8.2: 회귀직선(회색 직선)은 자료를 가장 잘 설명하는 선형모델로 볼 수 있다.
</p>
</div>
<p>추후 신경망 모델과의 비교를 위해서 회귀직선과 관찰값 사이의 잔차들의 제곱의 평균을 구해놓자.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="나의-첫-신경망-학습.html#cb306-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, obs_data)</span>
<span id="cb306-2"><a href="나의-첫-신경망-학습.html#cb306-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-3"><a href="나의-첫-신경망-학습.html#cb306-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean Squared Error for train data</span></span>
<span id="cb306-4"><a href="나의-첫-신경망-학습.html#cb306-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((model<span class="sc">$</span>residuals)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>#&gt; [1] 431.8235</code></pre>
</div>
<div id="신경망과-블랙박스black-box" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> 신경망과 블랙박스(Black-box)</h2>
<p>Figure <a href="나의-첫-신경망-학습.html#fig:add-regression">8.2</a> 을 보면 회귀 직선도 사실 자료의 <code>x</code>값에 따른 함수값의 변화를 아주 잘 잡아내는 것을 알 수 있다. 하지만, 우리가 자료를 발생시키는 함수의 구조가 비선형을 띈다는 것을 알고 있는 상태에서 보면(현실에서는 아무도 모른다.), 비선형성을 잡아내지 못하는 회귀 직선의 한계가 뚜렷하게 보인다. 따라서 통계학에서는 이러한 비선형성을 잡아내기 위해서 일반화선형모형(General Linear Model)이나 <a href="https://bookdown.org/cardiomoon/gam/">일반화 가법모형</a>(Generalized Additive Model; GAM)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> 등 여러가지 기법들이 발달했다. 신경망 역시 이것들의 연장선 상에 위치하는 모델이라고 생각해도 무방하다.</p>
<p>다만 모델의 해석적인 측면에서 일반화 선형모형에서는 모델을 만드는 사람들이 비선형성을 부여하는 툴들을 (예를 들어 link 함수를 자료를 보고 사용자가 선택한다.) 조절해서, 모델의 결과 해석력이 우수했다면, 일반화 가법모형과 신경망으로 갈 수록 사용자가 모델의 비선형성을 조절한다기모다 기법 자체가 비선형성을 잘 다루도록 설계가 되어있어서 해석력은 떨어지고 예측력은 증가했다. 신경망은 특히 모델 자체에 자유도를 높이고, 대량의 데이터를 사용하여 학습하면서 실제 함수를 찾아가는 방식이라서, 학습된 모델의 해석이 거의 불가능하게 되어버려서 블랙박스(Black box - 안이 어떻게 돌아가는지 모름) 모델이라는 별명이 생겼다.</p>
<p>모델러 입장에서는 ’뭐가 뭔지는 모르겠는데, 예측은 잘한다’라는 느낌이 드는 아이인데, 실제 성능적인 측면에서 기존 모델보다 월등하게 잘 예측을 하기 때문에, 신경망의 핫하게 된 이유가 되었다.</p>
<p>개인적으로 필자는 책의 뒷부분에 소개할 신경망의 여러 구조들이 결국에는 신경망의 모수 학습시(실제 함수를 찾아갈 때) 데이터를 넣었을 때 발산하지 않고 실제 함수로 잘 수렴하도록 잘 이끌어주는 신경망 구조를 만들어가는 것이라고 생각하고 있다. 즉, 대략적인 구조를 잡아주고, 그 안에서 데이터를 사용하여 tuning을 하는 방식이다. 어찌보면 이러한 과정은 통계에서 특정 조건을 만족하는 함수들의 집합을 정의하고 (회귀분석의 경우는 선형 함수들만을 생각하고), 그 안에서 최적 모델을 찾아가는 방식과 유사하다.</p>
</div>
<div id="신경망-학습" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> 신경망 학습</h2>
<p>앞에서 만들어낸 데이터를 사용하여, 신경망을 학습하도록 하자.
신경망을 학습한다는 이야기를 통계적으로 보면 주어진 혹은 설정한 손실 함수(loss function) 값을 최소화 시키는 신경망의 모수(weights)값을 찾는다는 이야기이다. 이런 최적 모수값 찾는 방법에는 여러가지가 있는데, <code>torch</code>에서는 이제까지 제안된 많은 방법들이 최적화 함수 (optimizer) 클래스 형식으로 제공이 된다. 당연한 것이겠지만 어떤 최적화 함수를 사용하느냐에 따라서 학습 결과가 달라진다.</p>
<p>앞에서 정의한 신경망 코드를 가져오자.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="나의-첫-신경망-학습.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb308-2"><a href="나의-첫-신경망-학습.html#cb308-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb308-3"><a href="나의-첫-신경망-학습.html#cb308-3" aria-hidden="true" tabindex="-1"></a><span class="fu">torch_manual_seed</span>(<span class="dv">2021</span>)</span>
<span id="cb308-4"><a href="나의-첫-신경망-학습.html#cb308-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb308-5"><a href="나의-첫-신경망-학습.html#cb308-5" aria-hidden="true" tabindex="-1"></a>TwoLayerNet <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(</span>
<span id="cb308-6"><a href="나의-첫-신경망-학습.html#cb308-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">classname =</span> <span class="st">&quot;TowLayerNet&quot;</span>,</span>
<span id="cb308-7"><a href="나의-첫-신경망-학습.html#cb308-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">initialize =</span> <span class="cf">function</span>(data_in, hidden, data_out){</span>
<span id="cb308-8"><a href="나의-첫-신경망-학습.html#cb308-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb308-9"><a href="나의-첫-신경망-학습.html#cb308-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cat</span>(<span class="st">&quot;Initiation complete!&quot;</span>)</span>
<span id="cb308-10"><a href="나의-첫-신경망-학습.html#cb308-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb308-11"><a href="나의-첫-신경망-학습.html#cb308-11" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>hidden1 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(data_in, hidden)</span>
<span id="cb308-12"><a href="나의-첫-신경망-학습.html#cb308-12" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>hidden2 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(hidden, hidden)</span>
<span id="cb308-13"><a href="나의-첫-신경망-학습.html#cb308-13" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>hidden3 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(hidden, hidden)</span>
<span id="cb308-14"><a href="나의-첫-신경망-학습.html#cb308-14" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>output_layer <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(hidden, data_out)</span>
<span id="cb308-15"><a href="나의-첫-신경망-학습.html#cb308-15" aria-hidden="true" tabindex="-1"></a>        self<span class="sc">$</span>tanh <span class="ot">&lt;-</span> <span class="fu">nn_tanh</span>()</span>
<span id="cb308-16"><a href="나의-첫-신경망-학습.html#cb308-16" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb308-17"><a href="나의-첫-신경망-학습.html#cb308-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 순전파 멤버함수 forward 정의 부분</span></span>
<span id="cb308-18"><a href="나의-첫-신경망-학습.html#cb308-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">forward =</span> <span class="cf">function</span>(X) {</span>
<span id="cb308-19"><a href="나의-첫-신경망-학습.html#cb308-19" aria-hidden="true" tabindex="-1"></a>        x <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">tanh</span>(self<span class="sc">$</span><span class="fu">hidden1</span>(X))</span>
<span id="cb308-20"><a href="나의-첫-신경망-학습.html#cb308-20" aria-hidden="true" tabindex="-1"></a>        x <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">tanh</span>(self<span class="sc">$</span><span class="fu">hidden2</span>(x))</span>
<span id="cb308-21"><a href="나의-첫-신경망-학습.html#cb308-21" aria-hidden="true" tabindex="-1"></a>        x <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">hidden3</span>(x)</span>
<span id="cb308-22"><a href="나의-첫-신경망-학습.html#cb308-22" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">output_layer</span>(x)</span>
<span id="cb308-23"><a href="나의-첫-신경망-학습.html#cb308-23" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(y_hat)</span>
<span id="cb308-24"><a href="나의-첫-신경망-학습.html#cb308-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb308-25"><a href="나의-첫-신경망-학습.html#cb308-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb308-26"><a href="나의-첫-신경망-학습.html#cb308-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb308-27"><a href="나의-첫-신경망-학습.html#cb308-27" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(zeallot)</span>
<span id="cb308-28"><a href="나의-첫-신경망-학습.html#cb308-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb308-29"><a href="나의-첫-신경망-학습.html#cb308-29" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU available</span></span>
<span id="cb308-30"><a href="나의-첫-신경망-학습.html#cb308-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cuda_is_available</span>()</span></code></pre></div>
<pre><code>#&gt; [1] TRUE</code></pre>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="나의-첫-신경망-학습.html#cb310-1" aria-hidden="true" tabindex="-1"></a>gpu <span class="ot">&lt;-</span> <span class="fu">torch_device</span>(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb310-2"><a href="나의-첫-신경망-학습.html#cb310-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb310-3"><a href="나의-첫-신경망-학습.html#cb310-3" aria-hidden="true" tabindex="-1"></a>x_tensor <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">scale</span>(x), <span class="at">dtype =</span> <span class="fu">torch_float</span>(),</span>
<span id="cb310-4"><a href="나의-첫-신경망-학습.html#cb310-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">requires_grad =</span> <span class="cn">TRUE</span>,</span>
<span id="cb310-5"><a href="나의-첫-신경망-학습.html#cb310-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">device =</span> gpu)<span class="sc">$</span><span class="fu">view</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb310-6"><a href="나의-첫-신경망-학습.html#cb310-6" aria-hidden="true" tabindex="-1"></a>y_tensor <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(y, <span class="at">dtype =</span> <span class="fu">torch_float</span>(),</span>
<span id="cb310-7"><a href="나의-첫-신경망-학습.html#cb310-7" aria-hidden="true" tabindex="-1"></a>                         <span class="at">device =</span> gpu)<span class="sc">$</span><span class="fu">view</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb310-8"><a href="나의-첫-신경망-학습.html#cb310-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb310-9"><a href="나의-첫-신경망-학습.html#cb310-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(D_in, H, D_out) <span class="sc">%&lt;-%</span>  <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">1</span>)</span>
<span id="cb310-10"><a href="나의-첫-신경망-학습.html#cb310-10" aria-hidden="true" tabindex="-1"></a>my_net <span class="ot">&lt;-</span> <span class="fu">TwoLayerNet</span>(D_in, H, D_out)</span></code></pre></div>
<pre><code>#&gt; Initiation complete!</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="나의-첫-신경망-학습.html#cb312-1" aria-hidden="true" tabindex="-1"></a>my_net<span class="sc">$</span><span class="fu">cuda</span>()</span>
<span id="cb312-2"><a href="나의-첫-신경망-학습.html#cb312-2" aria-hidden="true" tabindex="-1"></a>my_net</span></code></pre></div>
<pre><code>#&gt; An `nn_module` containing 251 parameters.
#&gt; 
#&gt; ── Modules ─────────────────────────────────────────────────────────────────────
#&gt; ● hidden1: &lt;nn_linear&gt; #20 parameters
#&gt; ● hidden2: &lt;nn_linear&gt; #110 parameters
#&gt; ● hidden3: &lt;nn_linear&gt; #110 parameters
#&gt; ● output_layer: &lt;nn_linear&gt; #11 parameters
#&gt; ● tanh: &lt;nn_tanh&gt; #0 parameters</code></pre>
<div id="손실함수와-최적화-방법-선택" class="section level3" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> 손실함수와 최적화 방법 선택</h3>
<p>토치에서는 많은 손실함수 (loss function)와 최적화 함수 (optimizer) 를 <a href="https://torch.mlverse.org/docs/reference/index.html">모두 제공</a>하는데, 그 중 가장 기본적인 손실함수인 MSE(Mean Squared Error)와 최적화 방법 SGD(Stochastic Gradient Desent) 방법을 사용하도록 하자. 둘은 다음과 같은 방법으로 선언한다.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="나의-첫-신경망-학습.html#cb314-1" aria-hidden="true" tabindex="-1"></a>mse_loss <span class="ot">&lt;-</span> <span class="fu">nn_mse_loss</span>(<span class="at">reduction =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb314-2"><a href="나의-첫-신경망-학습.html#cb314-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_sgd</span>(my_net<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">1e-5</span>) <span class="co"># </span></span></code></pre></div>
<p>손실함수와 최적화 방법에 대한 깊은 내용은 다른 챕터에서 다루도록 하고, 일단 간단하게 정리만 해보자. 지금은 신경망이 어떤 식의 구조를 가진 코드로 학습할 수 있는지 집중한다.</p>
<ol style="list-style-type: decimal">
<li><code>nn_mse_loss()</code></li>
</ol>
<p><code>nn_mse_loss</code> 함수의 경우, 다음의 두 가지 타입 손실함수를 제공한다. <code>reduction</code> 옵션을 <code>sum</code> 설정할 경우 손실함수는 다음과 같다.</p>
<p><span class="math display">\[
L(\hat{\boldsymbol{y}}, \boldsymbol{y}) = \sum_i^{n}(\hat{y_i}-y_i)^2
\]</span>
혹은 <code>reduction</code> 옵션을 <code>mean</code>으로 설정할 경우 손실함수는 MES를 반환한다.</p>
<p><span class="math display">\[
L(\hat{\boldsymbol{y}}, \boldsymbol{y}) = \frac{1}{n}\sum_i^{n}(\hat{y_i}-y_i)^2
\]</span></p>
<p>참고로 <code>none</code>으로 설정시 입력한 두 벡터의 차이의 제곱값들이 벡터 형식으로 나온다.</p>
<p>앞에 신경망 정의에서 보았듯 히든 레이어를 지날 때, activation 함수를 통과하므로, <strong>로스값 역시 어떤 activation 함수를 사용하느냐에 따라서 달라질 수 있다는 것을 염두해두자</strong>. 주어진 데이터에 대한 손실 함수 값은 다음과 같이 구할 수 있다.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="나의-첫-신경망-학습.html#cb315-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">my_net</span>(x_tensor)</span>
<span id="cb315-2"><a href="나의-첫-신경망-학습.html#cb315-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mse_loss</span>(y_hat, y_tensor)</span></code></pre></div>
<pre><code>#&gt; torch_tensor
#&gt; 4268.52
#&gt; [ CUDAFloatType{} ]</code></pre>
<ol style="list-style-type: decimal">
<li><code>optim_sgd()</code></li>
</ol>
<p>최적화 함수에 대하여는 나중에 따로 포스트로 다루겠다. 현재는 <code>optim_sgd</code>가 토치에서 제공하는 최적화 함수 중 하나이며, 입력값으로 신경망의 모수(weights)와 학습률(learning rate), <code>lr</code>,을 받는다는 것을 알아두자.</p>
<p>학습률(learning rate)은 자동 미분 챕터에서 다뤘던 경사하강도 알고리즘을 설명했던 부분에서도 다뤘는데, 신경망 학습 과정에서 중요한 역할을 차지한다. 이것에 따라서 학습이 잘 될 수도, 그렇지 않을 수도 있다. 보통 신경망이 복잡해 질 수록 학습률은 좀 더 세밀한 탐색을 위해 작게 잡아준다. 하지만, 학습률이 작은 경우에는 신경망을 학습하는 시간이 길어지게 된다. 최적은 학습률을 정하는 주제는 학문적으로도 아주 중요하고 방대한 주제이다. 한가지 예만 들면, 굳이 우리가 신경망을 학습시킬때 <a href="https://www.jeremyjordan.me/nn-learning-rate/">학습률을 동일하게 고정할 필요가 있을까?</a> 어떻게 보면 너무나 중용하고, 실무적인(당장 신경망 학습에 막대한 영향을 미치므로), 연구 주제같다.</p>
</div>
<div id="학습-구현" class="section level3" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> 학습 구현</h3>
<p>경사하강법에서 모수가 점점 업데이트 되면서 최적값으로 수렴하는 것을 보았다. 이렇게 업데이트 한번 진행이 되는 단계 단계를 딥러닝에서는 epoch라고 한다. 보통 데이터가 너무 많은 경우 전체 데이터를 한꺼번에 사용하는 것이 아니라 작은 단위로 잘라서 컴퓨터 메모리에 올리게 되는데, 이렇게 작게 잘린 데이터 단위를 배치(batch)라고 하며, 배치의 크기는 배치 안에 몇 개의 데이터가 들어가 있는가를 의미한다. 이와 관련한 내용은 추후에 데이터셋(Dataset) 클래스와 데이터 로더(Data loader) 클래스를 다룰 때 다시 자세하게 이야기하도록 한다.</p>
<p>다음의 코드는 <code>mse_loss</code> 값을 업데이트 단계마다 저장하고, 총 1000번의 모수 업데이트를 수행하여 신경망의 모수를 학습시키는 코드이다.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="나의-첫-신경망-학습.html#cb317-1" aria-hidden="true" tabindex="-1"></a>store_loss <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">50000</span>)</span>
<span id="cb317-2"><a href="나의-첫-신경망-학습.html#cb317-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (epoch <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50000</span>){</span>
<span id="cb317-3"><a href="나의-첫-신경망-학습.html#cb317-3" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb317-4"><a href="나의-첫-신경망-학습.html#cb317-4" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> <span class="fu">my_net</span>(x_tensor)</span>
<span id="cb317-5"><a href="나의-첫-신경망-학습.html#cb317-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">&lt;-</span> <span class="fu">mse_loss</span>(output, y_tensor)</span>
<span id="cb317-6"><a href="나의-첫-신경망-학습.html#cb317-6" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb317-7"><a href="나의-첫-신경망-학습.html#cb317-7" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb317-8"><a href="나의-첫-신경망-학습.html#cb317-8" aria-hidden="true" tabindex="-1"></a>    store_loss[epoch] <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb317-9"><a href="나의-첫-신경망-학습.html#cb317-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb317-10"><a href="나의-첫-신경망-학습.html#cb317-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (epoch <span class="sc">%%</span> <span class="dv">5000</span> <span class="sc">==</span> <span class="dv">0</span>){</span>
<span id="cb317-11"><a href="나의-첫-신경망-학습.html#cb317-11" aria-hidden="true" tabindex="-1"></a>        <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loss at epoch %d: %.2f</span><span class="sc">\n</span><span class="st">&quot;</span>, epoch, store_loss[epoch]))</span>
<span id="cb317-12"><a href="나의-첫-신경망-학습.html#cb317-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb317-13"><a href="나의-첫-신경망-학습.html#cb317-13" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>#&gt; Loss at epoch 5000: 123.00
#&gt; Loss at epoch 10000: 94.12
#&gt; Loss at epoch 15000: 86.84
#&gt; Loss at epoch 20000: 81.58
#&gt; Loss at epoch 25000: 76.47
#&gt; Loss at epoch 30000: 70.47
#&gt; Loss at epoch 35000: 58.99
#&gt; Loss at epoch 40000: 41.10
#&gt; Loss at epoch 45000: 28.03
#&gt; Loss at epoch 50000: 23.86</code></pre>
</div>
<div id="시각화-1" class="section level3" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> 시각화</h3>
<p>이전 섹션에서 우리는 신경망의 학습이 진행되면서 손실함수(loss)값이 점점 줄어드는 것을 확인할 수 있었다. 최종적으로 학습된 신경망은 어떻게 생겼을까? .</p>
<div class="figure" style="text-align: center"><span id="fig:vis-result"></span>
<img src="08-train-mynn_files/figure-html/vis-result-1.png" alt="학습된 신경망과 회귀직선 비교" width="100%" />
<p class="caption">
그림 8.3: 학습된 신경망과 회귀직선 비교
</p>
</div>
<p>학습된 신경망이 데이터가 발생되는 함수의 비선형성을 잘 반영하고 있는 것을 확인할 수 있다.</p>
</div>
</div>
<div id="과적합overfitting과의-싸움" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> 과적합(overfitting)과의 싸움</h2>
<p>이렇게 학습한 신경망은 너무나도 완벽해 보이지만, 사실 중대한 문제점이 있다. 바로 학습에 사용된 데이터에 나타난 패턴을 너무나도 잘 반영하고 있는 것이 문제이다. 사실 뭐가 문제냐 싶지만, 우리가 흔히 말하는 “이론과 현실은 달라요.” 라는 말이 신경망 학습에서도 그대로 적용이 된다고 생각하면 된다.</p>
<p>즉, 학습 데이터를 너무나 잘 반영하는 것도 좋지만, 이렇게 학습된 신경망을 사용해서 예측을 할 때, 신경망에 입력 될 데이터는 대부분 학습 데이터와 비슷하기도 하겠지만, 비슷하지 않은 전혀 다른 입력값이 들어올 수 있다. 이런 상황에 잘 대비하기(?) 위해서 혹은 신경망이 성능을 잘 내기 위해서는 신경망을 학습을 할 때 성과측정을 신경망의 학습에 한번도 사용되지 않는 새로운 데이터로 평가를 해야만 한다.</p>
<p>이렇게 모델이 학습 데이터 패턴을 너무나 많이 반영하고 있는 현상을 과하게 적합이 되어 있다고 하여 모델 과적합(overfitting) 상태라고 부른다. 기계학습과 딥러닝에서는 일단 베이스라인 모델이 정해진 후에는 어떤 모수값(weights)이 최적의 모수인지를 찾아내야하는 과정을 거친다. 이 과정을 거치는 가장 큰 이유는 모델 과적합 방지에 있다. 어떻게 하면 학습 데이터의 패턴은 잘 반영하면서, 새로이 들어올 데이터에도 잘 반응할 수 있는 모델을 세울 수 있을지 앞으로의 학습을 통해서 차근차근 배워보자.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>링크는 가톨릭대 문건웅 교수님이 쓴 일반화 가법모델에 대한 내용이다. R코드와 함께 친절하게 설명이 되어있다.<a href="나의-첫-신경망-학습.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="torch-nn-모듈로-첫-신경망-정의하기.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dataset과-dataloader-클래스.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": null,
"edit": {
"link": "https://github.com/statisticsplaybook/r-torch-playbook/edit/master/08-train-mynn.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
